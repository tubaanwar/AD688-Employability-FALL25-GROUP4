[
  {
    "objectID": "data_analysis.html",
    "href": "data_analysis.html",
    "title": "1 Set style for better visualizations",
    "section": "",
    "text": "â€œâ€œâ€ Complete Data Analysis & Exploratory Analysis of Job Market Trends Team: Tuba Anwar, Kriti Singh, Soham Deshkhaire Boston University â€œâ€œâ€\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import missingno as msno import warnings warnings.filterwarnings(â€˜ignoreâ€™)\n\n1 Set style for better visualizations\nsns.set_style(â€œwhitegridâ€) plt.rcParams[â€˜figure.figsizeâ€™] = (12, 6)\nprint(â€œ=â€80) print(â€œJOB MARKET ANALYSIS 2024 - DATA CLEANING & EXPLORATORY ANALYSISâ€) print(â€œ=â€80)\n\n\n2 ========================================\n\n\n3 LOAD DATA\n\n\n4 ========================================\ndf = pd.read_csv(â€˜lightcast_job_postings.csvâ€™) print(fâ€ğŸ“Š Original dataset shape: {df.shape}â€œ) print(fâ€ğŸ“Š Total records: {len(df):,}â€œ) print(fâ€ğŸ“Š Total columns: {len(df.columns)}â€œ)\n\n\n5 ========================================\n\n\n6 STEP 1: REMOVING REDUNDANT COLUMNS\n\n\n7 ========================================\nprint(â€œâ€ + â€œ=â€80) print(â€œSTEP 1: REMOVING REDUNDANT COLUMNSâ€) print(â€œ=â€80)\nprint(â€œâ€œâ€ WHY REMOVE THESE COLUMNS? - Tracking & Administrative: ID, URL, DUPLICATES - metadata not useful for analysis - Raw Text Fields: We have cleaned versions (TITLE, COMPANY_NAME) - Deprecated Classifications: Keep only latest standards (NAICS_2022_6, SOC_2021_4) - Duplicate Geographic Fields: Multiple versions of county/MSA create redundancy â€œâ€œâ€œ)\ncolumns_to_drop = [ # Administrative & tracking columns â€œIDâ€, â€œURLâ€, â€œACTIVE_URLSâ€, â€œDUPLICATESâ€, â€œLAST_UPDATED_TIMESTAMPâ€, # Raw text columns (we have cleaned versions) â€œBODYâ€, â€œTITLE_RAWâ€, â€œCOMPANY_RAWâ€, â€œACTIVE_SOURCES_INFOâ€, # Deprecated NAICS versions (keeping NAICS_2022_6) â€œNAICS2â€, â€œNAICS2_NAMEâ€, â€œNAICS3â€, â€œNAICS3_NAMEâ€, â€œNAICS4â€, â€œNAICS4_NAMEâ€, â€œNAICS5â€, â€œNAICS5_NAMEâ€, â€œNAICS6â€, â€œNAICS6_NAMEâ€, # Deprecated SOC versions (keeping SOC_2021_4) â€œSOC_2â€, â€œSOC_2_NAMEâ€, â€œSOC_3â€, â€œSOC_3_NAMEâ€, â€œSOC_4â€, â€œSOC_4_NAMEâ€, â€œSOC_5â€, â€œSOC_5_NAMEâ€, â€œSOC_2021_2â€, â€œSOC_2021_2_NAMEâ€, â€œSOC_2021_3â€, â€œSOC_2021_3_NAMEâ€, â€œSOC_2021_5â€, â€œSOC_2021_5_NAMEâ€, # Deprecated occupation classifications â€œLOT_CAREER_AREAâ€, â€œLOT_CAREER_AREA_NAMEâ€, â€œLOT_OCCUPATIONâ€, â€œLOT_OCCUPATION_NAMEâ€, â€œLOT_SPECIALIZED_OCCUPATIONâ€, â€œLOT_SPECIALIZED_OCCUPATION_NAMEâ€, â€œLOT_OCCUPATION_GROUPâ€, â€œLOT_OCCUPATION_GROUP_NAMEâ€, â€œLOT_V6_SPECIALIZED_OCCUPATIONâ€, â€œLOT_V6_SPECIALIZED_OCCUPATION_NAMEâ€, â€œLOT_V6_OCCUPATIONâ€, â€œLOT_V6_OCCUPATION_NAMEâ€, â€œLOT_V6_OCCUPATION_GROUPâ€, â€œLOT_V6_OCCUPATION_GROUP_NAMEâ€, â€œLOT_V6_CAREER_AREAâ€, â€œLOT_V6_CAREER_AREA_NAMEâ€, # Deprecated CIP and ONET versions â€œONET_2019â€, â€œONET_2019_NAMEâ€, â€œCIP6â€, â€œCIP6_NAMEâ€, â€œCIP2â€, â€œCIP2_NAMEâ€, # Duplicate geographic fields â€œCOUNTYâ€, â€œCOUNTY_NAMEâ€, â€œCOUNTY_OUTGOINGâ€, â€œCOUNTY_NAME_OUTGOINGâ€, â€œCOUNTY_INCOMINGâ€, â€œCOUNTY_NAME_INCOMINGâ€, â€œMSAâ€, â€œMSA_OUTGOINGâ€, â€œMSA_INCOMINGâ€, # Deprecated salary fields (keeping SALARY) â€œSALARY_TOâ€, â€œSALARY_FROMâ€, â€œORIGINAL_PAY_PERIODâ€, # Model versions â€œMODELED_EXPIREDâ€, â€œMODELED_DURATIONâ€]\ncolumns_to_drop = [col for col in columns_to_drop if col in df.columns] df_cleaned = df.drop(columns=columns_to_drop, inplace=False)\nprint(fâ€âœ… Original dataset: {df.shape}â€œ) print(fâ€âœ… Cleaned dataset: {df_cleaned.shape}â€œ) print(fâ€âœ… Columns removed: {len(columns_to_drop)}â€œ) print(fâ€âœ… Columns retained: {len(df_cleaned.columns)}â€œ)\n\n\n8 ========================================\n\n\n9 STEP 2: HANDLING MISSING VALUES\n\n\n10 ========================================\nprint(â€œâ€ + â€œ=â€80) print(â€œSTEP 2: UNDERSTANDING & HANDLING MISSING DATAâ€) print(â€œ=â€80)\nprint(â€œğŸ“Š Visualizing Missing Values Patternâ€¦â€)\n\n\n11 Create missing values heatmap\nfig, ax = plt.subplots(figsize=(14, 6)) msno.heatmap(df_cleaned, ax=ax, fontsize=8) plt.title(â€œMissing Values Heatmap - Lightcast Job Market Dataâ€, fontsize=14, fontweight=â€˜boldâ€™, pad=20) plt.tight_layout() plt.show()\n\n\n12 Show top missing value statistics\nmissing_stats = pd.DataFrame({ â€˜Columnâ€™: df_cleaned.columns, â€˜Missing_Countâ€™: df_cleaned.isnull().sum().values, â€˜Missing_Percentageâ€™: (df_cleaned.isnull().sum().values / len(df_cleaned) * 100).round(2) }) missing_stats = missing_stats[missing_stats[â€˜Missing_Countâ€™] &gt; 0].sort_values( â€˜Missing_Percentageâ€™, ascending=False )\nprint(â€œğŸ“‹ Top 10 Columns with Missing Values:â€) print(missing_stats.head(10).to_string(index=False))\nprint(â€œâ€œâ€ MISSING VALUE STRATEGY: 1. Drop columns with &gt;50% missing values (insufficient data) 2. Fill numerical columns with median (robust to outliers) 3. Fill categorical columns with â€˜Unknownâ€™ (preserve data) â€œâ€œâ€œ)\n\n\n13 Drop columns with &gt;50% missing\nthreshold = len(df_cleaned) * 0.5 cols_before = len(df_cleaned.columns) df_cleaned = df_cleaned.dropna(thresh=threshold, axis=1) cols_after = len(df_cleaned.columns) print(fâ€âœ… Dropped {cols_before - cols_after} columns with &gt;50% missing valuesâ€)\n\n\n14 Fill numerical columns with median\nnumerical_cols = df_cleaned.select_dtypes(include=[np.number]).columns.tolist() filled_numerical = 0 for col in numerical_cols: if df_cleaned[col].isnull().sum() &gt; 0: median_val = df_cleaned[col].median() df_cleaned[col].fillna(median_val, inplace=True) filled_numerical += 1\nprint(fâ€âœ… Filled {filled_numerical} numerical columns with median valuesâ€)\n\n\n15 Fill categorical columns with â€œUnknownâ€\ncategorical_cols = df_cleaned.select_dtypes(include=[â€˜objectâ€™]).columns.tolist() filled_categorical = 0 for col in categorical_cols: if df_cleaned[col].isnull().sum() &gt; 0: df_cleaned[col].fillna(â€œUnknownâ€, inplace=True) filled_categorical += 1\nprint(fâ€âœ… Filled {filled_categorical} categorical columns with â€˜Unknownâ€™â€œ) print(fâ€âœ… Total missing values after imputation: {df_cleaned.isnull().sum().sum()}â€œ)\n\n\n16 ========================================\n\n\n17 STEP 3: REMOVING DUPLICATES\n\n\n18 ========================================\nprint(â€œâ€ + â€œ=â€80) print(â€œSTEP 3: REMOVING DUPLICATE JOB POSTINGSâ€) print(â€œ=â€80)\nprint(â€œâ€œâ€ WHY REMOVE DUPLICATES? Duplicate job postings skew our analysis. We remove them based on: - Job Title - Company Name - Location This ensures each unique position is counted only once. â€œâ€œâ€œ)\nduplicates_before = df_cleaned.duplicated().sum() print(fâ€ğŸ“Š Duplicate records found: {duplicates_before:,}â€œ)\ndf_cleaned = df_cleaned.drop_duplicates( subset=[â€˜TITLE_NAMEâ€™, â€˜COMPANY_NAMEâ€™, â€˜LOCATIONâ€™], keep=â€˜firstâ€™ )\nduplicates_after = df_cleaned.duplicated().sum() print(fâ€âœ… Duplicates after removal: {duplicates_after:,}â€œ) print(fâ€âœ… Records removed: {duplicates_before - duplicates_after:,}â€œ) print(fâ€âœ… Final cleaned dataset: {df_cleaned.shape}â€œ)\n\n\n19 ========================================\n\n\n20 STEP 4: EXPLORATORY DATA ANALYSIS (EDA)\n\n\n21 ========================================\nprint(â€œâ€ + â€œ=â€80) print(â€œSTEP 4: EXPLORATORY DATA ANALYSISâ€) print(â€œ=â€80)\n\n\n22 VISUALIZATION 1: Top Job Titles\nprint(â€œğŸ“Š VISUALIZATION 1: TOP JOB TITLES BY DEMANDâ€) print(â€œ-â€ * 80) print(â€œâ€œâ€ KEY INSIGHT: Understanding which job titles are most in demand helps job seekers identify the skills and roles to target in their career development. â€œâ€œâ€œ)\ntitle_counts = df_cleaned[â€˜TITLE_NAMEâ€™].value_counts().head(10)\nfig, ax = plt.subplots(figsize=(12, 7)) bars = title_counts.plot(kind=â€˜barhâ€™, ax=ax, color=â€˜#6C9BCFâ€™) ax.set_xlabel(â€˜Number of Job Postingsâ€™, fontsize=13, fontweight=â€˜boldâ€™) ax.set_ylabel(â€˜Job Titleâ€™, fontsize=13, fontweight=â€˜boldâ€™) ax.set_title(â€˜Top 10 Most In-Demand Job Titles (2024)â€™, fontsize=15, fontweight=â€˜boldâ€™, pad=20) ax.invert_yaxis() ax.tick_params(left=False, bottom=False) ax.spines[â€˜topâ€™].set_visible(False) ax.spines[â€˜rightâ€™].set_visible(False)\nfor i, v in enumerate(title_counts): ax.text(v + 50, i, fâ€™{v:,}â€˜, va=â€™centerâ€™, fontweight=â€˜boldâ€™, fontsize=10)\nplt.tight_layout() plt.show()\nprint(fâ€ğŸ“ˆ Top 5 Job Titles:â€œ) for i, (title, count) in enumerate(title_counts.head(5).items(), 1): pct = (count / title_counts.sum() * 100) print(fâ€{i}. {title}: {count:,} postings ({pct:.1f}% of top 10)â€œ)\n\n\n23 VISUALIZATION 2: Employment Type Distribution\nprint(â€œVISUALIZATION 2: JOB POSTINGS BY EMPLOYMENT TYPEâ€) print(â€œ-â€ * 80) print(â€œâ€œâ€ KEY INSIGHT: Employment type distribution shows whether the market is primarily offering full-time, part-time, or contract positions, helping job seekers identify opportunities that match their work preferences. â€œâ€œâ€œ)\nemployment_counts = df_cleaned[â€˜EMPLOYMENT_TYPE_NAMEâ€™].value_counts()\nfig, ax = plt.subplots(figsize=(12, 7)) employment_counts.plot(kind=â€˜barhâ€™, ax=ax, color=â€˜#FF9B85â€™) ax.set_xlabel(â€˜Number of Job Postingsâ€™, fontsize=13, fontweight=â€˜boldâ€™) ax.set_ylabel(â€˜Employment Typeâ€™, fontsize=13, fontweight=â€˜boldâ€™) ax.set_title(â€˜Job Market by Employment Type (2024)â€™, fontsize=15, fontweight=â€˜boldâ€™, pad=20) ax.invert_yaxis() ax.tick_params(left=False, bottom=False) ax.spines[â€˜topâ€™].set_visible(False) ax.spines[â€˜rightâ€™].set_visible(False)\nfor i, v in enumerate(employment_counts): ax.text(v + 50, i, fâ€™{v:,}â€˜, va=â€™centerâ€™, fontweight=â€˜boldâ€™, fontsize=10)\nplt.tight_layout() plt.show()\nprint(fâ€Employment Type Breakdown:â€œ) for emp_type, count in employment_counts.items(): pct = (count / employment_counts.sum() * 100) print(fâ€â€¢ {emp_type}: {count:,} postings ({pct:.1f}%)â€œ)\n\n\n24 VISUALIZATION 3: Remote Work Distribution (IMPROVED WITH PASTEL COLORS)\nprint(â€œVISUALIZATION 3: REMOTE WORK DISTRIBUTIONâ€) print(â€œ-â€ * 80) print(â€œâ€œâ€ KEY INSIGHT: The balance between remote and on-site jobs reflects post-pandemic hiring trends and shows employersâ€™ flexibility preferences. Higher remote job percentages indicate industry acceptance of distributed workforces. â€œâ€œâ€œ)\nremote_counts = df_cleaned[â€˜REMOTE_TYPE_NAMEâ€™].value_counts()\n\n\n25 Beautiful pastel color palette\npastel_colors = [â€˜#B4E7CEâ€™, â€˜#FFB5A7â€™, â€˜#A7C7E7â€™, â€˜#F8E5F1â€™, â€˜#FFDAB9â€™]\nfig, ax = plt.subplots(figsize=(12, 8))\nwedges, texts, autotexts = ax.pie( remote_counts.values, labels=remote_counts.index, autopct=â€˜%1.1f%%â€™, colors=pastel_colors[:len(remote_counts)], startangle=90, pctdistance=0.85, explode=[0.05] * len(remote_counts), textprops={â€˜fontsizeâ€™: 12, â€˜fontweightâ€™: â€˜boldâ€™}, shadow=True )\n\n\n26 Style percentage text\nfor autotext in autotexts: autotext.set_color(â€˜#2C3E50â€™) autotext.set_fontweight(â€˜boldâ€™) autotext.set_fontsize(14)\n\n\n27 Style labels\nfor text in texts: text.set_fontsize(13) text.set_fontweight(â€˜boldâ€™) text.set_color(â€˜#34495Eâ€™)\n\n\n28 Title\nax.set_title( â€œJob Market Distribution: Remote vs.Â On-Site Work (2024)â€, fontsize=16, fontweight=â€˜boldâ€™, pad=20, color=â€˜#2C3E50â€™ )\n\n\n29 Donut effect\ncentre_circle = plt.Circle((0, 0), 0.70, fc=â€˜whiteâ€™, linewidth=0) fig.gca().add_artist(centre_circle)\nax.axis(â€˜equalâ€™) plt.tight_layout() plt.show()\nprint(fâ€Remote Work Statistics:â€œ) for remote_type, count in remote_counts.items(): pct = (count / remote_counts.sum() * 100) print(fâ€â€¢ {remote_type}: {count:,} jobs ({pct:.1f}%)â€œ)\n\n\n30 ========================================\n\n\n31 FINAL SUMMARY\n\n\n32 ========================================\nprint(â€œâ€ + â€œ=â€80) print(â€œDATA QUALITY & ANALYSIS SUMMARYâ€) print(â€œ=â€80) print(fâ€â€œâ€ CLEANING RESULTS: â€¢ Removed {len(columns_to_drop)} redundant columns â€¢ Handled missing values (median/Unknown imputation) â€¢ Removed {df.shape[0] - df_cleaned.shape[0]:,} duplicate records\nFINAL DATASET: â€¢ Records: {df_cleaned.shape[0]:,} â€¢ Features: {df_cleaned.shape[1]} â€¢ Data Quality: High (no missing values, no duplicates)\nKEY FINDINGS: â€¢ Top job demand: {title_counts.index[0]} â€¢ Primary employment type: {employment_counts.index[0]} â€¢ Remote work adoption: {(remote_counts.get(â€˜Remoteâ€™, 0) / remote_counts.sum() * 100):.1f}%\nNEXT STEPS: â€¢ Salary analysis by industry â€¢ AI vs.Â non-AI job trends â€¢ Geographic hiring patterns â€¢ Career strategy development â€œâ€œâ€œ) print(â€=â€œ*80)"
  },
  {
    "objectID": "data_analysis.html#introduction",
    "href": "data_analysis.html#introduction",
    "title": "Data Analysis",
    "section": "",
    "text": "This document provides a comprehensive analysis of job market data for 2024. We perform data cleaning, handle missing values, remove duplicates, and conduct exploratory data analysis to understand hiring trends, salary distributions, and job market dynamics.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set style for better visualizations\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# Load your data\ndf = pd.read_csv('lightcast_job_postings.csv')\nprint(f\"Original dataset shape: {df.shape}\")\nprint(f\"Total records: {len(df):,}\")\nprint(f\"Total columns: {len(df.columns)}\")\n\nOriginal dataset shape: (72498, 131)\nTotal records: 72,498\nTotal columns: 131"
  },
  {
    "objectID": "data_analysis.html#step-1-removing-redundant-columns",
    "href": "data_analysis.html#step-1-removing-redundant-columns",
    "title": "Data Analysis",
    "section": "2 Step 1: Removing Redundant Columns",
    "text": "2 Step 1: Removing Redundant Columns\n\n2.1 Why Remove These Columns?\nWe remove redundant columns to improve dataset quality and analysis efficiency. Specifically:\n\nTracking & Administrative Columns: ID, URL, DUPLICATES, LAST_UPDATED_TIMESTAMP - These are metadata and donâ€™t contribute to analysis\nRaw Text Fields: BODY, TITLE_RAW, COMPANY_RAW - We have cleaned versions (TITLE, COMPANY_NAME, TITLE_CLEAN)\nDeprecated Classifications: We keep only the latest NAICS_2022_6, SOC_2021_4, and CIP4 standards\nDuplicate Geographic Fields: Multiple versions of county/MSA data create redundancy\n\n\n# List of columns to drop\ncolumns_to_drop = [\n    # Administrative & tracking columns\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n    # Raw text columns (we have cleaned versions)\n    \"BODY\", \"TITLE_RAW\", \"COMPANY_RAW\", \"ACTIVE_SOURCES_INFO\",\n    # Deprecated NAICS versions (keeping NAICS_2022_6)\n    \"NAICS2\", \"NAICS2_NAME\", \"NAICS3\", \"NAICS3_NAME\", \n    \"NAICS4\", \"NAICS4_NAME\", \"NAICS5\", \"NAICS5_NAME\", \n    \"NAICS6\", \"NAICS6_NAME\",\n    # Deprecated SOC versions (keeping SOC_2021_4)\n    \"SOC_2\", \"SOC_2_NAME\", \"SOC_3\", \"SOC_3_NAME\", \n    \"SOC_4\", \"SOC_4_NAME\", \"SOC_5\", \"SOC_5_NAME\",\n    \"SOC_2021_2\", \"SOC_2021_2_NAME\", \"SOC_2021_3\", \"SOC_2021_3_NAME\", \n    \"SOC_2021_5\", \"SOC_2021_5_NAME\",\n    # Deprecated occupation classifications\n    \"LOT_CAREER_AREA\", \"LOT_CAREER_AREA_NAME\", \"LOT_OCCUPATION\", \"LOT_OCCUPATION_NAME\",\n    \"LOT_SPECIALIZED_OCCUPATION\", \"LOT_SPECIALIZED_OCCUPATION_NAME\", \n    \"LOT_OCCUPATION_GROUP\", \"LOT_OCCUPATION_GROUP_NAME\",\n    \"LOT_V6_SPECIALIZED_OCCUPATION\", \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\", \n    \"LOT_V6_OCCUPATION\", \"LOT_V6_OCCUPATION_NAME\",\n    \"LOT_V6_OCCUPATION_GROUP\", \"LOT_V6_OCCUPATION_GROUP_NAME\", \n    \"LOT_V6_CAREER_AREA\", \"LOT_V6_CAREER_AREA_NAME\",\n    # Deprecated CIP and ONET versions\n    \"ONET_2019\", \"ONET_2019_NAME\", \"CIP6\", \"CIP6_NAME\", \"CIP2\", \"CIP2_NAME\",\n    # Duplicate geographic fields\n    \"COUNTY\", \"COUNTY_NAME\", \"COUNTY_OUTGOING\", \"COUNTY_NAME_OUTGOING\", \n    \"COUNTY_INCOMING\", \"COUNTY_NAME_INCOMING\",\n    \"MSA\", \"MSA_OUTGOING\", \"MSA_INCOMING\",\n    # Deprecated salary fields (keeping SALARY)\n    \"SALARY_TO\", \"SALARY_FROM\", \"ORIGINAL_PAY_PERIOD\",\n    # Model versions (keep actual data)\n    \"MODELED_EXPIRED\", \"MODELED_DURATION\"\n]\n\n# Drop only columns that exist in the dataset\ncolumns_to_drop = [col for col in columns_to_drop if col in df.columns]\ndf_cleaned = df.drop(columns=columns_to_drop, inplace=False)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"DATA CLEANING SUMMARY\")\nprint(f\"{'='*60}\")\nprint(f\"Original dataset shape: {df.shape}\")\nprint(f\"Cleaned dataset shape: {df_cleaned.shape}\")\nprint(f\"Columns removed: {len(columns_to_drop)}\")\nprint(f\"Columns retained: {len(df_cleaned.columns)}\")\nprint(f\"\\nRemaining key columns:\\n{', '.join(df_cleaned.columns[:15])}\")\n\n\n============================================================\nDATA CLEANING SUMMARY\n============================================================\nOriginal dataset shape: (72498, 131)\nCleaned dataset shape: (72498, 62)\nColumns removed: 69\nColumns retained: 62\n\nRemaining key columns:\nLAST_UPDATED_DATE, POSTED, EXPIRED, DURATION, SOURCE_TYPES, SOURCES, COMPANY, COMPANY_NAME, COMPANY_IS_STAFFING, EDUCATION_LEVELS, EDUCATION_LEVELS_NAME, MIN_EDULEVELS, MIN_EDULEVELS_NAME, MAX_EDULEVELS, MAX_EDULEVELS_NAME"
  },
  {
    "objectID": "data_analysis.html#step-2-handling-missing-values",
    "href": "data_analysis.html#step-2-handling-missing-values",
    "title": "Data Analysis",
    "section": "3 Step 2: Handling Missing Values",
    "text": "3 Step 2: Handling Missing Values\n\n3.1 Understanding Missing Data\nBefore imputation, letâ€™s visualize where data is missing:\n\n# Create a heatmap of missing values\nfig, ax = plt.subplots(figsize=(14, 6))\nmsno.heatmap(df_cleaned, ax=ax)\nplt.title(\"Missing Values Heatmap - Lightcast Job Market Data\", fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Print detailed missing value statistics\nmissing_stats = pd.DataFrame({\n    'Column': df_cleaned.columns,\n    'Missing_Count': df_cleaned.isnull().sum().values,\n    'Missing_Percentage': (df_cleaned.isnull().sum().values / len(df_cleaned) * 100).round(2)\n})\nmissing_stats = missing_stats[missing_stats['Missing_Count'] &gt; 0].sort_values('Missing_Percentage', ascending=False)\n\nprint(\"\\nMissing Value Statistics:\")\nprint(missing_stats.head(10).to_string(index=False))\n\n\n\n\n\n\n\n\n\nMissing Value Statistics:\n                Column  Missing_Count  Missing_Percentage\n  MAX_YEARS_EXPERIENCE          64068               88.37\n         MAX_EDULEVELS          56183               77.50\n    MAX_EDULEVELS_NAME          56183               77.50\n     LIGHTCAST_SECTORS          54711               75.47\nLIGHTCAST_SECTORS_NAME          54711               75.47\n                SALARY          41690               57.51\n              DURATION          27316               37.68\n  MIN_YEARS_EXPERIENCE          23146               31.93\n               EXPIRED           7844               10.82\n     MSA_NAME_INCOMING           3962                5.46\n\n\n\n\n3.2 Missing Value Imputation Strategy\n\n# Strategy: Drop columns with &gt;50% missing values\nthreshold = len(df_cleaned) * 0.5\ncols_before = len(df_cleaned.columns)\ndf_cleaned = df_cleaned.dropna(thresh=threshold, axis=1)\ncols_after = len(df_cleaned.columns)\n\nprint(f\"Dropped {cols_before - cols_after} columns with &gt;50% missing values\")\n\n# Fill numerical columns with median\nnumerical_cols = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\nfor col in numerical_cols:\n    if df_cleaned[col].isnull().sum() &gt; 0:\n        median_val = df_cleaned[col].median()\n        df_cleaned[col].fillna(median_val, inplace=True)\n        print(f\"Filled '{col}' with median value: {median_val:.2f}\")\n\n# Fill categorical columns with \"Unknown\"\ncategorical_cols = df_cleaned.select_dtypes(include=['object']).columns.tolist()\nfor col in categorical_cols:\n    if df_cleaned[col].isnull().sum() &gt; 0:\n        df_cleaned[col].fillna(\"Unknown\", inplace=True)\n        print(f\"Filled '{col}' with 'Unknown'\")\n\nprint(f\"\\nTotal missing values after imputation: {df_cleaned.isnull().sum().sum()}\")\n\nDropped 6 columns with &gt;50% missing values\nFilled 'DURATION' with median value: 18.00\nFilled 'COMPANY' with median value: 37615159.00\nFilled 'MIN_EDULEVELS' with median value: 2.00\nFilled 'EMPLOYMENT_TYPE' with median value: 1.00\nFilled 'MIN_YEARS_EXPERIENCE' with median value: 5.00\nFilled 'REMOTE_TYPE' with median value: 0.00\nFilled 'STATE' with median value: 29.00\nFilled 'NAICS_2022_2' with median value: 54.00\nFilled 'NAICS_2022_3' with median value: 541.00\nFilled 'NAICS_2022_4' with median value: 5415.00\nFilled 'NAICS_2022_5' with median value: 54151.00\nFilled 'NAICS_2022_6' with median value: 541519.00\nFilled 'LAST_UPDATED_DATE' with 'Unknown'\nFilled 'POSTED' with 'Unknown'\nFilled 'EXPIRED' with 'Unknown'\nFilled 'SOURCE_TYPES' with 'Unknown'\nFilled 'SOURCES' with 'Unknown'\nFilled 'COMPANY_NAME' with 'Unknown'\nFilled 'COMPANY_IS_STAFFING' with 'Unknown'\nFilled 'EDUCATION_LEVELS' with 'Unknown'\nFilled 'EDUCATION_LEVELS_NAME' with 'Unknown'\nFilled 'MIN_EDULEVELS_NAME' with 'Unknown'\nFilled 'EMPLOYMENT_TYPE_NAME' with 'Unknown'\nFilled 'IS_INTERNSHIP' with 'Unknown'\nFilled 'REMOTE_TYPE_NAME' with 'Unknown'\nFilled 'LOCATION' with 'Unknown'\nFilled 'CITY' with 'Unknown'\nFilled 'CITY_NAME' with 'Unknown'\nFilled 'MSA_NAME' with 'Unknown'\nFilled 'STATE_NAME' with 'Unknown'\nFilled 'MSA_NAME_OUTGOING' with 'Unknown'\nFilled 'MSA_NAME_INCOMING' with 'Unknown'\nFilled 'TITLE' with 'Unknown'\nFilled 'TITLE_NAME' with 'Unknown'\nFilled 'TITLE_CLEAN' with 'Unknown'\nFilled 'SKILLS' with 'Unknown'\nFilled 'SKILLS_NAME' with 'Unknown'\nFilled 'SPECIALIZED_SKILLS' with 'Unknown'\nFilled 'SPECIALIZED_SKILLS_NAME' with 'Unknown'\nFilled 'CERTIFICATIONS' with 'Unknown'\nFilled 'CERTIFICATIONS_NAME' with 'Unknown'\nFilled 'COMMON_SKILLS' with 'Unknown'\nFilled 'COMMON_SKILLS_NAME' with 'Unknown'\nFilled 'SOFTWARE_SKILLS' with 'Unknown'\nFilled 'SOFTWARE_SKILLS_NAME' with 'Unknown'\nFilled 'ONET' with 'Unknown'\nFilled 'ONET_NAME' with 'Unknown'\nFilled 'CIP4' with 'Unknown'\nFilled 'CIP4_NAME' with 'Unknown'\nFilled 'SOC_2021_4' with 'Unknown'\nFilled 'SOC_2021_4_NAME' with 'Unknown'\nFilled 'NAICS_2022_2_NAME' with 'Unknown'\nFilled 'NAICS_2022_3_NAME' with 'Unknown'\nFilled 'NAICS_2022_4_NAME' with 'Unknown'\nFilled 'NAICS_2022_5_NAME' with 'Unknown'\nFilled 'NAICS_2022_6_NAME' with 'Unknown'\n\nTotal missing values after imputation: 0"
  },
  {
    "objectID": "data_analysis.html#step-3-removing-duplicates",
    "href": "data_analysis.html#step-3-removing-duplicates",
    "title": "Data Analysis",
    "section": "4 Step 3: Removing Duplicates",
    "text": "4 Step 3: Removing Duplicates\n\n4.1 Why Remove Duplicates?\nDuplicate job postings skew our analysis. We remove them based on job title, company, and location to ensure each unique position is counted only once.\n\nprint(f\"Duplicate records before removal: {df_cleaned.duplicated().sum():,}\")\n\n# Remove duplicates based on key identifiers\ndf_cleaned = df_cleaned.drop_duplicates(\n    subset=['TITLE_NAME', 'COMPANY_NAME', 'LOCATION'], \n    keep='first'\n)\n\nprint(f\"Duplicate records after removal: {df_cleaned.duplicated().sum():,}\")\nprint(f\"\\nFinal cleaned dataset shape: {df_cleaned.shape}\")\nprint(f\"Total records retained: {len(df_cleaned):,}\")\n\nDuplicate records before removal: 28\nDuplicate records after removal: 0\n\nFinal cleaned dataset shape: (59220, 56)\nTotal records retained: 59,220"
  },
  {
    "objectID": "data_analysis.html#step-4-exploratory-data-analysis-eda",
    "href": "data_analysis.html#step-4-exploratory-data-analysis-eda",
    "title": "Data Analysis",
    "section": "5 Step 4: Exploratory Data Analysis (EDA)",
    "text": "5 Step 4: Exploratory Data Analysis (EDA)\n\n5.1 Visualization 1: Top Job Titles\nUnderstanding which job titles are most in demand helps job seekers identify the skills and roles to target.\n\n# Get top 10 job titles\ntitle_counts = df_cleaned['TITLE_NAME'].value_counts().head(10)\n\nfig, ax = plt.subplots(figsize=(12, 6))\ntitle_counts.plot(kind='barh', ax=ax, color='steelblue')\nax.set_xlabel('Number of Job Postings', fontsize=12, fontweight='bold')\nax.set_ylabel('Job Title', fontsize=12, fontweight='bold')\nax.set_title('Top 10 Job Titles by Postings (2024)', fontsize=14, fontweight='bold')\nax.invert_yaxis()\nax.tick_params(left=False, bottom=False)\nfor i, v in enumerate(title_counts):\n    ax.text(v + 50, i, str(v), va='center', fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTop Job Titles by Postings:\")\nprint(title_counts)\nprint(f\"\\nJob Title Market Share (%):\")\nprint((title_counts / title_counts.sum() * 100).round(2))\n\n\n\n\n\n\n\n\n\nTop Job Titles by Postings:\nTITLE_NAME\nData Analysts                     6409\nUnclassified                      2405\nBusiness Intelligence Analysts    1624\nEnterprise Architects             1535\nData Modelers                      578\nData Governance Analysts           472\nOracle Cloud HCM Consultants       396\nSolutions Architects               377\nERP Business Analysts              369\nData Quality Analysts              359\nName: count, dtype: int64\n\nJob Title Market Share (%):\nTITLE_NAME\nData Analysts                     44.13\nUnclassified                      16.56\nBusiness Intelligence Analysts    11.18\nEnterprise Architects             10.57\nData Modelers                      3.98\nData Governance Analysts           3.25\nOracle Cloud HCM Consultants       2.73\nSolutions Architects               2.60\nERP Business Analysts              2.54\nData Quality Analysts              2.47\nName: count, dtype: float64\n\n\nKey Insight: The top job titles show where the job market is most active. This data helps identify roles with highest demand for new talent.\n\n\n5.2 Visualization 2: Job Postings by Employment Type\nEmployment type indicates the nature of positions available in the job market. Understanding the distribution helps job seekers target positions that match their preferences.\n\n# Count jobs by employment type\nemployment_counts = df_cleaned['EMPLOYMENT_TYPE_NAME'].value_counts()\n\nfig, ax = plt.subplots(figsize=(12, 6))\nemployment_counts.plot(kind='barh', ax=ax, color='coral')\nax.set_xlabel('Number of Job Postings', fontsize=12, fontweight='bold')\nax.set_ylabel('Employment Type', fontsize=12, fontweight='bold')\nax.set_title('Job Postings by Employment Type (2024)', fontsize=14, fontweight='bold')\nax.invert_yaxis()\nax.tick_params(left=False, bottom=False)\nfor i, v in enumerate(employment_counts):\n    ax.text(v + 50, i, str(v), va='center', fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nJob Postings by Employment Type:\")\nprint(employment_counts)\nprint(f\"\\nEmployment Type Distribution (%):\")\nprint((employment_counts / employment_counts.sum() * 100).round(2))\n\n\n\n\n\n\n\n\n\nJob Postings by Employment Type:\nEMPLOYMENT_TYPE_NAME\nFull-time (&gt; 32 hours)      56420\nPart-time (Ã¢â€°Â¤ 32 hours)     1977\nPart-time / full-time         822\nUnknown                         1\nName: count, dtype: int64\n\nEmployment Type Distribution (%):\nEMPLOYMENT_TYPE_NAME\nFull-time (&gt; 32 hours)      95.27\nPart-time (Ã¢â€°Â¤ 32 hours)     3.34\nPart-time / full-time        1.39\nUnknown                      0.00\nName: count, dtype: float64\n\n\nKey Insight: The distribution of employment types shows whether the market is primarily offering full-time, part-time, or contract positions, helping job seekers identify opportunities that match their work preferences.\n\n\n5.3 Visualization 3: Remote Work Distribution\nThe prevalence of remote work reflects post-pandemic hiring trends and work flexibility.\n\n# Count jobs by remote type\nremote_counts = df_cleaned['REMOTE_TYPE_NAME'].value_counts()\n\nfig, ax = plt.subplots(figsize=(10, 6))\ncolors = sns.color_palette(\"Set2\", len(remote_counts))\nwedges, texts, autotexts = ax.pie(remote_counts.values, \n                                    labels=remote_counts.index,\n                                    autopct='%1.1f%%',\n                                    colors=colors,\n                                    startangle=90)\nax.set_title(\"Job Market Distribution: Remote vs. On-Site (2024)\", fontsize=14, fontweight='bold')\nfor autotext in autotexts:\n    autotext.set_color('white')\n    autotext.set_fontweight('bold')\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nRemote Work Distribution:\")\nfor remote_type, count in remote_counts.items():\n    pct = (count / remote_counts.sum() * 100)\n    print(f\"{remote_type}: {count:,} jobs ({pct:.1f}%)\")\n\n\n\n\n\n\n\n\n\nRemote Work Distribution:\n[None]: 46,160 jobs (77.9%)\nRemote: 10,263 jobs (17.3%)\nHybrid Remote: 1,891 jobs (3.2%)\nNot Remote: 905 jobs (1.5%)\nUnknown: 1 jobs (0.0%)\n\n\nKey Insight: The balance between remote and on-site jobs shows employersâ€™ flexibility preferences. Higher remote job percentages indicate industry acceptance of distributed workforces."
  },
  {
    "objectID": "data_analysis.html#summary-of-data-cleaning-analysis",
    "href": "data_analysis.html#summary-of-data-cleaning-analysis",
    "title": "Data Analysis",
    "section": "6 Summary of Data Cleaning & Analysis",
    "text": "6 Summary of Data Cleaning & Analysis\n\nprint(f\"\\n{'='*60}\")\nprint(f\"DATA QUALITY SUMMARY\")\nprint(f\"{'='*60}\")\nprint(f\"Removed {len(columns_to_drop)} redundant columns\")\nprint(f\"Handled missing values with median/Unknown imputation\")\nprint(f\"Removed {df.shape[0] - df_cleaned.shape[0]:,} duplicate records\")\nprint(f\"Final dataset: {df_cleaned.shape[0]:,} records Ã— {df_cleaned.shape[1]} columns\")\nprint(f\"{'='*60}\")\n\n\n============================================================\nDATA QUALITY SUMMARY\n============================================================\nRemoved 69 redundant columns\nHandled missing values with median/Unknown imputation\nRemoved 13,278 duplicate records\nFinal dataset: 59,220 records Ã— 56 columns\n============================================================"
  },
  {
    "objectID": "data_analysis.html#references",
    "href": "data_analysis.html#references",
    "title": "Data Analysis",
    "section": "7 References",
    "text": "7 References"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Job Market Analysis 2024: Business Analytics, Data Science, and Machine Learning Trends",
    "section": "",
    "text": "This research explores the evolving landscape of Business Analytics, Data Science and Machine Learning (ML) careers in 2024. As organizations increasingly integrate AI in every function from operations to strategic decision-making, the demand for professionals with strong data and analytics skills is growing rapidly. This study explores four interconnected questions: the most sought-after skills for analytics and ML roles, how job descriptions are changing to require AI literacy, which industries are leading data-driven hiring, and what the short- to medium-term career outlook looks like for analytics professionals. By combining insights from labor market data and recent academic research, this project provides a holistic view of how AI and automation are transforming workforce expectations and employability strategies for students and professionals."
  },
  {
    "objectID": "index.html#project-overview",
    "href": "index.html#project-overview",
    "title": "Job Market Analysis 2024: Business Analytics, Data Science, and Machine Learning Trends",
    "section": "",
    "text": "This research explores the evolving landscape of Business Analytics, Data Science and Machine Learning (ML) careers in 2024. As organizations increasingly integrate AI in every function from operations to strategic decision-making, the demand for professionals with strong data and analytics skills is growing rapidly. This study explores four interconnected questions: the most sought-after skills for analytics and ML roles, how job descriptions are changing to require AI literacy, which industries are leading data-driven hiring, and what the short- to medium-term career outlook looks like for analytics professionals. By combining insights from labor market data and recent academic research, this project provides a holistic view of how AI and automation are transforming workforce expectations and employability strategies for students and professionals."
  },
  {
    "objectID": "index.html#data-source",
    "href": "index.html#data-source",
    "title": "Job Market Analysis 2024: Business Analytics, Data Science, and Machine Learning Trends",
    "section": "5.1 Data Source",
    "text": "5.1 Data Source\nWe will utilize the Lightcast 2024 dataset, which includes: - Job posting volumes for analytics, data science, and ML roles by industry and location - Salary data across data-related positions and geographies - Skill requirements extracted from job descriptions - Hiring trends across time periods - Company size and industry classifications - Emerging role titles and job requirements"
  },
  {
    "objectID": "index.html#analysis-approach",
    "href": "index.html#analysis-approach",
    "title": "Job Market Analysis 2024: Business Analytics, Data Science, and Machine Learning Trends",
    "section": "5.2 Analysis Approach",
    "text": "5.2 Analysis Approach\nOur team will:\n\nClean and preprocess the Lightcast data using Python (pandas, NumPy)\nExtract and categorize skills mentioned in job descriptions for analytics and ML roles\nCompare trends across industries, geographies, and job levels\nAnalyze salary patterns to understand compensation for different skill combinations\nIdentify emerging roles and how job requirements are evolving\nVisualize findings through interactive dashboards using Plotly and Matplotlib\nDevelop career strategy recommendations based on market insights"
  },
  {
    "objectID": "index.html#expected-findings",
    "href": "index.html#expected-findings",
    "title": "Job Market Analysis 2024: Business Analytics, Data Science, and Machine Learning Trends",
    "section": "5.3 Expected Findings",
    "text": "5.3 Expected Findings\nWe anticipate discovering that: - Python, SQL, and cloud platforms (AWS, GCP, Azure) remain foundational skills - Generative AI and prompt engineering are emerging as newly valued competencies - Finance, healthcare, and technology sectors lead in analytics hiring - Soft skills like communication and domain expertise are increasingly emphasized - Analytics roles offer strong job security and career growth potential - Salary premiums exist for ML/AI-specialized roles compared to traditional business analytics"
  },
  {
    "objectID": "index.html#deliverables-future-phases",
    "href": "index.html#deliverables-future-phases",
    "title": "Job Market Analysis 2024: Business Analytics, Data Science, and Machine Learning Trends",
    "section": "5.4 Deliverables (Future Phases)",
    "text": "5.4 Deliverables (Future Phases)\n\nExploratory Data Analysis (EDA) with visualizations\nInteractive dashboards showing skill trends, industry hiring patterns, and salary insights\nCareer pathway recommendations for different specializations\nPersonal career action plans for each team member"
  }
]