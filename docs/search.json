[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Job Market Analysis 2024: Business Analytics, Data Science, and Machine Learning Trends",
    "section": "",
    "text": "This research explores the evolving landscape of Business Analytics, Data Science and Machine Learning (ML) careers in 2024. As organizations increasingly integrate AI in every function from operations to strategic decision-making, the demand for professionals with strong data and analytics skills is growing rapidly. This study explores four interconnected questions: the most sought-after skills for analytics and ML roles, how job descriptions are changing to require AI literacy, which industries are leading data-driven hiring, and what the short- to medium-term career outlook looks like for analytics professionals. By combining insights from labor market data and recent academic research, this project provides a holistic view of how AI and automation are transforming workforce expectations and employability strategies for students and professionals."
  },
  {
    "objectID": "index.html#project-overview",
    "href": "index.html#project-overview",
    "title": "Job Market Analysis 2024: Business Analytics, Data Science, and Machine Learning Trends",
    "section": "",
    "text": "This research explores the evolving landscape of Business Analytics, Data Science and Machine Learning (ML) careers in 2024. As organizations increasingly integrate AI in every function from operations to strategic decision-making, the demand for professionals with strong data and analytics skills is growing rapidly. This study explores four interconnected questions: the most sought-after skills for analytics and ML roles, how job descriptions are changing to require AI literacy, which industries are leading data-driven hiring, and what the short- to medium-term career outlook looks like for analytics professionals. By combining insights from labor market data and recent academic research, this project provides a holistic view of how AI and automation are transforming workforce expectations and employability strategies for students and professionals."
  },
  {
    "objectID": "index.html#data-source",
    "href": "index.html#data-source",
    "title": "Job Market Analysis 2024: Business Analytics, Data Science, and Machine Learning Trends",
    "section": "Data Source",
    "text": "Data Source\nWe will utilize the Lightcast 2024 dataset, which includes: - Job posting volumes for analytics, data science, and ML roles by industry and location - Salary data across data-related positions and geographies - Skill requirements extracted from job descriptions - Hiring trends across time periods - Company size and industry classifications - Emerging role titles and job requirements"
  },
  {
    "objectID": "index.html#analysis-approach",
    "href": "index.html#analysis-approach",
    "title": "Job Market Analysis 2024: Business Analytics, Data Science, and Machine Learning Trends",
    "section": "Analysis Approach",
    "text": "Analysis Approach\nOur team will:\n\nClean and preprocess the Lightcast data using Python (pandas, NumPy)\nExtract and categorize skills mentioned in job descriptions for analytics and ML roles\nCompare trends across industries, geographies, and job levels\nAnalyze salary patterns to understand compensation for different skill combinations\nIdentify emerging roles and how job requirements are evolving\nVisualize findings through interactive dashboards using Plotly and Matplotlib\nDevelop career strategy recommendations based on market insights"
  },
  {
    "objectID": "index.html#expected-findings",
    "href": "index.html#expected-findings",
    "title": "Job Market Analysis 2024: Business Analytics, Data Science, and Machine Learning Trends",
    "section": "Expected Findings",
    "text": "Expected Findings\nWe anticipate discovering that: - Python, SQL, and cloud platforms (AWS, GCP, Azure) remain foundational skills - Generative AI and prompt engineering are emerging as newly valued competencies - Finance, healthcare, and technology sectors lead in analytics hiring - Soft skills like communication and domain expertise are increasingly emphasized - Analytics roles offer strong job security and career growth potential - Salary premiums exist for ML/AI-specialized roles compared to traditional business analytics"
  },
  {
    "objectID": "index.html#deliverables-future-phases",
    "href": "index.html#deliverables-future-phases",
    "title": "Job Market Analysis 2024: Business Analytics, Data Science, and Machine Learning Trends",
    "section": "Deliverables (Future Phases)",
    "text": "Deliverables (Future Phases)\n\nExploratory Data Analysis (EDA) with visualizations\nInteractive dashboards showing skill trends, industry hiring patterns, and salary insights\nCareer pathway recommendations for different specializations\nPersonal career action plans for each team member"
  },
  {
    "objectID": "data_analysis.html",
    "href": "data_analysis.html",
    "title": "Data Analysis",
    "section": "",
    "text": "This document provides a comprehensive analysis of job market data for 2024. We perform data cleaning, handle missing values, remove duplicates, and conduct exploratory data analysis to understand hiring trends, salary distributions, and job market dynamics.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set style for better visualizations\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# Load your data\ndf = pd.read_csv('lightcast_job_postings.csv')\nprint(f\"Original dataset shape: {df.shape}\")\nprint(f\"Total records: {len(df):,}\")\nprint(f\"Total columns: {len(df.columns)}\")\n\nOriginal dataset shape: (72498, 131)\nTotal records: 72,498\nTotal columns: 131"
  },
  {
    "objectID": "data_analysis.html#introduction",
    "href": "data_analysis.html#introduction",
    "title": "Data Analysis",
    "section": "",
    "text": "This document provides a comprehensive analysis of job market data for 2024. We perform data cleaning, handle missing values, remove duplicates, and conduct exploratory data analysis to understand hiring trends, salary distributions, and job market dynamics.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set style for better visualizations\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 6)\n\n# Load your data\ndf = pd.read_csv('lightcast_job_postings.csv')\nprint(f\"Original dataset shape: {df.shape}\")\nprint(f\"Total records: {len(df):,}\")\nprint(f\"Total columns: {len(df.columns)}\")\n\nOriginal dataset shape: (72498, 131)\nTotal records: 72,498\nTotal columns: 131"
  },
  {
    "objectID": "data_analysis.html#step-1-removing-redundant-columns",
    "href": "data_analysis.html#step-1-removing-redundant-columns",
    "title": "Data Analysis",
    "section": "2 Step 1: Removing Redundant Columns",
    "text": "2 Step 1: Removing Redundant Columns\n\n2.1 Why Remove These Columns?\nWe remove redundant columns to improve dataset quality and analysis efficiency. Specifically:\n\nTracking & Administrative Columns: ID, URL, DUPLICATES, LAST_UPDATED_TIMESTAMP - These are metadata and don’t contribute to analysis\nRaw Text Fields: BODY, TITLE_RAW, COMPANY_RAW - We have cleaned versions (TITLE, COMPANY_NAME, TITLE_CLEAN)\nDeprecated Classifications: We keep only the latest NAICS_2022_6, SOC_2021_4, and CIP4 standards\nDuplicate Geographic Fields: Multiple versions of county/MSA data create redundancy\n\n\n# List of columns to drop\ncolumns_to_drop = [\n    # Administrative & tracking columns\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n    # Raw text columns (we have cleaned versions)\n    \"BODY\", \"TITLE_RAW\", \"COMPANY_RAW\", \"ACTIVE_SOURCES_INFO\",\n    # Deprecated NAICS versions (keeping NAICS_2022_6)\n    \"NAICS2\", \"NAICS2_NAME\", \"NAICS3\", \"NAICS3_NAME\", \n    \"NAICS4\", \"NAICS4_NAME\", \"NAICS5\", \"NAICS5_NAME\", \n    \"NAICS6\", \"NAICS6_NAME\",\n    # Deprecated SOC versions (keeping SOC_2021_4)\n    \"SOC_2\", \"SOC_2_NAME\", \"SOC_3\", \"SOC_3_NAME\", \n    \"SOC_4\", \"SOC_4_NAME\", \"SOC_5\", \"SOC_5_NAME\",\n    \"SOC_2021_2\", \"SOC_2021_2_NAME\", \"SOC_2021_3\", \"SOC_2021_3_NAME\", \n    \"SOC_2021_5\", \"SOC_2021_5_NAME\",\n    # Deprecated occupation classifications\n    \"LOT_CAREER_AREA\", \"LOT_CAREER_AREA_NAME\", \"LOT_OCCUPATION\", \"LOT_OCCUPATION_NAME\",\n    \"LOT_SPECIALIZED_OCCUPATION\", \"LOT_SPECIALIZED_OCCUPATION_NAME\", \n    \"LOT_OCCUPATION_GROUP\", \"LOT_OCCUPATION_GROUP_NAME\",\n    \"LOT_V6_SPECIALIZED_OCCUPATION\", \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\", \n    \"LOT_V6_OCCUPATION\", \"LOT_V6_OCCUPATION_NAME\",\n    \"LOT_V6_OCCUPATION_GROUP\", \"LOT_V6_OCCUPATION_GROUP_NAME\", \n    \"LOT_V6_CAREER_AREA\", \"LOT_V6_CAREER_AREA_NAME\",\n    # Deprecated CIP and ONET versions\n    \"ONET_2019\", \"ONET_2019_NAME\", \"CIP6\", \"CIP6_NAME\", \"CIP2\", \"CIP2_NAME\",\n    # Duplicate geographic fields\n    \"COUNTY\", \"COUNTY_NAME\", \"COUNTY_OUTGOING\", \"COUNTY_NAME_OUTGOING\", \n    \"COUNTY_INCOMING\", \"COUNTY_NAME_INCOMING\",\n    \"MSA\", \"MSA_OUTGOING\", \"MSA_INCOMING\",\n    # Deprecated salary fields (keeping SALARY)\n    \"SALARY_TO\", \"SALARY_FROM\", \"ORIGINAL_PAY_PERIOD\",\n    # Model versions (keep actual data)\n    \"MODELED_EXPIRED\", \"MODELED_DURATION\"\n]\n\n# Drop only columns that exist in the dataset\ncolumns_to_drop = [col for col in columns_to_drop if col in df.columns]\ndf_cleaned = df.drop(columns=columns_to_drop, inplace=False)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"DATA CLEANING SUMMARY\")\nprint(f\"{'='*60}\")\nprint(f\"Original dataset shape: {df.shape}\")\nprint(f\"Cleaned dataset shape: {df_cleaned.shape}\")\nprint(f\"Columns removed: {len(columns_to_drop)}\")\nprint(f\"Columns retained: {len(df_cleaned.columns)}\")\nprint(f\"\\nRemaining key columns:\\n{', '.join(df_cleaned.columns[:15])}\")\n\n\n============================================================\nDATA CLEANING SUMMARY\n============================================================\nOriginal dataset shape: (72498, 131)\nCleaned dataset shape: (72498, 62)\nColumns removed: 69\nColumns retained: 62\n\nRemaining key columns:\nLAST_UPDATED_DATE, POSTED, EXPIRED, DURATION, SOURCE_TYPES, SOURCES, COMPANY, COMPANY_NAME, COMPANY_IS_STAFFING, EDUCATION_LEVELS, EDUCATION_LEVELS_NAME, MIN_EDULEVELS, MIN_EDULEVELS_NAME, MAX_EDULEVELS, MAX_EDULEVELS_NAME"
  },
  {
    "objectID": "data_analysis.html#step-2-handling-missing-values",
    "href": "data_analysis.html#step-2-handling-missing-values",
    "title": "Data Analysis",
    "section": "3 Step 2: Handling Missing Values",
    "text": "3 Step 2: Handling Missing Values\n\n3.1 Understanding Missing Data\nBefore imputation, let’s visualize where data is missing:\n\n# Create a heatmap of missing values\nfig, ax = plt.subplots(figsize=(14, 6))\nmsno.heatmap(df_cleaned, ax=ax)\nplt.title(\"Missing Values Heatmap - Lightcast Job Market Data\", fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Print detailed missing value statistics\nmissing_stats = pd.DataFrame({\n    'Column': df_cleaned.columns,\n    'Missing_Count': df_cleaned.isnull().sum().values,\n    'Missing_Percentage': (df_cleaned.isnull().sum().values / len(df_cleaned) * 100).round(2)\n})\nmissing_stats = missing_stats[missing_stats['Missing_Count'] &gt; 0].sort_values('Missing_Percentage', ascending=False)\n\nprint(\"\\nMissing Value Statistics:\")\nprint(missing_stats.head(10).to_string(index=False))\n\n\n\n\n\n\n\n\n\nMissing Value Statistics:\n                Column  Missing_Count  Missing_Percentage\n  MAX_YEARS_EXPERIENCE          64068               88.37\n         MAX_EDULEVELS          56183               77.50\n    MAX_EDULEVELS_NAME          56183               77.50\n     LIGHTCAST_SECTORS          54711               75.47\nLIGHTCAST_SECTORS_NAME          54711               75.47\n                SALARY          41690               57.51\n              DURATION          27316               37.68\n  MIN_YEARS_EXPERIENCE          23146               31.93\n               EXPIRED           7844               10.82\n     MSA_NAME_INCOMING           3962                5.46\n\n\n\n\n3.2 Missing Value Imputation Strategy\n\n# Strategy: Drop columns with &gt;50% missing values\nthreshold = len(df_cleaned) * 0.5\ncols_before = len(df_cleaned.columns)\ndf_cleaned = df_cleaned.dropna(thresh=threshold, axis=1)\ncols_after = len(df_cleaned.columns)\n\nprint(f\"Dropped {cols_before - cols_after} columns with &gt;50% missing values\")\n\n# Fill numerical columns with median\nnumerical_cols = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\nfor col in numerical_cols:\n    if df_cleaned[col].isnull().sum() &gt; 0:\n        median_val = df_cleaned[col].median()\n        df_cleaned[col].fillna(median_val, inplace=True)\n        print(f\"Filled '{col}' with median value: {median_val:.2f}\")\n\n# Fill categorical columns with \"Unknown\"\ncategorical_cols = df_cleaned.select_dtypes(include=['object']).columns.tolist()\nfor col in categorical_cols:\n    if df_cleaned[col].isnull().sum() &gt; 0:\n        df_cleaned[col].fillna(\"Unknown\", inplace=True)\n        print(f\"Filled '{col}' with 'Unknown'\")\n\nprint(f\"\\nTotal missing values after imputation: {df_cleaned.isnull().sum().sum()}\")\n\nDropped 6 columns with &gt;50% missing values\nFilled 'DURATION' with median value: 18.00\nFilled 'COMPANY' with median value: 37615159.00\nFilled 'MIN_EDULEVELS' with median value: 2.00\nFilled 'EMPLOYMENT_TYPE' with median value: 1.00\nFilled 'MIN_YEARS_EXPERIENCE' with median value: 5.00\nFilled 'REMOTE_TYPE' with median value: 0.00\nFilled 'STATE' with median value: 29.00\nFilled 'NAICS_2022_2' with median value: 54.00\nFilled 'NAICS_2022_3' with median value: 541.00\nFilled 'NAICS_2022_4' with median value: 5415.00\nFilled 'NAICS_2022_5' with median value: 54151.00\nFilled 'NAICS_2022_6' with median value: 541519.00\nFilled 'LAST_UPDATED_DATE' with 'Unknown'\nFilled 'POSTED' with 'Unknown'\nFilled 'EXPIRED' with 'Unknown'\nFilled 'SOURCE_TYPES' with 'Unknown'\nFilled 'SOURCES' with 'Unknown'\nFilled 'COMPANY_NAME' with 'Unknown'\nFilled 'COMPANY_IS_STAFFING' with 'Unknown'\nFilled 'EDUCATION_LEVELS' with 'Unknown'\nFilled 'EDUCATION_LEVELS_NAME' with 'Unknown'\nFilled 'MIN_EDULEVELS_NAME' with 'Unknown'\nFilled 'EMPLOYMENT_TYPE_NAME' with 'Unknown'\nFilled 'IS_INTERNSHIP' with 'Unknown'\nFilled 'REMOTE_TYPE_NAME' with 'Unknown'\nFilled 'LOCATION' with 'Unknown'\nFilled 'CITY' with 'Unknown'\nFilled 'CITY_NAME' with 'Unknown'\nFilled 'MSA_NAME' with 'Unknown'\nFilled 'STATE_NAME' with 'Unknown'\nFilled 'MSA_NAME_OUTGOING' with 'Unknown'\nFilled 'MSA_NAME_INCOMING' with 'Unknown'\nFilled 'TITLE' with 'Unknown'\nFilled 'TITLE_NAME' with 'Unknown'\nFilled 'TITLE_CLEAN' with 'Unknown'\nFilled 'SKILLS' with 'Unknown'\nFilled 'SKILLS_NAME' with 'Unknown'\nFilled 'SPECIALIZED_SKILLS' with 'Unknown'\nFilled 'SPECIALIZED_SKILLS_NAME' with 'Unknown'\nFilled 'CERTIFICATIONS' with 'Unknown'\nFilled 'CERTIFICATIONS_NAME' with 'Unknown'\nFilled 'COMMON_SKILLS' with 'Unknown'\nFilled 'COMMON_SKILLS_NAME' with 'Unknown'\nFilled 'SOFTWARE_SKILLS' with 'Unknown'\nFilled 'SOFTWARE_SKILLS_NAME' with 'Unknown'\nFilled 'ONET' with 'Unknown'\nFilled 'ONET_NAME' with 'Unknown'\nFilled 'CIP4' with 'Unknown'\nFilled 'CIP4_NAME' with 'Unknown'\nFilled 'SOC_2021_4' with 'Unknown'\nFilled 'SOC_2021_4_NAME' with 'Unknown'\nFilled 'NAICS_2022_2_NAME' with 'Unknown'\nFilled 'NAICS_2022_3_NAME' with 'Unknown'\nFilled 'NAICS_2022_4_NAME' with 'Unknown'\nFilled 'NAICS_2022_5_NAME' with 'Unknown'\nFilled 'NAICS_2022_6_NAME' with 'Unknown'\n\nTotal missing values after imputation: 0"
  },
  {
    "objectID": "data_analysis.html#step-3-removing-duplicates",
    "href": "data_analysis.html#step-3-removing-duplicates",
    "title": "Data Analysis",
    "section": "4 Step 3: Removing Duplicates",
    "text": "4 Step 3: Removing Duplicates\n\n4.1 Why Remove Duplicates?\nDuplicate job postings skew our analysis. We remove them based on job title, company, and location to ensure each unique position is counted only once.\n\nprint(f\"Duplicate records before removal: {df_cleaned.duplicated().sum():,}\")\n\n# Remove duplicates based on key identifiers\ndf_cleaned = df_cleaned.drop_duplicates(\n    subset=['TITLE_NAME', 'COMPANY_NAME', 'LOCATION'], \n    keep='first'\n)\n\nprint(f\"Duplicate records after removal: {df_cleaned.duplicated().sum():,}\")\nprint(f\"\\nFinal cleaned dataset shape: {df_cleaned.shape}\")\nprint(f\"Total records retained: {len(df_cleaned):,}\")\n\nDuplicate records before removal: 28\nDuplicate records after removal: 0\n\nFinal cleaned dataset shape: (59220, 56)\nTotal records retained: 59,220"
  },
  {
    "objectID": "data_analysis.html#step-4-exploratory-data-analysis-eda",
    "href": "data_analysis.html#step-4-exploratory-data-analysis-eda",
    "title": "Data Analysis",
    "section": "5 Step 4: Exploratory Data Analysis (EDA)",
    "text": "5 Step 4: Exploratory Data Analysis (EDA)\n\n5.1 Visualization 1: Top Job Titles\nUnderstanding which job titles are most in demand helps job seekers identify the skills and roles to target.\n\n# Get top 10 job titles\ntitle_counts = df_cleaned['TITLE_NAME'].value_counts().head(10)\n\nfig, ax = plt.subplots(figsize=(12, 6))\ntitle_counts.plot(kind='barh', ax=ax, color='steelblue')\nax.set_xlabel('Number of Job Postings', fontsize=12, fontweight='bold')\nax.set_ylabel('Job Title', fontsize=12, fontweight='bold')\nax.set_title('Top 10 Job Titles by Postings (2024)', fontsize=14, fontweight='bold')\nax.invert_yaxis()\nax.tick_params(left=False, bottom=False)\nfor i, v in enumerate(title_counts):\n    ax.text(v + 50, i, str(v), va='center', fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nTop Job Titles by Postings:\")\nprint(title_counts)\nprint(f\"\\nJob Title Market Share (%):\")\nprint((title_counts / title_counts.sum() * 100).round(2))\n\n\n\n\n\n\n\n\n\nTop Job Titles by Postings:\nTITLE_NAME\nData Analysts                     6409\nUnclassified                      2405\nBusiness Intelligence Analysts    1624\nEnterprise Architects             1535\nData Modelers                      578\nData Governance Analysts           472\nOracle Cloud HCM Consultants       396\nSolutions Architects               377\nERP Business Analysts              369\nData Quality Analysts              359\nName: count, dtype: int64\n\nJob Title Market Share (%):\nTITLE_NAME\nData Analysts                     44.13\nUnclassified                      16.56\nBusiness Intelligence Analysts    11.18\nEnterprise Architects             10.57\nData Modelers                      3.98\nData Governance Analysts           3.25\nOracle Cloud HCM Consultants       2.73\nSolutions Architects               2.60\nERP Business Analysts              2.54\nData Quality Analysts              2.47\nName: count, dtype: float64\n\n\nKey Insight: The top job titles show where the job market is most active. This data helps identify roles with highest demand for new talent.\n\n\n5.2 Visualization 2: Job Postings by Employment Type\nEmployment type indicates the nature of positions available in the job market. Understanding the distribution helps job seekers target positions that match their preferences.\n\n# Count jobs by employment type\nemployment_counts = df_cleaned['EMPLOYMENT_TYPE_NAME'].value_counts()\n\nfig, ax = plt.subplots(figsize=(12, 6))\nemployment_counts.plot(kind='barh', ax=ax, color='coral')\nax.set_xlabel('Number of Job Postings', fontsize=12, fontweight='bold')\nax.set_ylabel('Employment Type', fontsize=12, fontweight='bold')\nax.set_title('Job Postings by Employment Type (2024)', fontsize=14, fontweight='bold')\nax.invert_yaxis()\nax.tick_params(left=False, bottom=False)\nfor i, v in enumerate(employment_counts):\n    ax.text(v + 50, i, str(v), va='center', fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nJob Postings by Employment Type:\")\nprint(employment_counts)\nprint(f\"\\nEmployment Type Distribution (%):\")\nprint((employment_counts / employment_counts.sum() * 100).round(2))\n\n\n\n\n\n\n\n\n\nJob Postings by Employment Type:\nEMPLOYMENT_TYPE_NAME\nFull-time (&gt; 32 hours)      56420\nPart-time (â‰¤ 32 hours)     1977\nPart-time / full-time         822\nUnknown                         1\nName: count, dtype: int64\n\nEmployment Type Distribution (%):\nEMPLOYMENT_TYPE_NAME\nFull-time (&gt; 32 hours)      95.27\nPart-time (â‰¤ 32 hours)     3.34\nPart-time / full-time        1.39\nUnknown                      0.00\nName: count, dtype: float64\n\n\nKey Insight: The distribution of employment types shows whether the market is primarily offering full-time, part-time, or contract positions, helping job seekers identify opportunities that match their work preferences.\n\n\n5.3 Visualization 3: Remote Work Distribution\nThe prevalence of remote work reflects post-pandemic hiring trends and work flexibility.\n\n# Count jobs by remote type\nremote_counts = df_cleaned['REMOTE_TYPE_NAME'].value_counts()\n\nfig, ax = plt.subplots(figsize=(10, 6))\ncolors = sns.color_palette(\"Set2\", len(remote_counts))\nwedges, texts, autotexts = ax.pie(remote_counts.values, \n                                    labels=remote_counts.index,\n                                    autopct='%1.1f%%',\n                                    colors=colors,\n                                    startangle=90)\nax.set_title(\"Job Market Distribution: Remote vs. On-Site (2024)\", fontsize=14, fontweight='bold')\nfor autotext in autotexts:\n    autotext.set_color('white')\n    autotext.set_fontweight('bold')\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nRemote Work Distribution:\")\nfor remote_type, count in remote_counts.items():\n    pct = (count / remote_counts.sum() * 100)\n    print(f\"{remote_type}: {count:,} jobs ({pct:.1f}%)\")\n\n\n\n\n\n\n\n\n\nRemote Work Distribution:\n[None]: 46,160 jobs (77.9%)\nRemote: 10,263 jobs (17.3%)\nHybrid Remote: 1,891 jobs (3.2%)\nNot Remote: 905 jobs (1.5%)\nUnknown: 1 jobs (0.0%)\n\n\nKey Insight: The balance between remote and on-site jobs shows employers’ flexibility preferences. Higher remote job percentages indicate industry acceptance of distributed workforces."
  },
  {
    "objectID": "data_analysis.html#summary-of-data-cleaning-analysis",
    "href": "data_analysis.html#summary-of-data-cleaning-analysis",
    "title": "Data Analysis",
    "section": "6 Summary of Data Cleaning & Analysis",
    "text": "6 Summary of Data Cleaning & Analysis\n\nprint(f\"\\n{'='*60}\")\nprint(f\"DATA QUALITY SUMMARY\")\nprint(f\"{'='*60}\")\nprint(f\"Removed {len(columns_to_drop)} redundant columns\")\nprint(f\"Handled missing values with median/Unknown imputation\")\nprint(f\"Removed {df.shape[0] - df_cleaned.shape[0]:,} duplicate records\")\nprint(f\"Final dataset: {df_cleaned.shape[0]:,} records × {df_cleaned.shape[1]} columns\")\nprint(f\"{'='*60}\")\n\n\n============================================================\nDATA QUALITY SUMMARY\n============================================================\nRemoved 69 redundant columns\nHandled missing values with median/Unknown imputation\nRemoved 13,278 duplicate records\nFinal dataset: 59,220 records × 56 columns\n============================================================"
  },
  {
    "objectID": "data_analysis.html#references",
    "href": "data_analysis.html#references",
    "title": "Data Analysis",
    "section": "7 References",
    "text": "7 References"
  }
]