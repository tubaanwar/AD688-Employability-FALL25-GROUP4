---
title: "Natural Language Processing Methods"
subtitle: "Extracting Insights from Job Descriptions"
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
    code-overflow: wrap
    embed-resources: true
    css: styles.css
jupyter: python3
execute:
  echo: false
  warning: false
  message: false
---

## Introduction

This section applies Natural Language Processing (NLP) techniques to analyze job postings and extract meaningful insights about skills, requirements, and industry trends. By processing structured skills data, we can uncover patterns about the most in-demand competencies.

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
import warnings
warnings.filterwarnings('ignore')

# Set seaborn style
sns.set_style("whitegrid")
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (12, 6)

# Load data
df = pd.read_csv('cleanedjob_postings.csv')

print(f"Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns")

# Identify skills columns
skills_columns = [col for col in df.columns if 'SKILLS_NAME' in col or 'SKILLS' in col]
print(f"Skills columns found: {skills_columns}")
```

## 1. Data Preparation

### 1.1 Skills Data Overview

```{python}
# Combine all skills into a single text field for analysis
def combine_skills(row):
    """Combine all skills columns into a single text"""
    skills = []
    for col in ['SKILLS_NAME', 'SPECIALIZED_SKILLS_NAME', 'COMMON_SKILLS_NAME', 'SOFTWARE_SKILLS_NAME']:
        if col in df.columns and pd.notna(row.get(col)):
            skills.append(str(row[col]))
    return ' | '.join(skills)

df['all_skills'] = df.apply(combine_skills, axis=1)
df_skills = df[df['all_skills'].str.len() > 0].copy()

print(f"✓ Successfully processed: {len(df_skills):,} job postings with skills")
print(f"✓ Average skills text length: {df_skills['all_skills'].str.len().mean():.0f} characters")

# Show sample
if len(df_skills) > 0:
    print(f"\n✓ Sample skills: {df_skills['all_skills'].iloc[0][:200]}...")
```

## 2. Keyword Extraction

### 2.1 Top 10 Most Common Skills

```{python}
# Extract most common skill keywords (excluding the extreme outlier)
if len(df_skills) > 0:
    all_skills = []
    
    for skills_text in df_skills['all_skills']:
        # Split by pipe separator and clean thoroughly
        skills = [s.strip() for s in str(skills_text).split('|')]
        # Remove quotes, brackets, and clean each skill
        cleaned_skills = []
        for skill in skills:
            clean = skill.replace('"', '').replace("'", "").replace('[', '').replace(']', '')
            clean = clean.replace('(', '').replace(')', '').strip().lower()
            if len(clean) > 2:  # Only keep skills with more than 2 characters
                cleaned_skills.append(clean)
        all_skills.extend(cleaned_skills)
    
    # Remove empty strings
    all_skills = [s for s in all_skills if len(s) > 0]
    
    if len(all_skills) > 0:
        skill_freq = Counter(all_skills)
        
        # Get top 11 skills, then exclude the top one if it's an extreme outlier
        top_skills_raw = skill_freq.most_common(11)
        
        # Check if the top skill is an extreme outlier (>10x the second)
        if len(top_skills_raw) > 1 and top_skills_raw[0][1] > 10 * top_skills_raw[1][1]:
            print(f"Note: Excluded extreme outlier '{top_skills_raw[0][0]}' with {top_skills_raw[0][1]:,} occurrences")
            top_skills = top_skills_raw[1:11]  # Skip the first, take next 10
        else:
            top_skills = top_skills_raw[:10]
        
        skills, counts = zip(*top_skills)
        
        # Create dataframe for seaborn
        skill_df = pd.DataFrame({'Skill': skills, 'Frequency': counts})
        skill_df = skill_df.sort_values('Frequency')
        
        # Clean up skill names - remove quotes and brackets, capitalize properly
        skill_df['Skill'] = skill_df['Skill'].str.replace('"', '').str.replace("'", "")
        skill_df['Skill'] = skill_df['Skill'].str.replace(r'[\[\]\(\)]', '', regex=True)
        skill_df['Skill'] = skill_df['Skill'].str.strip()
        
        # Split combined labels - if comma-separated, take first item only
        skill_df['Skill'] = skill_df['Skill'].apply(lambda x: x.split(',')[0].strip() if ',' in x else x)
        skill_df['Skill'] = skill_df['Skill'].str.title()
        
        fig, ax = plt.subplots(figsize=(10, 6))
        bars = sns.barplot(data=skill_df, y='Skill', x='Frequency', palette='Blues_r', ax=ax)
        ax.set_title('Top 10 Most Common Skills in Job Postings', fontsize=16, fontweight='bold', pad=20)
        ax.set_xlabel('Frequency', fontsize=12, fontweight='bold')
        ax.set_ylabel('Skill', fontsize=12, fontweight='bold')
        
        # Add value labels
        for i, v in enumerate(skill_df['Frequency']):
            ax.text(v + max(skill_df['Frequency'])*0.02, i, f'{int(v):,}', 
                   va='center', fontweight='bold', fontsize=10)
        
        # Improve layout
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        plt.tight_layout()
        plt.show()
    else:
        print("No skills found")
```

**Top 10 Skills Insight:**

SAP Applications dominates with 1,114 postings, reflecting the strong demand for enterprise resource planning expertise in large organizations. Oracle Cloud (667) and Microsoft Office (637) follow, showing that cloud platforms and productivity tools remain essential. Communication (532) appears as the top soft skill, emphasizing that technical roles require strong interpersonal abilities. The data reveals a mix of enterprise software (SAP, Oracle Cloud), business analysis capabilities (Data Analysis, Dashboard, Project Management), and emerging areas like Cyber Security and UX Design. Job seekers should prioritize SAP expertise if targeting enterprise roles, while building a foundation in Microsoft Office, data analysis, and communication skills for broader market appeal.

## 3. Technical Skills Analysis

### 3.1 Top 10 Software & Technical Skills

```{python}
# Focus on software/technical skills column
if 'SOFTWARE_SKILLS_NAME' in df.columns:
    software_skills = []
    
    for skills_text in df['SOFTWARE_SKILLS_NAME'].dropna():
        skills = [s.strip() for s in str(skills_text).split('|')]
        # Clean each skill thoroughly
        for skill in skills:
            clean = skill.replace('"', '').replace("'", "").replace('[', '').replace(']', '')
            clean = clean.replace('(', '').replace(')', '').strip().lower()
            if len(clean) > 2:
                software_skills.append(clean)
    
    software_skills = [s for s in software_skills if len(s) > 0]
    
    if len(software_skills) > 0:
        software_freq = Counter(software_skills)
        
        # Get top 11, check for outlier
        top_software_raw = software_freq.most_common(11)
        
        if len(top_software_raw) > 1 and top_software_raw[0][1] > 10 * top_software_raw[1][1]:
            print(f"Note: Excluded extreme outlier '{top_software_raw[0][0]}' with {top_software_raw[0][1]:,} occurrences")
            top_software = top_software_raw[1:11]
        else:
            top_software = top_software_raw[:10]
        
        software, counts = zip(*top_software)
        
        software_df = pd.DataFrame({'Software': software, 'Count': counts})
        software_df = software_df.sort_values('Count')
        
        # Clean software names - remove quotes, brackets, parentheses
        software_df['Software'] = software_df['Software'].str.replace('"', '').str.replace("'", "")
        software_df['Software'] = software_df['Software'].str.replace(r'[\[\]\(\)]', '', regex=True)
        software_df['Software'] = software_df['Software'].str.strip()
        
        # Split combined labels - if comma-separated, take first item only
        software_df['Software'] = software_df['Software'].apply(lambda x: x.split(',')[0].strip() if ',' in x else x)
        software_df['Software'] = software_df['Software'].str.title()
        
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.barplot(data=software_df, y='Software', x='Count', palette='viridis_r', ax=ax)
        ax.set_title('Top 10 Software Skills in Job Postings', fontsize=16, fontweight='bold', pad=20)
        ax.set_xlabel('Number of Job Postings', fontsize=12, fontweight='bold')
        ax.set_ylabel('Software Skill', fontsize=12, fontweight='bold')
        
        # Add value labels at the end of each bar
        max_val = software_df['Count'].max()
        for i, v in enumerate(software_df['Count']):
            ax.text(v + max_val*0.01, i, f'{int(v):,}', 
                   va='center', fontweight='bold', fontsize=10)
        
        # Set x-axis limit to prevent cutoff
        ax.set_xlim(0, max_val * 1.15)
        
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        plt.tight_layout()
        plt.show()
```

**Top 10 Software Skills - Job Seeker Insights:**

SAP Applications leads dramatically with 934 postings, making it the single most valuable software skill for enterprise-focused careers. Oracle Cloud (633) and Microsoft Office (602) demonstrate the importance of cloud infrastructure and productivity suites. Dashboard skills (433) reflect the growing need for data visualization capabilities across all roles. SQL Programming Language (353) and Microsoft Excel (282) remain fundamental for data manipulation and analysis. The presence of specialized tools like Anaplan (196), Onestream CPM Software (196), and Oracle E-Business Suite (173) indicates niche opportunities in financial planning and enterprise systems. Job seekers should master SAP for enterprise roles, Excel and SQL for data work, and consider specializing in emerging tools like Anaplan for competitive advantage.

## 4. Specialized Skills Analysis

### 4.1 Top 10 Specialized Skills

```{python}
# Focus on specialized skills column
if 'SPECIALIZED_SKILLS_NAME' in df.columns:
    specialized_skills = []
    
    for skills_text in df['SPECIALIZED_SKILLS_NAME'].dropna():
        skills = [s.strip() for s in str(skills_text).split('|')]
        # Clean each skill thoroughly
        for skill in skills:
            clean = skill.replace('"', '').replace("'", "").replace('[', '').replace(']', '')
            clean = clean.replace('(', '').replace(')', '').strip().lower()
            if len(clean) > 2:
                specialized_skills.append(clean)
    
    specialized_skills = [s for s in specialized_skills if len(s) > 0]
    
    if len(specialized_skills) > 0:
        spec_freq = Counter(specialized_skills)
        top_spec = spec_freq.most_common(10)
        
        spec, counts = zip(*top_spec)
        
        spec_df = pd.DataFrame({'Skill': spec, 'Count': counts})
        spec_df = spec_df.sort_values('Count')
        
        # Clean skill names - remove quotes, brackets, parentheses
        spec_df['Skill'] = spec_df['Skill'].str.replace('"', '').str.replace("'", "")
        spec_df['Skill'] = spec_df['Skill'].str.replace(r'[\[\]\(\)]', '', regex=True)
        spec_df['Skill'] = spec_df['Skill'].str.strip()
        
        # Split combined labels - if comma-separated, take first item only
        spec_df['Skill'] = spec_df['Skill'].apply(lambda x: x.split(',')[0].strip() if ',' in x else x)
        
        # Truncate long skill names to prevent overlap
        spec_df['Skill'] = spec_df['Skill'].apply(lambda x: x[:35] + '...' if len(x) > 35 else x)
        spec_df['Skill'] = spec_df['Skill'].str.title()
        
        fig, ax = plt.subplots(figsize=(10, 7))
        sns.barplot(data=spec_df, y='Skill', x='Count', palette='rocket_r', ax=ax)
        ax.set_title('Top 10 Specialized Skills in Job Postings', fontsize=16, fontweight='bold', pad=20)
        ax.set_xlabel('Number of Job Postings', fontsize=12, fontweight='bold')
        ax.set_ylabel('Specialized Skill', fontsize=12, fontweight='bold')
        
        # Add value labels
        max_val = spec_df['Count'].max()
        for i, v in enumerate(spec_df['Count']):
            ax.text(v + max_val*0.01, i, f'{int(v):,}', 
                   va='center', fontweight='bold', fontsize=10)
        
        # Set x-axis limit
        ax.set_xlim(0, max_val * 1.15)
        
        # Adjust y-axis labels to prevent overlap
        ax.tick_params(axis='y', labelsize=10)
        plt.yticks(rotation=0)
        
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        plt.tight_layout()
        plt.show()
```

**Top 10 Specialized Skills - Job Seeker Insights:**

User Experience (UX) Design leads with 260 postings, highlighting the critical importance of user-centered design in modern product development. Data Analysis (220) and Cloud Computing (196) show strong demand for analytical and cloud architecture expertise. Emergency Response (143) and Pivot Tables and Charts (125) represent specialized operational and analytical capabilities. SAP Applications (104) appears again, reinforcing its enterprise value. Microsoft Access (89), Databricks (85), Mulesoft (85), and Sales Process (74) round out the list, showing diverse specializations from database management to integration platforms and CRM. Job seekers should prioritize UX design for product roles, data analysis for analytical positions, and cloud computing for infrastructure careers, while considering niche specializations like Databricks or Mulesoft for premium positioning.

## 5. Skill Requirements by Job Title

### 5.1 Skills vs Top Job Titles - Heatmap

```{python}
# Cross-tabulate skills by top job titles
if 'TITLE_NAME' in df.columns and 'SOFTWARE_SKILLS_NAME' in df.columns:
    # Get top 8 job titles
    top_titles = df['TITLE_NAME'].value_counts().head(8).index
    
    # Get top 8 software skills (excluding extreme outlier and empty strings)
    all_software = []
    for skills_text in df['SOFTWARE_SKILLS_NAME'].dropna():
        skills = [s.strip() for s in str(skills_text).split('|')]
        for skill in skills:
            # Clean thoroughly
            clean = skill.replace('"', '').replace("'", "").replace('[', '').replace(']', '')
            clean = clean.replace('(', '').replace(')', '').strip().lower()
            if len(clean) > 2:
                all_software.append(clean)
    
    software_freq = Counter(all_software)
    top_software_list = software_freq.most_common(15)
    
    # Filter out problematic entries
    filtered_software = []
    for skill, count in top_software_list:
        # Skip if contains 'cpm', 'onestream', or is too short
        if 'cpm' not in skill.lower() and 'onestream' not in skill.lower() and len(skill) > 2:
            filtered_software.append((skill, count))
    
    # Check for extreme outlier
    if len(filtered_software) > 1 and filtered_software[0][1] > 10 * filtered_software[1][1]:
        top_8_software = [s[0] for s in filtered_software[1:9]]
    else:
        top_8_software = [s[0] for s in filtered_software[:8]]
    
    # Build matrix
    skill_title_matrix = []
    
    for title in top_titles:
        title_data = df[df['TITLE_NAME'] == title].copy()
        skill_row = {'Title': title}
        
        for skill in top_8_software:
            count = 0
            for skills_text in title_data['SOFTWARE_SKILLS_NAME'].dropna():
                if skill in str(skills_text).lower():
                    count += 1
            # Clean skill name for column header - remove all special characters
            clean_skill = skill.replace('"', '').replace("'", "").replace('[', '').replace(']', '')
            clean_skill = clean_skill.replace('(', '').replace(')', '').replace(',', '').strip()
            # Take only first part if multiple words separated by comma
            if ',' in clean_skill or len(clean_skill) > 20:
                clean_skill = clean_skill.split(',')[0].split()[0:3]  # First 3 words max
                clean_skill = ' '.join(clean_skill)
            clean_skill = clean_skill.title()[:20]  # Max 20 chars
            skill_row[clean_skill] = (count / len(title_data) * 100) if len(title_data) > 0 else 0
        
        skill_title_matrix.append(skill_row)
    
    if skill_title_matrix:
        skill_matrix_df = pd.DataFrame(skill_title_matrix)
        skill_matrix_df = skill_matrix_df.set_index('Title')
        
        # Shorten job titles if too long
        skill_matrix_df.index = [title[:25] + '...' if len(title) > 25 else title 
                                  for title in skill_matrix_df.index]
        
        fig, ax = plt.subplots(figsize=(9, 4.5))
        sns.heatmap(skill_matrix_df, annot=True, fmt='.1f', cmap='YlOrRd', 
                    cbar_kws={'label': '% of Postings'}, 
                    ax=ax, linewidths=0.5, linecolor='white', annot_kws={'size': 8})
        ax.set_title('Software Skill Requirements by Job Title', 
                     fontsize=13, fontweight='bold', pad=12)
        ax.set_xlabel('Software Skill', fontsize=10, fontweight='bold')
        ax.set_ylabel('Job Title', fontsize=10, fontweight='bold')
        plt.xticks(rotation=45, ha='right', fontsize=8.5)
        plt.yticks(rotation=0, fontsize=8.5)
        
        plt.tight_layout()
        plt.show()
```

**Skills vs Job Titles - Job Seeker Insights:**

The heatmap reveals distinct skill patterns across job roles. Unclassified positions show the highest demand for general software skills (37.2% require SAP Applications), suggesting broad technical requirements. Oracle Cloud HCM Consultants have specialized focus with 11.1% requiring Oracle Cloud expertise. Data Governance Analysts need the most diverse skillset with strong SQL Programming Language (13.8%) and moderate requirements across Dashboard, Microsoft Office, and SAP. Enterprise Architects and Solutions Architects show concentrated needs in SAP (10.5% and 8.5% respectively) with minimal other software requirements. Data Modelers uniquely emphasize SQL Programming (2.1%) over other tools. The low percentages overall indicate that most roles don't mandate specific software, creating opportunities for candidates to differentiate through technical mastery. Job seekers should target SAP for enterprise roles, Oracle Cloud for HCM consulting, and SQL for data-focused positions.

## 6. Skill Co-occurrence Analysis

### 6.1 Top 10 Skill Combinations

```{python}
# Find commonly co-occurring skills
if 'SKILLS_NAME' in df.columns:
    skill_pairs = []
    
    for skills_text in df['SKILLS_NAME'].dropna():
        skills = [s.strip().lower() for s in str(skills_text).split('|') if len(s.strip()) > 0]
        
        # Get all pairs
        if len(skills) > 1:
            for i in range(len(skills)):
                for j in range(i+1, len(skills)):
                    pair = tuple(sorted([skills[i], skills[j]]))
                    skill_pairs.append(pair)
    
    if len(skill_pairs) > 0:
        pair_freq = Counter(skill_pairs)
        top_pairs = pair_freq.most_common(10)
        
        # Format pair names with line breaks for readability
        pair_names = []
        for p in top_pairs:
            name1 = p[0][0].title()[:25]
            name2 = p[0][1].title()[:25]
            pair_names.append(f"{name1}\n+\n{name2}")
        
        pair_counts = [p[1] for p in top_pairs]
        
        pair_df = pd.DataFrame({'Skill Pair': pair_names, 'Co-occurrence': pair_counts})
        pair_df = pair_df.sort_values('Co-occurrence')
        
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.barplot(data=pair_df, y='Skill Pair', x='Co-occurrence', palette='coolwarm', ax=ax)
        ax.set_title('Top 10 Skill Combinations in Job Postings', fontsize=16, fontweight='bold', pad=20)
        ax.set_xlabel('Co-occurrence Count', fontsize=12, fontweight='bold')
        ax.set_ylabel('Skill Pair', fontsize=12, fontweight='bold')
        
        # Add value labels
        for i, v in enumerate(pair_df['Co-occurrence']):
            ax.text(v + max(pair_df['Co-occurrence'])*0.02, i, f'{int(v):,}', 
                   va='center', fontweight='bold', fontsize=10)
        
        ax.spines['top'].set_visible(False)
        ax.spines['right'].set_visible(False)
        plt.tight_layout()
        plt.show()
```

**Top 10 Skill Combinations – Summary:**

Skill combinations reveal which competencies employers expect candidates to have together. These pairs show complementary skills that are frequently required in the same job posting. Job seekers should develop these skills in tandem, as having both skills in a pair makes you more attractive to employers. Focus on building skill combinations rather than isolated competencies to maximize your marketability.

## Summary

::: {.callout-important icon=false}
## NLP Analysis Key Findings

*Skills Analysis Results:*

- ✓ Successfully analyzed skills from structured data columns
- ✓ Identified top 10 most common skills across all job postings
- ✓ Extracted and ranked top 10 software/technical skills
- ✓ Analyzed top 10 specialized skills for advanced roles
- ✓ Cross-referenced skills with job title requirements via heatmap
- ✓ Discovered top 10 commonly co-occurring skill combinations

*Key Insights:*

- Specific skills dominate the job market across different categories
- Software skills are critical technical differentiators
- Specialized skills offer opportunities for premium positioning
- Skills requirements vary significantly by job title
- Certain skill pairs frequently appear together, indicating complementary competencies

*Recommendations for Job Seekers:*

- Prioritize learning the top 10 skills identified in each category
- Develop proficiency in the most demanded software tools
- Build specialized skills for senior or expert-level opportunities
- Focus on developing complementary skills that frequently co-occur
- Use the heatmap to understand skill priorities for your target job titles
- Tailor your resume to highlight relevant skill combinations
:::

## References

- Pandas Documentation: https://pandas.pydata.org/
- Seaborn Documentation: https://seaborn.pydata.org/
- Collections Counter: https://docs.python.org/3/library/collections.html#collections.Counter