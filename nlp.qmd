---
title: "Natural Language Processing Methods"
subtitle: "Extracting Insights from Job Descriptions"
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
    code-overflow: wrap
    embed-resources: true
    theme: 
      light: [cosmo, custom.scss]
    css: styles.css
jupyter: python3
execute:
  echo: false
  warning: false
  message: false
---

## Introduction

This section applies Natural Language Processing (NLP) techniques to analyze job descriptions and extract meaningful insights about skills, requirements, and industry trends. By processing text data, we can uncover patterns not visible in structured data alone.

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
import warnings
warnings.filterwarnings('ignore')

# Set seaborn style
sns.set_style("whitegrid")
sns.set_palette("husl")
plt.rcParams['figure.figsize'] = (10, 5)

# Load data - try multiple filenames
try:
    df = pd.read_csv('cleaned_job_postings.csv')
except:
    try:
        df = pd.read_csv('lightcast_job_postings.csv')
    except:
        df = pd.read_csv('cleanedjob_postings.csv')

print(f"Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns")
print(f"Columns: {list(df.columns)}")

# Find text columns
text_columns = [col for col in df.columns if 'DESCRIPTION' in col.upper() or 'DESC' in col.upper()]

# If not found, search for object columns with long text
if not text_columns:
    for col in df.columns:
        if df[col].dtype == 'object':
            try:
                avg_len = df[col].astype(str).str.len().mean()
                if avg_len > 100:
                    text_columns.append(col)
            except:
                pass

print(f"Text columns found: {text_columns}")
```

## 1. Text Preprocessing

### 1.1 Data Preparation

```{python}
# Text preprocessing function
def preprocess_text(text):
    """Clean and preprocess text data"""
    if pd.isna(text):
        return ""
    text = str(text).lower()
    text = re.sub(r'[^a-z\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# Apply preprocessing
if len(text_columns) > 0:
    text_col = text_columns[0]
    df['processed_text'] = df[text_col].apply(preprocess_text)
    df_text = df[df['processed_text'].str.len() > 0].copy()
    
    print(f"âœ“ Text column used: {text_col}")
    print(f"âœ“ Successfully preprocessed: {len(df_text):,} job descriptions")
    print(f"âœ“ Average text length: {df_text['processed_text'].str.len().mean():.0f} characters")
else:
    print("âœ— No text columns found for processing")
    df_text = pd.DataFrame()
```

## 2. Keyword Extraction

### 2.1 Top 10 Most Common Keywords

```{python}
# Extract most common meaningful words
if len(text_columns) > 0 and len(df_text) > 0:
    # Comprehensive stopwords list including common non-meaningful terms
    stopwords = {
        # Basic stopwords
        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
        'of', 'is', 'are', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 
        'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 
        'must', 'can', 'this', 'that', 'with', 'as', 'by', 'from', 'your', 
        'we', 'you', 'they', 'our', 'their', 'what', 'which', 'who', 
        'where', 'why', 'how', 'all', 'each', 'every', 'both', 'few', 'more', 
        'most', 'other', 'such', 'no', 'nor', 'not', 'only', 'same', 'so', 
        'than', 'too', 'very', 'if', 'about', 'above', 'after', 'again',
        # Job posting specific stopwords
        'years', 'experience', 'job', 'position', 'role', 'work', 'required',
        'day', 'time', 'one', 'two', 'three', 'well', 'also', 'new', 'us',
        'www', 'http', 'https', 'com', 'org', 'net', 'html', 'php', 'asp',
        'jobs', 'job', 'career', 'careers', 'apply', 'application',
        'company', 'position', 'opportunity', 'opportunities',
        # Common filler words
        'including', 'within', 'through', 'during', 'before', 'after',
        'over', 'between', 'out', 'off', 'then', 'once', 'here', 'there',
        'when', 'while', 'who', 'whom', 'whose', 'whether', 'why',
        'make', 'made', 'many', 'much', 'get', 'got', 'give', 'take',
        'see', 'know', 'use', 'find', 'tell', 'ask', 'seem', 'feel',
        'try', 'leave', 'call', 'etc', 'ie', 'eg', 'per', 'via'
    }
    
    all_words = []
    for text in df_text['processed_text']:
        # Split into words
        words = text.split()
        # Filter: remove stopwords, short words, and words that look like URLs/codes
        filtered_words = [
            w for w in words 
            if w not in stopwords 
            and len(w) > 3  # At least 4 characters
            and len(w) < 20  # Not too long (likely URLs or codes)
            and not any(char.isdigit() for char in w)  # No numbers
            and w.isalpha()  # Only alphabetic characters
        ]
        all_words.extend(filtered_words)
    
    if len(all_words) > 0:
        word_freq = Counter(all_words)
        top_words = word_freq.most_common(10)
        words, counts = zip(*top_words)
        
        # Create dataframe for seaborn
        word_df = pd.DataFrame({'Keyword': words, 'Frequency': counts})
        word_df = word_df.sort_values('Frequency')
        
        # Capitalize first letter of each keyword for better presentation
        word_df['Keyword'] = word_df['Keyword'].str.capitalize()
        
        fig, ax = plt.subplots(figsize=(8, 5))
        sns.barplot(data=word_df, y='Keyword', x='Frequency', palette='Blues_r', ax=ax)
        ax.set_title('Top 10 Most Common Keywords in Job Descriptions', fontsize=14, fontweight='bold', pad=15)
        ax.set_xlabel('Frequency', fontsize=11, fontweight='bold')
        ax.set_ylabel('Keyword', fontsize=11, fontweight='bold')
        
        # Add value labels
        for i, v in enumerate(word_df['Frequency']):
            ax.text(v + max(word_df['Frequency'])*0.02, i, f'{int(v):,}', va='center', fontweight='bold', fontsize=9)
        
        plt.tight_layout()
        plt.show()
        
        print("\nðŸ“Š Top 10 Keywords Found:")
        for word, count in zip(word_df['Keyword'], word_df['Frequency']):
            print(f"  {word:20s} - {count:,} occurrences")
    else:
        print("No meaningful keywords found after filtering")
```

## 3. Technical Skills Analysis

### 3.1 Top 10 Technical Skills

```{python}
# Extract technical skills
if len(text_columns) > 0 and len(df_text) > 0:
    tech_skills = {
        'Python': ['python'],
        'SQL': ['sql', 'mysql', 'postgresql', 'plpgsql'],
        'Java': ['java'],
        'JavaScript': ['javascript', ' js '],
        'R': [r'\br\b'],
        'Machine Learning': ['machine learning', 'ml ', 'deep learning'],
        'Data Analysis': ['data analysis', 'analytics', 'analyst'],
        'Excel': ['excel', 'vba'],
        'Power BI': ['power bi', 'powerbi'],
        'Tableau': ['tableau'],
        'AWS': ['aws', 'amazon web'],
        'Azure': ['azure', 'microsoft azure'],
        'Spark': ['spark', 'pyspark'],
        'Hadoop': ['hadoop'],
        'Git': ['git ', 'github', 'gitlab'],
        'Docker': ['docker'],
        'Kubernetes': ['kubernetes', 'k8s'],
    }
    
    skill_counts = {}
    for skill, keywords in tech_skills.items():
        count = 0
        pattern = '|'.join(keywords)
        for text in df_text['processed_text']:
            if re.search(pattern, text):
                count += 1
        if count > 0:
            skill_counts[skill] = count
    
    if skill_counts:
        sorted_skills = dict(sorted(skill_counts.items(), key=lambda x: x[1], reverse=True))
        
        # Get top 10 skills
        top_10_skills = dict(list(sorted_skills.items())[:10])
        
        # Create dataframe for seaborn
        skill_df = pd.DataFrame(list(top_10_skills.items()), columns=['Skill', 'Count'])
        skill_df = skill_df.sort_values('Count')
        
        fig, ax = plt.subplots(figsize=(8, 5))
        sns.barplot(data=skill_df, y='Skill', x='Count', palette='viridis_r', ax=ax)
        ax.set_title('Top 10 Technical Skills in Job Descriptions', fontsize=14, fontweight='bold', pad=15)
        ax.set_xlabel('Number of Job Postings', fontsize=11, fontweight='bold')
        ax.set_ylabel('Skill', fontsize=11, fontweight='bold')
        
        # Add value labels
        for i, v in enumerate(skill_df['Count']):
            ax.text(v + 50, i, str(int(v)), va='center', fontweight='bold', fontsize=9)
        
        plt.tight_layout()
        plt.show()
        
        print("\nðŸ“Š Top 10 Technical Skills:")
        for skill, count in top_10_skills.items():
            pct = (count / len(df_text)) * 100
            print(f"  {skill:20s} - {count:5d} jobs ({pct:5.1f}%)")
```

## 4. Skill Requirements by Job Title

### 4.1 Skills vs Top Job Titles - Heatmap

```{python}
# Cross-tabulate skills by top job titles
if 'TITLE_NAME' in df.columns and len(text_columns) > 0:
    top_titles = df['TITLE_NAME'].value_counts().head(8).index
    
    tech_skills_short = {
        'Python': ['python'],
        'SQL': ['sql'],
        'Java': ['java'],
        'JavaScript': ['javascript'],
        'Machine Learning': ['machine learning'],
        'AWS': ['aws'],
        'Excel': ['excel'],
        'Power BI': ['power bi']
    }
    
    skill_title_matrix = []
    
    for title in top_titles:
        title_data = df[df['TITLE_NAME'] == title].copy()
        title_data['processed_text'] = title_data[text_columns[0]].apply(preprocess_text)
        
        skill_row = {'Title': title}
        for skill, keywords in tech_skills_short.items():
            count = 0
            pattern = '|'.join(keywords)
            for text in title_data['processed_text']:
                if re.search(pattern, str(text)):
                    count += 1
            skill_row[skill] = (count / len(title_data) * 100) if len(title_data) > 0 else 0
        
        skill_title_matrix.append(skill_row)
    
    if skill_title_matrix:
        skill_df = pd.DataFrame(skill_title_matrix)
        skill_df = skill_df.set_index('Title')
        
        fig, ax = plt.subplots(figsize=(8, 5))
        sns.heatmap(skill_df, annot=True, fmt='.1f', cmap='YlOrRd', cbar_kws={'label': 'Percentage (%)'}, 
                    ax=ax, linewidths=0.5, linecolor='gray')
        ax.set_title('Skill Requirements by Job Title (% of postings)', fontsize=14, fontweight='bold', pad=15)
        ax.set_xlabel('Skill', fontsize=11, fontweight='bold')
        ax.set_ylabel('Job Title', fontsize=11, fontweight='bold')
        
        plt.tight_layout()
        plt.show()
```

## 5. Keyword Correlation Analysis

### 5.1 Top 10 Keyword Pairs

```{python}
# Find commonly co-occurring keywords
if len(text_columns) > 0 and len(df_text) > 0:
    keywords = ['python', 'sql', 'java', 'machine', 'data', 'analytics', 'development', 
                'cloud', 'aws', 'leadership', 'management', 'communication']
    
    keyword_cooccurrence = {}
    
    for text in df_text['processed_text']:
        text_keywords = [kw for kw in keywords if kw in text]
        if len(text_keywords) > 1:
            for i, kw1 in enumerate(text_keywords):
                for kw2 in text_keywords[i+1:]:
                    pair = tuple(sorted([kw1, kw2]))
                    keyword_cooccurrence[pair] = keyword_cooccurrence.get(pair, 0) + 1
    
    if keyword_cooccurrence:
        sorted_pairs = sorted(keyword_cooccurrence.items(), key=lambda x: x[1], reverse=True)[:10]
        pair_names = [f"{p[0][0].capitalize()}-{p[0][1].capitalize()}" for p in sorted_pairs]
        pair_counts = [p[1] for p in sorted_pairs]
        
        pair_df = pd.DataFrame({'Keyword Pair': pair_names, 'Co-occurrence': pair_counts})
        pair_df = pair_df.sort_values('Co-occurrence')
        
        fig, ax = plt.subplots(figsize=(8, 5))
        sns.barplot(data=pair_df, y='Keyword Pair', x='Co-occurrence', palette='coolwarm', ax=ax)
        ax.set_title('Top 10 Keyword Pairs in Job Descriptions', fontsize=14, fontweight='bold', pad=15)
        ax.set_xlabel('Co-occurrence Count', fontsize=11, fontweight='bold')
        ax.set_ylabel('Keyword Pair', fontsize=11, fontweight='bold')
        
        # Add value labels
        for i, v in enumerate(pair_df['Co-occurrence']):
            ax.text(v + 5, i, str(int(v)), va='center', fontweight='bold', fontsize=9)
        
        plt.tight_layout()
        plt.show()
```

## Summary

::: {.callout-important icon=false}
## NLP Analysis Key Findings

*Text Analysis Results:*
- âœ“ Successfully processed job descriptions using NLP techniques
- âœ“ Identified top 10 most common keywords in job postings
- âœ“ Extracted and ranked top 10 technical skills
- âœ“ Cross-referenced skills with job title requirements via heatmap
- âœ“ Discovered top 10 commonly co-occurring keyword pairs

*Key Insights:*
- Top 10 technical skills dominate the job market requirements
- Specific keywords appear consistently across job descriptions
- Skills requirements vary significantly by job title
- Certain keyword pairs frequently appear together, indicating related competencies
- Job descriptions follow consistent patterns in technical terminology

*Recommendations for Job Seekers:*
- Prioritize learning the top 10 technical skills identified
- Tailor resume language to match common keywords found in postings
- Focus on developing complementary skills that frequently co-occur
- Target jobs where your skills align with the identified requirements
- Use heatmap insights to understand skill priorities by job title
:::


## References

- NLTK Documentation: https://www.nltk.org/
- spaCy Documentation: https://spacy.io/
- Seaborn Documentation: https://seaborn.pydata.org/
- Scikit-learn Text Processing: https://scikit-learn.org/stable/tutorial/text_analytics/