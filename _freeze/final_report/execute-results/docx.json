{
  "hash": "e5f8d664515b07c7b4dd99fd8fb037d6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Evaluating Personal Job Market Prospects in 2024\"\nsubtitle: \"Final Report - Business Analytics Project\"\nauthor:\n  - name: Tuba Anwar\n    affiliations:\n      - ref: bu\n  - name: Kriti Singh\n    affiliations:\n      - ref: bu\n  - name: Soham Deshkhaire\n    affiliations:\n      - ref: bu\naffiliations:\n  - id: bu\n    name: Boston University\n    city: Boston\n    state: MA\ndate: 2025-12-06\ndate-modified: today\ndate-format: long\nformat: \n  docx:\n    toc: true\n    toc-depth: 3\n    number-sections: true\n    highlight-style: github\n    fig-width: 7\n    fig-height: 5\n---\n\n# Executive Summary\n\nThis report presents a comprehensive analysis of the job market prospects for data analytics and related roles in 2024. Using a dataset of over 70,000 job postings, we conducted exploratory data analysis, natural language processing of job descriptions, skill gap analysis, and predictive modeling to understand current market trends and identify opportunities for career development.\n\n**Key Findings:**\n\n- SAP Applications, Oracle Cloud, and Microsoft Office are the most in-demand software skills\n- Data Analysis capabilities appear in over 25,000 job postings\n- Significant skill gaps exist in enterprise software (SAP, Oracle) and cloud platforms (AWS, Azure)\n- Machine learning models achieved XX% accuracy in predicting job categories\n- Salary predictions show strong correlation with experience level and technical skill proficiency\n\n# Introduction\n\n---\n\ntitle: \"Job Market Analysis 2024- Business Analytics, Data Science, and Machine Learning Trends\"\nauthor:\n  - name: Tuba Anwar\n    affiliations:\n      - id: bu\n        name: Boston University\n        city: Boston\n        state: MA\n  - name: Kriti Singh\n    affiliations:\n      - ref: bu\n  - name: Soham Deshkhaire\n    affiliations:\n      - ref: bu\nformat: \n  html:\n    toc: true\n    number-sections: true\n    df-print: paged\n    code-overflow: wrap\n    embed-resources: true\n    css: styles.css\n    bibliography: references.bib\n---\n    \n\n# Introduction\n\n## Project Overview\n\nThis research explores the evolving landscape of Business Analytics, Data Science and Machine Learning (ML) careers in 2024. As organizations increasingly integrate AI in every function from operations to strategic decision-making, the demand for professionals with strong data and analytics skills is growing rapidly. This study explores four interconnected questions: the most sought-after skills for analytics and ML roles, how job descriptions are changing to require AI literacy, which industries are leading data-driven hiring, and what the short- to medium-term career outlook looks like for analytics professionals. By combining insights from labor market data and recent academic research, this project provides a holistic view of how AI and automation are transforming workforce expectations and employability strategies for students and professionals.\n\n---\n\n# Research Rationale\n\nAI, ML and data analytics have changed the global job market, creating both opportunities and challenges for new professionals. Organizations are looking for people with strong technical skills such as Python, SQL and ML modeling along with soft skills like communication, ethical reasoning and problem framing. Understanding these changing expectations is important for job seekers so they can match their academic learning and career plans with what the market is demanding. \n\nThis research aims to map out the connection between AI adoption, skill demand and career pathways. Examining these trends can help predict how automation, cloud computing, and generative AI will affect job prospects. Additionally, identifying leading sectors such as finance, healthcare and consulting in analytics hiring will clarify future opportunities and guide both individual career planning and institutional curriculum development.\n\n---\n\n# Research Questions\n\nOur analysis addresses the following core questions:\n\n1. What are the most in-demand technical and soft skills for data science, business analytics, and ML roles in 2024?\n2. How have job descriptions evolved to incorporate AI/ML requirements and competencies?\n3. Which industries are hiring the most data scientists and analytics professionals, and why?\n4. What is the realistic career outlook for business analytics professionals in the short to medium term?\n\n---\n\n# Literature Review\n\nRecent research highlights how analytics and ML skills are becoming increasingly central across industries. Job postings now demand expertise in the full data lifecycle, from engineering and modeling to interpretability along with governance and ethical considerations @liu2024embracing, @liu2024insights. AI and ML proficiency is now a core requirement for business analytics, signaling a blend of traditional analytics and data science roles. Industries such as finance, healthcare and technology are leading this shift, relying on predictive and prescriptive analytics to drive strategy and innovation @smaldone2022.\n\nThe literature also points to an optimistic but dynamic career outlook. Georgieff and Hyee (2022) found that AI adoption does not necessarily displace analytics jobs but rather increases demand for those combining digital and domain expertise @georgieff2022. Gerhart et al. (2024) emphasize that communication and application of insights are as vital as technical proficiency @gerhart2024. Together, these studies illustrate that analytics professionals best positioned for long-term success are those with adaptable, interdisciplinary, and AI-literate skill sets in a rapidly evolving job market.\n\nThe evidence from the U.S. Bureau of Labor Statistics suggests strong growth in data science careers, with increasing demand for professionals who understand both technical analytics and business strategy @bls2024. Graham (2025) highlights emerging AI skills in technology sectors, demonstrating how new competencies are rapidly becoming market requirements @graham2025aiskills. Liu and Li (2024) provide insights into talent cultivation in big data management, showing that employers seek candidates with 1-3 years of experience and strong foundational skills in data mining and analysis @liu2024insights.\n\nThe evidence suggests that the analytics field is expanding rather than contracting. Organizations are not replacing human analysts with AI systems; instead, they are expanding their teams and seeking professionals who can work alongside AI tools. This presents a compelling opportunity for students entering the field, provided they develop both technical depth and the soft skills necessary to translate data insights into business value.\n\n---\n\n# Project Scope & Methodology\n\n## Data Source\n\nWe will utilize the Lightcast 2024 dataset, which includes:\n- Job posting volumes for analytics, data science, and ML roles by industry and location\n- Salary data across data-related positions and geographies\n- Skill requirements extracted from job descriptions\n- Hiring trends across time periods\n- Company size and industry classifications\n- Emerging role titles and job requirements\n\n## Analysis Approach\n\nOur team will:\n\n1. Clean and preprocess the Lightcast data using Python (pandas, NumPy)\n2. Extract and categorize skills mentioned in job descriptions for analytics and ML roles\n3. Compare trends across industries, geographies, and job levels\n4. Analyze salary patterns to understand compensation for different skill combinations\n5. Identify emerging roles and how job requirements are evolving\n6. Visualize findings through interactive dashboards using Plotly and Matplotlib\n7. Develop career strategy recommendations based on market insights\n\n## Expected Findings\n\nWe anticipate discovering that:\n- Python, SQL, and cloud platforms (AWS, GCP, Azure) remain foundational skills\n- Generative AI and prompt engineering are emerging as newly valued competencies\n- Finance, healthcare, and technology sectors lead in analytics hiring\n- Soft skills like communication and domain expertise are increasingly emphasized\n- Analytics roles offer strong job security and career growth potential\n- Salary premiums exist for ML/AI-specialized roles compared to traditional business analytics\n\n## Deliverables (Future Phases)\n\n- Exploratory Data Analysis (EDA) with visualizations\n- Interactive dashboards showing skill trends, industry hiring patterns, and salary insights\n- Career pathway recommendations for different specializations\n- Personal career action plans for each team member\n\n---\n\n# References\n\n\n# Data Cleaning and Preparation\n\n---\ntitle: \"Data Cleaning & Exploration\"\nsubtitle: \"Preparing the Job Market Dataset for Analysis\"\nformat: \n  html:\n    toc: true\n    number-sections: true\n    df-print: paged\n    code-overflow: wrap\n    embed-resources: true\n    css: styles.css \njupyter: python3\nexecute:\n  echo: false\n  warning: false\n  message: false\n---\n\n## Introduction\n\nThis document details the comprehensive data cleaning and preprocessing steps applied to the 2024 job market dataset.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load raw data\ndf = pd.read_csv('lightcast_job_postings.csv')\n\nprint(f\"Initial dataset shape: {df.shape}\")\nprint(f\"Total rows: {df.shape[0]:,}\")\nprint(f\"Total columns: {df.shape[1]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInitial dataset shape: (72498, 131)\nTotal rows: 72,498\nTotal columns: 131\n```\n:::\n:::\n\n\n## Step 1: Removing Redundant Columns\n\nWe remove redundant columns to improve dataset quality and analysis efficiency.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# List of columns to drop\ncolumns_to_drop = [\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n    \"BODY\", \"TITLE_RAW\", \"COMPANY_RAW\", \"ACTIVE_SOURCES_INFO\",\n    \"NAICS2\", \"NAICS2_NAME\", \"NAICS3\", \"NAICS3_NAME\", \n    \"NAICS4\", \"NAICS4_NAME\", \"NAICS5\", \"NAICS5_NAME\", \n    \"NAICS6\", \"NAICS6_NAME\",\n    \"SOC_2\", \"SOC_2_NAME\", \"SOC_3\", \"SOC_3_NAME\", \n    \"SOC_4\", \"SOC_4_NAME\", \"SOC_5\", \"SOC_5_NAME\",\n    \"SOC_2021_2\", \"SOC_2021_2_NAME\", \"SOC_2021_3\", \"SOC_2021_3_NAME\", \n    \"SOC_2021_5\", \"SOC_2021_5_NAME\",\n    \"LOT_CAREER_AREA\", \"LOT_CAREER_AREA_NAME\", \"LOT_OCCUPATION\", \"LOT_OCCUPATION_NAME\",\n    \"LOT_SPECIALIZED_OCCUPATION\", \"LOT_SPECIALIZED_OCCUPATION_NAME\", \n    \"LOT_OCCUPATION_GROUP\", \"LOT_OCCUPATION_GROUP_NAME\",\n    \"LOT_V6_SPECIALIZED_OCCUPATION\", \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\", \n    \"LOT_V6_OCCUPATION\", \"LOT_V6_OCCUPATION_NAME\",\n    \"LOT_V6_OCCUPATION_GROUP\", \"LOT_V6_OCCUPATION_GROUP_NAME\", \n    \"LOT_V6_CAREER_AREA\", \"LOT_V6_CAREER_AREA_NAME\",\n    \"ONET_2019\", \"ONET_2019_NAME\", \"CIP6\", \"CIP6_NAME\", \"CIP2\", \"CIP2_NAME\",\n    \"COUNTY\", \"COUNTY_NAME\", \"COUNTY_OUTGOING\", \"COUNTY_NAME_OUTGOING\", \n    \"COUNTY_INCOMING\", \"COUNTY_NAME_INCOMING\",\n    \"MSA\", \"MSA_OUTGOING\", \"MSA_INCOMING\",\n    \"SALARY_TO\", \"SALARY_FROM\", \"ORIGINAL_PAY_PERIOD\",\n    \"MODELED_EXPIRED\", \"MODELED_DURATION\"\n]\n\n# Drop only columns that exist\ncolumns_to_drop = [col for col in columns_to_drop if col in df.columns]\ndf_cleaned = df.drop(columns=columns_to_drop, inplace=False)\n\nprint(f\"Columns removed: {len(columns_to_drop)}\")\nprint(f\"New dataset shape: {df_cleaned.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nColumns removed: 69\nNew dataset shape: (72498, 62)\n```\n:::\n:::\n\n\n## Step 2: Handling Missing Values\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Missing value statistics\nmissing_stats = pd.DataFrame({\n    'Column': df_cleaned.columns,\n    'Missing_Count': df_cleaned.isnull().sum().values,\n    'Missing_Percentage': (df_cleaned.isnull().sum().values / len(df_cleaned) * 100).round(2)\n})\nmissing_stats = missing_stats[missing_stats['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n\nprint(f\"Columns with missing values: {len(missing_stats)}\")\nprint(\"\\nTop 10 columns with highest missing percentages:\")\nprint(missing_stats.head(10))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nColumns with missing values: 62\n\nTop 10 columns with highest missing percentages:\n                    Column  Missing_Count  Missing_Percentage\n18    MAX_YEARS_EXPERIENCE          64068               88.37\n13           MAX_EDULEVELS          56183               77.50\n14      MAX_EDULEVELS_NAME          56183               77.50\n50       LIGHTCAST_SECTORS          54711               75.47\n51  LIGHTCAST_SECTORS_NAME          54711               75.47\n20                  SALARY          41690               57.51\n3                 DURATION          27316               37.68\n17    MIN_YEARS_EXPERIENCE          23146               31.93\n2                  EXPIRED           7844               10.82\n30       MSA_NAME_INCOMING           3962                5.46\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Drop columns with >50% missing values\nthreshold = len(df_cleaned) * 0.5\ncols_before = len(df_cleaned.columns)\ndf_cleaned = df_cleaned.dropna(thresh=threshold, axis=1)\ncols_after = len(df_cleaned.columns)\n\nprint(f\"Columns dropped due to >50% missing values: {cols_before - cols_after}\")\n\n# Fill numerical columns with median\nnumerical_cols = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\nfor col in numerical_cols:\n    if df_cleaned[col].isnull().sum() > 0:\n        median_val = df_cleaned[col].median()\n        df_cleaned[col].fillna(median_val, inplace=True)\n\n# Fill categorical columns with \"Unknown\"\ncategorical_cols = df_cleaned.select_dtypes(include=['object']).columns.tolist()\nfor col in categorical_cols:\n    if df_cleaned[col].isnull().sum() > 0:\n        df_cleaned[col].fillna(\"Unknown\", inplace=True)\n\nprint(f\"Total missing values after imputation: {df_cleaned.isnull().sum().sum()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nColumns dropped due to >50% missing values: 6\nTotal missing values after imputation: 0\n```\n:::\n:::\n\n\n## Step 3: Removing Duplicates\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Check for duplicates\ninitial_rows = len(df_cleaned)\nduplicates_count = df_cleaned.duplicated(subset=['TITLE_NAME', 'COMPANY_NAME', 'LOCATION']).sum()\n\nprint(f\"Initial rows: {initial_rows:,}\")\nprint(f\"Duplicate rows detected: {duplicates_count:,}\")\n\n# Remove duplicates\ndf_cleaned = df_cleaned.drop_duplicates(\n    subset=['TITLE_NAME', 'COMPANY_NAME', 'LOCATION'], \n    keep='first'\n)\n\nfinal_rows = len(df_cleaned)\nprint(f\"Final rows after duplicate removal: {final_rows:,}\")\nprint(f\"Rows removed: {initial_rows - final_rows:,}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInitial rows: 72,498\nDuplicate rows detected: 13,278\nFinal rows after duplicate removal: 59,220\nRows removed: 13,278\n```\n:::\n:::\n\n\n## Step 4: Final Summary\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Create summary statistics\nsummary_stats = pd.DataFrame({\n    'Metric': [\n        'Total Rows',\n        'Total Columns',\n        'Numerical Columns',\n        'Categorical Columns',\n        'Missing Values'\n    ],\n    'Value': [\n        f\"{len(df_cleaned):,}\",\n        f\"{len(df_cleaned.columns)}\",\n        f\"{len(df_cleaned.select_dtypes(include=[np.number]).columns)}\",\n        f\"{len(df_cleaned.select_dtypes(include=['object']).columns)}\",\n        f\"{df_cleaned.isnull().sum().sum()}\"\n    ]\n})\n\nprint(summary_stats.to_string(index=False))\n\n# Save cleaned dataset\ndf_cleaned.to_csv('cleanedjob_postings.csv', index=False)\nprint(\"\\nCleaned dataset saved as 'cleanedjob_postings.csv'\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Metric  Value\n         Total Rows 59,220\n      Total Columns     56\n  Numerical Columns     12\nCategorical Columns     44\n     Missing Values      0\n\nCleaned dataset saved as 'cleanedjob_postings.csv'\n```\n:::\n:::\n\n\n## Summary\n\nThe data cleaning process has successfully prepared the job market dataset:\n\n- Removed redundant columns\n- Handled missing values strategically\n- Removed duplicate postings\n- Final clean dataset ready for analysis\n\n\n# Exploratory Data Analysis\n\n---\n\ntitle: \"Exploratory Data Analysis\"\nsubtitle: \"Uncovering Job Market Trends and Patterns in 2024\"\nformat: \n  html:\n    toc: true\n    number-sections: true\n    df-print: paged\n    code-overflow: wrap\n    embed-resources: true\n    css: styles.css \njupyter: python3\nexecute:\n  echo: false\n  warning: false\n  message: false\n---\n\n## Introduction\n\nThis document presents a comprehensive exploratory data analysis of the 2024 job market. We examine hiring trends, employment types, geographic patterns, and remote work opportunities to provide actionable insights for job seekers and market analysts.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport panel as pn\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Enable panel for rendering\npn.extension()\n\n# Updated color palette - blue, pink, purple, red\nCOLORS = ['#2196F3', '#E91E63', '#9B59B6', '#F44336', '#00BCD4', '#FF4081', '#7E57C2', '#EF5350']\n\n# Load cleaned data\ndf = pd.read_csv('cleanedjob_postings.csv')\n\n#print(f\"Dataset shape: {df.shape}\")\n#print(f\"Analysis period: 2024 Job Market\")\n#print(f\"Total job postings analyzed: {len(df):,}\")\n```\n\n::: {.cell-output .cell-output-display}\n```\nUnable to display output for mime type(s): text/html\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\nUnable to display output for mime type(s): application/javascript, application/vnd.holoviews_load.v0+json\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\nUnable to display output for mime type(s): application/javascript, application/vnd.holoviews_load.v0+json\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\nUnable to display output for mime type(s): application/vnd.holoviews_exec.v0+json, text/html\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\nUnable to display output for mime type(s): text/html\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\nUnable to display output for mime type(s): application/javascript, application/vnd.holoviews_load.v0+json\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\nUnable to display output for mime type(s): application/javascript, application/vnd.holoviews_load.v0+json\n```\n:::\n:::\n\n\n## 1. Job Title Analysis\n\n### 1.1 Top In-Demand Job Titles\n\nUnderstanding which job titles dominate the market helps identify high-demand roles and emerging career opportunities.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Get top 10 job titles \ntitle_counts = df[df['TITLE_NAME'].notna()]['TITLE_NAME'].value_counts().head(10)\ntitle_df = pd.DataFrame({\n    'Job Title': title_counts.index, \n    'Count': title_counts.values,\n    'Percentage': (title_counts.values / len(df[df['TITLE_NAME'].notna()]) * 100).round(2)\n})\n\nprint(f\"\\nTop job titles : {len(title_counts)} titles\")\n\n# Create interactive horizontal bar chart\ntitle_df.hvplot.barh(\n    x='Job Title',\n    y='Count',\n    title='Top 10 Most In-Demand Job Titles (2024)',\n    height=500,\n    width=800,\n    color='#2196F3',\n    hover_cols=['Percentage'],\n    ylabel='',\n    xlabel='Number of Job Postings',\n    flip_yaxis=True\n).opts(xformatter='%.0f')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTop job titles : 10 titles\n```\n:::\n\n::: {.cell-output .cell-output-display}\n:::\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n:Bars   [Job Title]   (Count,Percentage)\n```\n:::\n:::\n\n\nThe Top 10 most in-demand jobs are dominated by Data Analyst roles, which appear far more frequently than any other title. Business Intelligence Analysts, Enterprise Architects, and Data Modelers also rank highly, highlighting strong employer demand for data-focused and technical architecture skills. Overall, the top roles show a clear market emphasis on analytics, data management, and solution-oriented positions in 2024.\n\n## 2. Employment Type Distribution\n\n### 2.1 Full-Time vs Part-Time vs Contract\n\nUnderstanding employment type distribution helps job seekers target positions matching their career preferences.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Count jobs by employment type \ndf_employment = df[df['EMPLOYMENT_TYPE_NAME'].notna()].copy()\n\n# Remove \"Unknown\" values\ndf_employment = df_employment[df_employment['EMPLOYMENT_TYPE_NAME'] != 'Unknown']\n\nemployment_counts = df_employment['EMPLOYMENT_TYPE_NAME'].value_counts()\nemployment_df = pd.DataFrame({\n    'Employment Type': employment_counts.index, \n    'Count': employment_counts.values,\n    'Percentage': (employment_counts.values / employment_counts.sum() * 100).round(1)\n})\n\n#print(f\"\\nEmployment types : {len(df_employment):,} jobs\")\n#print(employment_df.to_string(index=False))\n\n# Create bar chart\nemployment_df.hvplot.bar(\n    x='Employment Type',\n    y='Count',\n    title='Job Market Distribution by Employment Type (2024)',\n    height=400,\n    width=700,\n    color='Employment Type',\n    cmap=COLORS,\n    hover_cols=['Percentage'],\n    ylabel='Number of Job Postings',\n    rot=45\n).opts(yformatter='%.0f')\n```\n\n::: {.cell-output .cell-output-display}\n:::\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n:Bars   [Employment Type]   (Count,Percentage)\n```\n:::\n:::\n\n\nMost job postings in 2024 require around 5 years of experience, making it the dominant requirement across roles. Entry-level opportunities (0–2 years) exist but are significantly fewer, showing that employers prioritize mid-level talent with proven skills. Requirements beyond 7+ years drop sharply, indicating limited demand for highly senior roles compared to mid-level positions, jobs that are full-time, makes up over 95% of all postings only a small share of roles are part-time (3.3%) or mixed part-time/full-time (1.4%). This shows that employers mainly prefer hiring for stable, full-time positions.\n\n## 3. Remote Work Analysis\n\n### 3.1 Remote vs Hybrid vs On-Site\n\nThe prevalence of remote work reflects post-pandemic hiring trends and employer flexibility.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Count jobs by remote type and remove not specified values\ndf_remote = df.copy()\n\n# Replace None, NaN, Unknown, empty strings with NaN\ndf_remote['REMOTE_TYPE_NAME'] = df_remote['REMOTE_TYPE_NAME'].replace({\n    '[None]': np.nan,\n    'None': np.nan,\n    'Unknown': np.nan,\n    '': np.nan,\n    'Not Specified': np.nan\n})\n\n# Drop rows with NaN in REMOTE_TYPE_NAME\ndf_remote = df_remote[df_remote['REMOTE_TYPE_NAME'].notna()]\n\n#print(f\"\\nRemote type distribution (excluding not specified):\")\n#print(f\"Total jobs with remote info: {len(df_remote):,}\")\n\n# Count jobs by remote type\nremote_counts = df_remote['REMOTE_TYPE_NAME'].value_counts()\nremote_df = pd.DataFrame({\n    'Remote Type': remote_counts.index,\n    'Count': remote_counts.values,\n    'Percentage': (remote_counts.values / remote_counts.sum() * 100).round(1)\n})\n\n#print(remote_df.to_string(index=False))\n\n# Create bar chart with custom colors\nremote_df.hvplot.bar(\n    x='Remote Type',\n    y='Count',\n    title='Job Market Distribution: Remote, Hybrid & On-Site Opportunities (2024)',\n    height=400,\n    width=700,\n    color='Remote Type',\n    cmap=['#2196F3', '#E91E63', '#9B59B6', '#F44336'],\n    hover_cols=['Percentage'],\n    ylabel='Number of Job Postings',\n    xlabel='Work Location Type',\n    rot=0\n).opts(yformatter='%.0f')\n```\n\n::: {.cell-output .cell-output-display}\n:::\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n:Bars   [Remote Type]   (Count,Percentage)\n```\n:::\n:::\n\n\nThe dominance of full-time roles in 2024 reflects a strong post-COVID recovery, as companies shift back toward stable, long-term hiring after years of uncertainty. The very small share of part-time and hybrid hour roles suggests that businesses are prioritizing consistent workforce availability to meet rising operational demands. Overall, the trend indicates increased employer confidence and a return to pre-pandemic hiring patterns focused on full-time talent.\n\n## 4. Geographic Analysis\n\n### 4.1 Top States for Job Opportunities\n\nGeographic distribution shows where job opportunities are concentrated.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# Get top 10 states by job postings \ndf_states = df[df['STATE_NAME'].notna()].copy()\n\nstate_counts = df_states['STATE_NAME'].value_counts().head(10)\nstate_df = pd.DataFrame({\n    'State': state_counts.index,\n    'Job Postings': state_counts.values,\n    'Percentage': (state_counts.values / len(df_states) * 100).round(2)\n})\n\n#print(f\"\\nStates with job postings : {len(df_states):,} jobs\")\n\n# Create bar chart\nstate_df.hvplot.bar(\n    x='State',\n    y='Job Postings',\n    title='Top 10 States by Number of Job Postings (2024)',\n    height=450,\n    width=850,\n    color='#9B59B6',\n    hover_cols=['Percentage'],\n    ylabel='Number of Job Postings',\n    xlabel='State',\n    rot=45\n).opts(yformatter='%.0f')\n```\n\n::: {.cell-output .cell-output-display}\n:::\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n:Bars   [State]   (Job Postings,Percentage)\n```\n:::\n:::\n\n\nThe top 10 states for job postings in 2024 are led by Texas and California, which together dominate the job market with substantially higher opportunities than the rest. States like Virginia, Florida, New York, and Illinois follow, reflecting strong demand in both tech-heavy and fast-growing regional economies. Overall, job opportunities are concentrated in major economic hubs and states with strong corporate, technology, and government sectors.\n\n## 5. Top Hiring Companies\n\n### 5.1 Companies with Most Job Openings\n\nIdentifying top hiring companies helps job seekers target organizations with multiple opportunities.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# Get top 10 companies by job postings (remove None values)\ndf_companies = df[df['COMPANY_NAME'].notna()].copy()\n\ncompany_counts = df_companies['COMPANY_NAME'].value_counts().head(10)\ncompany_df = pd.DataFrame({\n    'Company': company_counts.index,\n    'Job Postings': company_counts.values\n})\n\n#print(f\"\\nCompanies with job postings : {len(df_companies):,} jobs\")\n\n# Create horizontal bar chart\ncompany_df.hvplot.barh(\n    x='Company',\n    y='Job Postings',\n    title='Top 10 Companies by Number of Job Postings (2024)',\n    height=550,\n    width=800,\n    color='#E91E63',\n    ylabel='',\n    xlabel='Number of Job Postings',\n    flip_yaxis=True\n).opts(xformatter='%.0f')\n```\n\n::: {.cell-output .cell-output-display}\n:::\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n:Bars   [Company]   (Job Postings)\n```\n:::\n:::\n\n\nThe Top 10 hiring companies in 2024 are led by Deloitte and Accenture, which show significantly higher job postings than other firms,reflecting strong demand for consulting, technology, and analytics roles. Companies like PwC, Insight Global, KPMG, and Lumen Technologies also appear prominently, indicating consistent hiring across both consulting and IT services sectors. Overall, the hiring landscape is dominated by large professional services firms, showcasing continued growth in advisory, digital transformation, and data-driven business roles.\n\n## 6. Job Posting Timeline\n\n### 6.1 When Jobs Are Posted\n\nUnderstanding posting patterns helps with strategic job search timing.\nSince job postings peaked in August and September 2024, job seekers should target this late-summer period for applications. Companies appear to increase hiring toward the end of Q3, meaning more openings, faster responses, and higher chances of landing interviews. Preparing resumes, portfolios, and applications ahead of this surge (during May–July) can give job seekers a strategic advantage when the market becomes most active.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# Print diagnostics\n#print(\"\\nAnalyzing POSTED column:\")\n#print(f\"Sample values: {df['POSTED'].head(10)}\")\n#print(f\"Data type: {df['POSTED'].dtype}\")\n\n# Convert POSTED to datetime\ndf_time = df.copy()\ndf_time['POSTED'] = pd.to_datetime(df_time['POSTED'], errors='coerce')\n\n#print(f\"\\nAfter conversion:\")\n#print(f\"Date range: {df_time['POSTED'].min()} to {df_time['POSTED'].max()}\")\n#print(f\"Valid dates: {df_time['POSTED'].notna().sum():,} out of {len(df_time):,}\")\n#print(f\"Missing dates: {df_time['POSTED'].isna().sum():,}\")\n\n# Filter valid dates\ntime_data = df_time[df_time['POSTED'].notna()].copy()\n\nif len(time_data) > 0:\n    # Extract year-month\n    time_data['YearMonth'] = time_data['POSTED'].dt.to_period('M')\n    \n    # Group by month and count\n    monthly_counts = time_data.groupby('YearMonth').size().reset_index(name='Job Postings')\n    monthly_counts['Month'] = monthly_counts['YearMonth'].astype(str)\n    \n    # Sort by month\n    monthly_counts = monthly_counts.sort_values('Month')\n    \n    #print(f\"\\nMonthly posting counts:\")\n    #print(monthly_counts[['Month', 'Job Postings']].to_string(index=False))\n    \n    # Create line and area chart\n    line_chart = monthly_counts.hvplot.line(\n        x='Month',\n        y='Job Postings',\n        title='Job Posting Trends Over Time (2024)',\n        height=400,\n        width=850,\n        color='#2196F3',\n        line_width=3,\n        ylabel='Number of Job Postings',\n        xlabel='Month',\n        rot=45\n    ).opts(yformatter='%.0f')\n    \n    area_chart = monthly_counts.hvplot.area(\n        x='Month',\n        y='Job Postings',\n        alpha=0.3,\n        color='#2196F3'\n    )\n    \n    # Combine line and area\n    (line_chart * area_chart)\nelse:\n    #print(\"\\nWarning: No valid dates found in POSTED column\")\n    ##print(\"Sample of POSTED values:\")\n    print(df['POSTED'].head(20))\n```\n:::\n\n\n## 7. Experience Requirements\n\n### 7.1 Minimum Years of Experience Required\n\nUnderstanding experience requirements helps assess job market accessibility.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n# Analyze minimum years of experience (remove None values)\ndf_experience = df[df['MIN_YEARS_EXPERIENCE'].notna()].copy()\n\nexp_counts = df_experience['MIN_YEARS_EXPERIENCE'].value_counts().sort_index()\nexp_df = pd.DataFrame({\n    'Years of Experience': exp_counts.index,\n    'Job Postings': exp_counts.values\n})\n\n# Convert Years of Experience to string for better labels\nexp_df['Years Label'] = exp_df['Years of Experience'].astype(int).astype(str) + ' years'\n\n#print(f\"\\nJobs with experience requirements (excluding None): {len(df_experience):,} jobs\")\n#print(exp_df[['Years of Experience', 'Job Postings']].to_string(index=False))\n\n# Create bar chart with pastel purple color\nexp_df.hvplot.bar(\n    x='Years Label',\n    y='Job Postings',\n    title='Job Postings by Minimum Years of Experience Required (2024)',\n    height=400,\n    width=800,\n    color='#B39DDB',\n    ylabel='Number of Job Postings',\n    xlabel='Minimum Years of Experience',\n    rot=45\n).opts(yformatter='%.0f')\n```\n\n::: {.cell-output .cell-output-display}\n:::\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n:Bars   [Years Label]   (Job Postings)\n```\n:::\n:::\n\n\nMost job postings in 2024 require around 5 years of experience, making it the dominant requirement across roles. Entry-level opportunities (0–2 years) exist but are significantly fewer, showing that employers prioritize mid-level talent with proven skills. Requirements beyond 7+ years drop sharply, indicating limited demand for highly senior roles compared to mid-level positions.\n\n## References\n\n- Data source: Lightcast Job Postings Dataset (2024)\n- Visualization tools: hvPlot, Panel, Python\n- Analysis framework: Standard EDA best practices\n\n\n# Data Analysis Methods\n\n---\ntitle: \"Data Analysis\"\nsubtitle: \"Comprehensive Data Cleaning & Exploratory Analysis of Job Market Trends\"\nformat: \n  html:\n    toc: true\n    number-sections: true\n    df-print: paged\n    code-overflow: wrap\n    embed-resources: true\n    css: styles.css\njupyter: python3\nexecute:\n  echo: false  \n  warning: false\n  message: false\n---\n\n## Introduction\n\nThis document provides a comprehensive analysis of job market data for 2024. We perform data cleaning, handle missing values, remove duplicates, and conduct exploratory data analysis to understand hiring trends, salary distributions, and job market dynamics.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport holoviews as hv\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Initialize hvplot with bokeh backend\nhv.extension('bokeh')\n\n# Load your data\ndf = pd.read_csv('lightcast_job_postings.csv')\n```\n\n::: {.cell-output .cell-output-display}\n```\nUnable to display output for mime type(s): text/html\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\nUnable to display output for mime type(s): application/javascript, application/vnd.holoviews_load.v0+json\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\nUnable to display output for mime type(s): application/javascript, application/vnd.holoviews_load.v0+json\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\nUnable to display output for mime type(s): application/vnd.holoviews_exec.v0+json, text/html\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\nUnable to display output for mime type(s): text/html\n```\n:::\n:::\n\n\n## Step 1: Removing Redundant Columns\n\n### Why Remove These Columns?\n\nWe remove redundant columns to improve dataset quality and analysis efficiency. Specifically:\n\n- **Tracking & Administrative Columns**: ID, URL, DUPLICATES, LAST_UPDATED_TIMESTAMP - These are metadata and don't contribute to analysis\n- **Raw Text Fields**: BODY, TITLE_RAW, COMPANY_RAW - We have cleaned versions (TITLE, COMPANY_NAME, TITLE_CLEAN)\n- **Deprecated Classifications**: We keep only the latest NAICS_2022_6, SOC_2021_4, and CIP4 standards\n- **Duplicate Geographic Fields**: Multiple versions of county/MSA data create redundancy\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# List of columns to drop\ncolumns_to_drop = [\n    # Administrative & tracking columns\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n    # Raw text columns (we have cleaned versions)\n    \"BODY\", \"TITLE_RAW\", \"COMPANY_RAW\", \"ACTIVE_SOURCES_INFO\",\n    # Deprecated NAICS versions (keeping NAICS_2022_6)\n    \"NAICS2\", \"NAICS2_NAME\", \"NAICS3\", \"NAICS3_NAME\", \n    \"NAICS4\", \"NAICS4_NAME\", \"NAICS5\", \"NAICS5_NAME\", \n    \"NAICS6\", \"NAICS6_NAME\",\n    # Deprecated SOC versions (keeping SOC_2021_4)\n    \"SOC_2\", \"SOC_2_NAME\", \"SOC_3\", \"SOC_3_NAME\", \n    \"SOC_4\", \"SOC_4_NAME\", \"SOC_5\", \"SOC_5_NAME\",\n    \"SOC_2021_2\", \"SOC_2021_2_NAME\", \"SOC_2021_3\", \"SOC_2021_3_NAME\", \n    \"SOC_2021_5\", \"SOC_2021_5_NAME\",\n    # Deprecated occupation classifications\n    \"LOT_CAREER_AREA\", \"LOT_CAREER_AREA_NAME\", \"LOT_OCCUPATION\", \"LOT_OCCUPATION_NAME\",\n    \"LOT_SPECIALIZED_OCCUPATION\", \"LOT_SPECIALIZED_OCCUPATION_NAME\", \n    \"LOT_OCCUPATION_GROUP\", \"LOT_OCCUPATION_GROUP_NAME\",\n    \"LOT_V6_SPECIALIZED_OCCUPATION\", \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\", \n    \"LOT_V6_OCCUPATION\", \"LOT_V6_OCCUPATION_NAME\",\n    \"LOT_V6_OCCUPATION_GROUP\", \"LOT_V6_OCCUPATION_GROUP_NAME\", \n    \"LOT_V6_CAREER_AREA\", \"LOT_V6_CAREER_AREA_NAME\",\n    # Deprecated CIP and ONET versions\n    \"ONET_2019\", \"ONET_2019_NAME\", \"CIP6\", \"CIP6_NAME\", \"CIP2\", \"CIP2_NAME\",\n    # Duplicate geographic fields\n    \"COUNTY\", \"COUNTY_NAME\", \"COUNTY_OUTGOING\", \"COUNTY_NAME_OUTGOING\", \n    \"COUNTY_INCOMING\", \"COUNTY_NAME_INCOMING\",\n    \"MSA\", \"MSA_OUTGOING\", \"MSA_INCOMING\",\n    # Deprecated salary fields (keeping SALARY)\n    \"SALARY_TO\", \"SALARY_FROM\", \"ORIGINAL_PAY_PERIOD\",\n    # Model versions (keep actual data)\n    \"MODELED_EXPIRED\", \"MODELED_DURATION\"\n]\n\n# Drop only columns that exist in the dataset\ncolumns_to_drop = [col for col in columns_to_drop if col in df.columns]\ndf_cleaned = df.drop(columns=columns_to_drop, inplace=False)\n```\n:::\n\n\n## Step 2: Handling Missing Values\n\n### Understanding Missing Data\n\nBefore imputation, let's visualize where data is missing:\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n# Missing value statistics\nmissing_stats = pd.DataFrame({\n    'Column': df_cleaned.columns,\n    'Missing_Count': df_cleaned.isnull().sum().values,\n    'Missing_Percentage': (df_cleaned.isnull().sum().values / len(df_cleaned) * 100).round(2)\n})\nmissing_stats = missing_stats[missing_stats['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n\n# Interactive missing values visualization\nif len(missing_stats) > 0:\n    # Select top 10 for cleaner visualization\n    top_missing = missing_stats.head(10)\n    missing_plot = top_missing.hvplot.barh(\n        x='Column', \n        y='Missing_Percentage',\n        title='Top 10 Columns with Missing Values',\n        xlabel='Missing Percentage (%)',\n        ylabel='Column Name',\n        height=450,\n        width=900,\n        color='#e74c3c',\n        hover_cols=['Missing_Count', 'Missing_Percentage']\n    ).opts(invert_yaxis=True)\n    missing_plot\n```\n:::\n\n\n### Missing Value Imputation Strategy\n\nWe applied a strategic approach to handle missing data:\n\n- **Dropped columns** with more than 50% missing values\n- **Filled numerical columns** with median values to maintain distribution\n- **Filled categorical columns** with \"Unknown\" for clarity\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n# Strategy: Drop columns with >50% missing values\nthreshold = len(df_cleaned) * 0.5\ncols_before = len(df_cleaned.columns)\ndf_cleaned = df_cleaned.dropna(thresh=threshold, axis=1)\ncols_after = len(df_cleaned.columns)\n\n# Fill numerical columns with median\nnumerical_cols = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\nfor col in numerical_cols:\n    if df_cleaned[col].isnull().sum() > 0:\n        median_val = df_cleaned[col].median()\n        df_cleaned[col].fillna(median_val, inplace=True)\n\n# Fill categorical columns with \"Unknown\"\ncategorical_cols = df_cleaned.select_dtypes(include=['object']).columns.tolist()\nfor col in categorical_cols:\n    if df_cleaned[col].isnull().sum() > 0:\n        df_cleaned[col].fillna(\"Unknown\", inplace=True)\n```\n:::\n\n\n## Step 3: Removing Duplicates\n\n### Why Remove Duplicates?\n\nDuplicate job postings skew our analysis. We remove them based on job title, company, and location to ensure each unique position is counted only once.\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\n# Remove duplicates based on key identifiers\ndf_cleaned = df_cleaned.drop_duplicates(\n    subset=['TITLE_NAME', 'COMPANY_NAME', 'LOCATION'], \n    keep='first'\n)\n```\n:::\n\n\n## Step 4: Exploratory Data Analysis (EDA)\n\n### Visualization 1: Top Job Titles\n\nUnderstanding which job titles are most in demand helps job seekers identify the skills and roles to target.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n# Get top 10 job titles\ntitle_counts = df_cleaned['TITLE_NAME'].value_counts().head(10)\ntitle_df = pd.DataFrame({'Job Title': title_counts.index, 'Count': title_counts.values})\n\n# Create interactive plot\ntitle_plot = title_df.hvplot.barh(\n    x='Job Title',\n    y='Count',\n    title='Top 10 Job Titles by Postings (2024)',\n    xlabel='Number of Job Postings',\n    ylabel='Job Title',\n    height=500,\n    width=1000,\n    color='#3498db',\n    hover_cols='all'\n).opts(invert_yaxis=True)\n\ntitle_plot\n```\n\n::: {.cell-output .cell-output-display}\n:::\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\n:Bars   [Job Title]   (Count,index)\n```\n:::\n:::\n\n\n::: {.callout-note icon=false}\n## Key Insight\nThe top job titles show where the job market is most active. This data helps identify roles with highest demand for new talent.\n:::\n\n### Visualization 2: Job Postings by Employment Type\n\nEmployment type indicates the nature of positions available in the job market. Understanding the distribution helps job seekers target positions that match their preferences.\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\n# Count jobs by employment type\nemployment_counts = df_cleaned['EMPLOYMENT_TYPE_NAME'].value_counts()\nemployment_df = pd.DataFrame({\n    'Employment Type': employment_counts.index, \n    'Count': employment_counts.values,\n    'Percentage': (employment_counts.values / employment_counts.sum() * 100).round(1)\n})\n\n# Create interactive scatter plot with size\nemployment_plot = employment_df.hvplot.scatter(\n    x='Employment Type',\n    y='Percentage',\n    size='Count',\n    title='Job Market Distribution by Employment Type (2024)',\n    xlabel='Employment Type',\n    ylabel='Percentage of Total Jobs (%)',\n    height=500,\n    width=1000,\n    color='#e74c3c',\n    hover_cols=['Count', 'Percentage'],\n    alpha=0.6,\n    size_max=1000\n)\n\nemployment_plot\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWARNING:param.main: size_max option not found for scatter plot with bokeh; similar options include: ['size']\n```\n:::\n\n::: {.cell-output .cell-output-display}\n:::\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\n:Scatter   [Employment Type]   (Percentage,Count)\n```\n:::\n:::\n\n\n::: {.callout-note icon=false}\n## Key Insight\nThe distribution of employment types shows whether the market is primarily offering full-time, part-time, or contract positions, helping job seekers identify opportunities that match their work preferences.\n:::\n\n### Visualization 3: Remote Work Distribution\nThe prevalence of remote work reflects post-pandemic hiring trends and work flexibility.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\n# Count jobs by remote type\nremote_counts = df_cleaned['REMOTE_TYPE_NAME'].value_counts()\nremote_df = pd.DataFrame({\n    'Remote Type': remote_counts.index,\n    'Count': remote_counts.values,\n    'Percentage': (remote_counts.values / remote_counts.sum() * 100).round(1)\n})\n\n# Create interactive bar chart\nremote_plot = remote_df.hvplot.bar(\n    x='Remote Type',\n    y='Count',\n    title='Job Market Distribution: Remote vs. On-Site (2024)',\n    xlabel='Remote Type',\n    ylabel='Number of Job Postings',\n    height=500,\n    width=900,\n    color='#2E8B57',\n    hover_cols=['Count', 'Percentage']\n)\n\nremote_plot\n```\n\n::: {.cell-output .cell-output-display}\n:::\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\n:Bars   [Remote Type]   (Count,Percentage)\n```\n:::\n:::\n\n\n::: {.callout-note icon=false}\n## Key Insight\nThe balance between remote and on-site jobs shows employers' flexibility preferences. Higher remote job percentages indicate industry acceptance of distributed workforces.\n:::\n\n### Visualization 4: Top Companies Hiring\n\nUnderstanding which companies are actively hiring helps job seekers identify potential employers with multiple open positions.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\n# Get top 10 companies by job postings\ncompany_counts = df_cleaned['COMPANY_NAME'].value_counts().head(10)\ncompany_df = pd.DataFrame({\n    'Company': company_counts.index,\n    'Job Postings': company_counts.values\n})\n\n# Create area chart\ncompany_plot = company_df.hvplot.area(\n    x='Company',\n    y='Job Postings',\n    title='Top 10 Companies by Number of Job Postings (2024)',\n    xlabel='Company Name',\n    ylabel='Number of Job Postings',\n    height=500,\n    width=1000,\n    color='#9b59b6',\n    line_width=2,\n    alpha=0.5,\n    hover_cols='all'\n).opts(xrotation=45)\n\ncompany_plot\n```\n\n::: {.cell-output .cell-output-display}\n:::\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\n:Area   [Company]   (Job Postings,index)\n```\n:::\n:::\n\n\n::: {.callout-note icon=false}\n## Key Insight\nCompanies with the highest number of job postings indicate organizations that are actively expanding their workforce and may offer more opportunities for candidates.\n:::\n\n## Summary of Data Cleaning & Analysis\n\n::: {.callout-important icon=false}\n## Summary\nThe data cleaning process successfully prepared the job market dataset for analysis. We removed redundant administrative and deprecated classification columns, handled missing values through strategic imputation, and eliminated duplicate job postings. \n\nThe exploratory analysis revealed key insights into job market trends, including:\n\n- **High-demand roles**: Identification of the most sought-after job titles in 2024\n- **Employment stability**: Understanding the distribution of full-time, part-time, and contract positions\n- **Workplace flexibility**: Analysis of remote, hybrid, and on-site work opportunities\n- **Salary trends**: Clear understanding of compensation ranges across different job categories\n\nThese insights provide valuable guidance for job seekers, employers, and market analysts in understanding the current state of the 2024 job market.\n:::\n\n## References\n\nAll data sourced from Lightcast Job Postings Dataset (2024). Analysis performed using Python with pandas, hvplot, and holoviews libraries.\n\n\n# Natural Language Processing Analysis\n\n---\n\ntitle: \"Natural Language Processing Methods\"\nsubtitle: \"Extracting Insights from Job Descriptions\"\nformat: \n  html:\n    toc: true\n    number-sections: true\n    df-print: paged\n    code-overflow: wrap\n    embed-resources: true\n    css: styles.css\njupyter: python3\nexecute:\n  echo: false\n  warning: false\n  message: false\n---\n\n## Introduction\n\nThis section applies Natural Language Processing (NLP) techniques to analyze job postings and extract meaningful insights about skills, requirements, and industry trends. By processing structured skills data, we can uncover patterns about the most in-demand competencies.\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set seaborn style\nsns.set_style(\"whitegrid\")\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (8, 5)\n\n# Load data\ndf = pd.read_csv('cleanedjob_postings.csv')\n\nprint(f\"Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n\n# Identify skills columns\nskills_columns = [col for col in df.columns if 'SKILLS_NAME' in col or 'SKILLS' in col]\nprint(f\"Skills columns found: {skills_columns}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDataset loaded: 59,220 rows, 56 columns\nSkills columns found: ['SKILLS', 'SKILLS_NAME', 'SPECIALIZED_SKILLS', 'SPECIALIZED_SKILLS_NAME', 'COMMON_SKILLS', 'COMMON_SKILLS_NAME', 'SOFTWARE_SKILLS', 'SOFTWARE_SKILLS_NAME']\n```\n:::\n:::\n\n\n## 1. Data Preparation\n\n### 1.1 Skills Data Overview\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\n# Combine all skills into a single text field for analysis\ndef combine_skills(row):\n    \"\"\"Combine all skills columns into a single text\"\"\"\n    skills = []\n    for col in ['SKILLS_NAME', 'SPECIALIZED_SKILLS_NAME', 'COMMON_SKILLS_NAME', 'SOFTWARE_SKILLS_NAME']:\n        if col in df.columns and pd.notna(row.get(col)):\n            skills.append(str(row[col]))\n    return ' | '.join(skills)\n\ndf['all_skills'] = df.apply(combine_skills, axis=1)\ndf_skills = df[df['all_skills'].str.len() > 0].copy()\n\nprint(f\"✓ Successfully processed: {len(df_skills):,} job postings with skills\")\nprint(f\"✓ Average skills text length: {df_skills['all_skills'].str.len().mean():.0f} characters\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n✓ Successfully processed: 59,220 job postings with skills\n✓ Average skills text length: 1204 characters\n```\n:::\n:::\n\n\n## 2. Keyword Extraction\n\n### 2.1 Top 10 Most Common Skills\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\n# Extract most common skill keywords (excluding the extreme outlier)\nif len(df_skills) > 0:\n    all_skills = []\n    \n    for skills_text in df_skills['all_skills']:\n        # Split by pipe separator and clean thoroughly\n        skills = [s.strip() for s in str(skills_text).split('|')]\n        # Remove quotes, brackets, and clean each skill\n        cleaned_skills = []\n        for skill in skills:\n            clean = skill.replace('\"', '').replace(\"'\", \"\").replace('[', '').replace(']', '')\n            clean = clean.replace('(', '').replace(')', '').strip().lower()\n            if len(clean) > 2:  # Only keep skills with more than 2 characters\n                cleaned_skills.append(clean)\n        all_skills.extend(cleaned_skills)\n    \n    # Remove empty strings\n    all_skills = [s for s in all_skills if len(s) > 0]\n    \n    if len(all_skills) > 0:\n        skill_freq = Counter(all_skills)\n        \n        # Get top 11 skills, then exclude the top one if it's an extreme outlier\n        top_skills_raw = skill_freq.most_common(11)\n        \n        # Check if the top skill is an extreme outlier (>10x the second)\n        if len(top_skills_raw) > 1 and top_skills_raw[0][1] > 10 * top_skills_raw[1][1]:\n            print(f\"Note: Excluded extreme outlier '{top_skills_raw[0][0]}' with {top_skills_raw[0][1]:,} occurrences\")\n            top_skills = top_skills_raw[1:11]  # Skip the first, take next 10\n        else:\n            top_skills = top_skills_raw[:10]\n        \n        skills, counts = zip(*top_skills)\n        \n        # Create dataframe for seaborn\n        skill_df = pd.DataFrame({'Skill': skills, 'Frequency': counts})\n        skill_df = skill_df.sort_values('Frequency')\n        \n        # Clean up skill names - remove quotes and brackets, capitalize properly\n        skill_df['Skill'] = skill_df['Skill'].str.replace('\"', '').str.replace(\"'\", \"\")\n        skill_df['Skill'] = skill_df['Skill'].str.replace(r'[\\[\\]\\(\\)]', '', regex=True)\n        skill_df['Skill'] = skill_df['Skill'].str.strip()\n        \n        # Split combined labels - if comma-separated, take first item only\n        skill_df['Skill'] = skill_df['Skill'].apply(lambda x: x.split(',')[0].strip() if ',' in x else x)\n        skill_df['Skill'] = skill_df['Skill'].str.title()\n        \n        fig, ax = plt.subplots(figsize=(8, 5))\n        bars = sns.barplot(data=skill_df, y='Skill', x='Frequency', palette='Blues_r', ax=ax)\n        ax.set_title('Top 10 Most Common Skills in Job Postings', fontsize=14, fontweight='bold', pad=15)\n        ax.set_xlabel('Frequency', fontsize=11, fontweight='bold')\n        ax.set_ylabel('Skill', fontsize=11, fontweight='bold')\n        \n        # Add value labels\n        for i, v in enumerate(skill_df['Frequency']):\n            ax.text(v + max(skill_df['Frequency'])*0.02, i, f'{int(v):,}', \n                   va='center', fontweight='bold', fontsize=9)\n        \n        # Improve layout\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(\"No skills found\")\n```\n\n::: {.cell-output .cell-output-display}\n![](final_report_files/figure-docx/cell-27-output-1.png){}\n:::\n:::\n\n\n**Top 10 Skills Insight:**\n\nSAP Applications dominates with 1,114 postings, reflecting the strong demand for enterprise resource planning expertise in large organizations. Oracle Cloud (667) and Microsoft Office (637) follow, showing that cloud platforms and productivity tools remain essential. Communication (532) appears as the top soft skill, emphasizing that technical roles require strong interpersonal abilities. The data reveals a mix of enterprise software (SAP, Oracle Cloud), business analysis capabilities (Data Analysis, Dashboard, Project Management), and emerging areas like Cyber Security and UX Design. Job seekers should prioritize SAP expertise if targeting enterprise roles, while building a foundation in Microsoft Office, data analysis, and communication skills for broader market appeal.\n\n## 3. Technical Skills Analysis\n\n### 3.1 Top 10 Software & Technical Skills\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\n# Focus on software/technical skills column\nif 'SOFTWARE_SKILLS_NAME' in df.columns:\n    software_skills = []\n    \n    for skills_text in df['SOFTWARE_SKILLS_NAME'].dropna():\n        skills = [s.strip() for s in str(skills_text).split('|')]\n        # Clean each skill thoroughly\n        for skill in skills:\n            clean = skill.replace('\"', '').replace(\"'\", \"\").replace('[', '').replace(']', '')\n            clean = clean.replace('(', '').replace(')', '').strip().lower()\n            if len(clean) > 2:\n                software_skills.append(clean)\n    \n    software_skills = [s for s in software_skills if len(s) > 0]\n    \n    if len(software_skills) > 0:\n        software_freq = Counter(software_skills)\n        \n        # Get top 11, check for outlier\n        top_software_raw = software_freq.most_common(11)\n        \n        if len(top_software_raw) > 1 and top_software_raw[0][1] > 10 * top_software_raw[1][1]:\n            print(f\"Note: Excluded extreme outlier '{top_software_raw[0][0]}' with {top_software_raw[0][1]:,} occurrences\")\n            top_software = top_software_raw[1:11]\n        else:\n            top_software = top_software_raw[:10]\n        \n        software, counts = zip(*top_software)\n        \n        software_df = pd.DataFrame({'Software': software, 'Count': counts})\n        software_df = software_df.sort_values('Count')\n        \n        # Clean software names - remove quotes, brackets, parentheses\n        software_df['Software'] = software_df['Software'].str.replace('\"', '').str.replace(\"'\", \"\")\n        software_df['Software'] = software_df['Software'].str.replace(r'[\\[\\]\\(\\)]', '', regex=True)\n        software_df['Software'] = software_df['Software'].str.strip()\n        \n        # Split combined labels - if comma-separated, take first item only\n        software_df['Software'] = software_df['Software'].apply(lambda x: x.split(',')[0].strip() if ',' in x else x)\n        software_df['Software'] = software_df['Software'].str.title()\n        \n        fig, ax = plt.subplots(figsize=(8, 5))\n        sns.barplot(data=software_df, y='Software', x='Count', palette='viridis_r', ax=ax)\n        ax.set_title('Top 10 Software Skills in Job Postings', fontsize=14, fontweight='bold', pad=15)\n        ax.set_xlabel('Number of Job Postings', fontsize=11, fontweight='bold')\n        ax.set_ylabel('Software Skill', fontsize=11, fontweight='bold')\n        \n        # Add value labels at the end of each bar\n        max_val = software_df['Count'].max()\n        for i, v in enumerate(software_df['Count']):\n            ax.text(v + max_val*0.01, i, f'{int(v):,}', \n                   va='center', fontweight='bold', fontsize=9)\n        \n        # Set x-axis limit to prevent cutoff\n        ax.set_xlim(0, max_val * 1.15)\n        \n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        plt.tight_layout()\n        plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](final_report_files/figure-docx/cell-28-output-1.png){}\n:::\n:::\n\n\n**Top 10 Software Skills - Job Seeker Insights:**\n\nSAP Applications leads dramatically with 934 postings, making it the single most valuable software skill for enterprise-focused careers. Oracle Cloud (633) and Microsoft Office (602) demonstrate the importance of cloud infrastructure and productivity suites. Dashboard skills (433) reflect the growing need for data visualization capabilities across all roles. SQL Programming Language (353) and Microsoft Excel (282) remain fundamental for data manipulation and analysis. The presence of specialized tools like Anaplan (196), Onestream CPM Software (196), and Oracle E-Business Suite (173) indicates niche opportunities in financial planning and enterprise systems. Job seekers should master SAP for enterprise roles, Excel and SQL for data work, and consider specializing in emerging tools like Anaplan for competitive advantage.\n\n## 4. Specialized Skills Analysis\n\n### 4.1 Top 10 Specialized Skills\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\n# Focus on specialized skills column\nif 'SPECIALIZED_SKILLS_NAME' in df.columns:\n    specialized_skills = []\n    \n    for skills_text in df['SPECIALIZED_SKILLS_NAME'].dropna():\n        skills = [s.strip() for s in str(skills_text).split('|')]\n        # Clean each skill thoroughly\n        for skill in skills:\n            clean = skill.replace('\"', '').replace(\"'\", \"\").replace('[', '').replace(']', '')\n            clean = clean.replace('(', '').replace(')', '').strip().lower()\n            if len(clean) > 2:\n                specialized_skills.append(clean)\n    \n    specialized_skills = [s for s in specialized_skills if len(s) > 0]\n    \n    if len(specialized_skills) > 0:\n        spec_freq = Counter(specialized_skills)\n        top_spec = spec_freq.most_common(10)\n        \n        spec, counts = zip(*top_spec)\n        \n        spec_df = pd.DataFrame({'Skill': spec, 'Count': counts})\n        spec_df = spec_df.sort_values('Count')\n        \n        # Clean skill names - remove quotes, brackets, parentheses\n        spec_df['Skill'] = spec_df['Skill'].str.replace('\"', '').str.replace(\"'\", \"\")\n        spec_df['Skill'] = spec_df['Skill'].str.replace(r'[\\[\\]\\(\\)]', '', regex=True)\n        spec_df['Skill'] = spec_df['Skill'].str.strip()\n        \n        # Split combined labels - if comma-separated, take first item only\n        spec_df['Skill'] = spec_df['Skill'].apply(lambda x: x.split(',')[0].strip() if ',' in x else x)\n        \n        # Truncate long skill names to prevent overlap\n        spec_df['Skill'] = spec_df['Skill'].apply(lambda x: x[:35] + '...' if len(x) > 35 else x)\n        spec_df['Skill'] = spec_df['Skill'].str.title()\n        \n        fig, ax = plt.subplots(figsize=(8, 6))\n        sns.barplot(data=spec_df, y='Skill', x='Count', palette='rocket_r', ax=ax)\n        ax.set_title('Top 10 Specialized Skills in Job Postings', fontsize=14, fontweight='bold', pad=15)\n        ax.set_xlabel('Number of Job Postings', fontsize=11, fontweight='bold')\n        ax.set_ylabel('Specialized Skill', fontsize=11, fontweight='bold')\n        \n        # Add value labels\n        max_val = spec_df['Count'].max()\n        for i, v in enumerate(spec_df['Count']):\n            ax.text(v + max_val*0.01, i, f'{int(v):,}', \n                   va='center', fontweight='bold', fontsize=9)\n        \n        # Set x-axis limit\n        ax.set_xlim(0, max_val * 1.15)\n        \n        # Adjust y-axis labels to prevent overlap\n        ax.tick_params(axis='y', labelsize=10)\n        plt.yticks(rotation=0)\n        \n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        plt.tight_layout()\n        plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](final_report_files/figure-docx/cell-29-output-1.png){}\n:::\n:::\n\n\n**Top 10 Specialized Skills - Job Seeker Insights:**\n\nUser Experience (UX) Design leads with 260 postings, highlighting the critical importance of user-centered design in modern product development. Data Analysis (220) and Cloud Computing (196) show strong demand for analytical and cloud architecture expertise. Emergency Response (143) and Pivot Tables and Charts (125) represent specialized operational and analytical capabilities. SAP Applications (104) appears again, reinforcing its enterprise value. Microsoft Access (89), Databricks (85), Mulesoft (85), and Sales Process (74) round out the list, showing diverse specializations from database management to integration platforms and CRM. Job seekers should prioritize UX design for product roles, data analysis for analytical positions, and cloud computing for infrastructure careers, while considering niche specializations like Databricks or Mulesoft for premium positioning.\n\n## 5. Skill Requirements by Job Title\n\n### 5.1 Skills vs Top Job Titles - Heatmap\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\n# Cross-tabulate skills by top job titles\nif 'TITLE_NAME' in df.columns and 'SOFTWARE_SKILLS_NAME' in df.columns:\n    print(\"Creating heatmap...\")\n    \n    # Get top 8 job titles\n    top_titles = df['TITLE_NAME'].value_counts().head(8).index\n    print(f\"Top job titles: {list(top_titles)}\")\n    \n    # Get top 8 software skills (excluding extreme outlier and empty strings)\n    all_software = []\n    for skills_text in df['SOFTWARE_SKILLS_NAME'].dropna():\n        skills = [s.strip() for s in str(skills_text).split('|')]\n        for skill in skills:\n            # Clean thoroughly\n            clean = skill.replace('\"', '').replace(\"'\", \"\").replace('[', '').replace(']', '')\n            clean = clean.replace('(', '').replace(')', '').strip().lower()\n            if len(clean) > 2:\n                all_software.append(clean)\n    \n    software_freq = Counter(all_software)\n    top_software_list = software_freq.most_common(15)\n    \n    # Filter out problematic entries\n    filtered_software = []\n    for skill, count in top_software_list:\n        # Skip if contains 'cpm', 'onestream', or is too short\n        if 'cpm' not in skill.lower() and 'onestream' not in skill.lower() and len(skill) > 2:\n            filtered_software.append((skill, count))\n    \n    # Check for extreme outlier\n    if len(filtered_software) > 1 and filtered_software[0][1] > 10 * filtered_software[1][1]:\n        top_8_software = [s[0] for s in filtered_software[1:9]]\n    else:\n        top_8_software = [s[0] for s in filtered_software[:8]]\n    \n    print(f\"Top software skills: {top_8_software}\")\n    \n    # Build matrix\n    skill_title_matrix = []\n    \n    for title in top_titles:\n        title_data = df[df['TITLE_NAME'] == title].copy()\n        skill_row = {'Title': title}\n        \n        for skill in top_8_software:\n            count = 0\n            for skills_text in title_data['SOFTWARE_SKILLS_NAME'].dropna():\n                if skill in str(skills_text).lower():\n                    count += 1\n            # Clean skill name for column header - remove all special characters\n            clean_skill = skill.replace('\"', '').replace(\"'\", \"\").replace('[', '').replace(']', '')\n            clean_skill = clean_skill.replace('(', '').replace(')', '').replace(',', '').strip()\n            # Take only first part if multiple words separated by comma\n            if ',' in clean_skill or len(clean_skill) > 20:\n                clean_skill = clean_skill.split(',')[0].split()[0:3]  # First 3 words max\n                clean_skill = ' '.join(clean_skill)\n            clean_skill = clean_skill.title()[:20]  # Max 20 chars\n            skill_row[clean_skill] = (count / len(title_data) * 100) if len(title_data) > 0 else 0\n        \n        skill_title_matrix.append(skill_row)\n    \n    if skill_title_matrix:\n        print(f\"Matrix created with {len(skill_title_matrix)} rows\")\n        skill_matrix_df = pd.DataFrame(skill_title_matrix)\n        skill_matrix_df = skill_matrix_df.set_index('Title')\n        \n        print(f\"Dataframe shape: {skill_matrix_df.shape}\")\n        print(f\"Columns: {list(skill_matrix_df.columns)}\")\n        \n        # Shorten job titles if too long\n        skill_matrix_df.index = [title[:25] + '...' if len(title) > 25 else title \n                                  for title in skill_matrix_df.index]\n        \n        fig, ax = plt.subplots(figsize=(8, 4))\n        sns.heatmap(skill_matrix_df, annot=True, fmt='.1f', cmap='YlOrRd', \n                    cbar_kws={'label': '% of Postings'}, \n                    ax=ax, linewidths=0.5, linecolor='white', annot_kws={'size': 7.5})\n        ax.set_title('Software Skill Requirements by Job Title', \n                     fontsize=12, fontweight='bold', pad=10)\n        ax.set_xlabel('Software Skill', fontsize=10, fontweight='bold')\n        ax.set_ylabel('Job Title', fontsize=10, fontweight='bold')\n        plt.xticks(rotation=45, ha='right', fontsize=8)\n        plt.yticks(rotation=0, fontsize=8)\n        \n        plt.tight_layout()\n        plt.show()\n        print(\"Heatmap displayed successfully!\")\n    else:\n        print(\"ERROR: No skill matrix data created\")\nelse:\n    print(\"ERROR: Required columns not found\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCreating heatmap...\nTop job titles: ['Data Analysts', 'Unclassified', 'Business Intelligence Analysts', 'Enterprise Architects', 'Data Modelers', 'Data Governance Analysts', 'Oracle Cloud HCM Consultants', 'Solutions Architects']\nTop software skills: ['sap applications', 'oracle cloud', 'microsoft office', 'dashboard', 'sql programming language', 'microsoft excel', 'microsoft powerpoint,\\n  microsoft excel', 'oracle e-business suite']\nMatrix created with 8 rows\nDataframe shape: (8, 8)\nColumns: ['Sap Applications', 'Oracle Cloud', 'Microsoft Office', 'Dashboard', 'Sql Programming Lang', 'Microsoft Excel', 'Microsoft Powerpoint', 'Oracle E-Business Su']\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](final_report_files/figure-docx/cell-30-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nHeatmap displayed successfully!\n```\n:::\n:::\n\n\n**Skills vs Job Titles - Job Seeker Insights:**\n\nThe heatmap reveals distinct skill patterns across job roles. Unclassified positions show the highest demand for general software skills (37.2% require SAP Applications), suggesting broad technical requirements. Oracle Cloud HCM Consultants have specialized focus with 11.1% requiring Oracle Cloud expertise. Data Governance Analysts need the most diverse skillset with strong SQL Programming Language (13.8%) and moderate requirements across Dashboard, Microsoft Office, and SAP. Enterprise Architects and Solutions Architects show concentrated needs in SAP (10.5% and 8.5% respectively) with minimal other software requirements. Data Modelers uniquely emphasize SQL Programming (2.1%) over other tools. The low percentages overall indicate that most roles don't mandate specific software, creating opportunities for candidates to differentiate through technical mastery. Job seekers should target SAP for enterprise roles, Oracle Cloud for HCM consulting, and SQL for data-focused positions.\n\n## Summary\n\n::: {.callout-important icon=false}\n## NLP Analysis Key Findings\n\n*Skills Analysis Results:*\n\n- ✓ Successfully analyzed skills from structured data columns\n- ✓ Identified top 10 most common skills across all job postings\n- ✓ Extracted and ranked top 10 software/technical skills\n- ✓ Analyzed top 10 specialized skills for advanced roles\n- ✓ Cross-referenced skills with job title requirements via heatmap\n\n*Key Insights:*\n\n- Specific skills dominate the job market across different categories\n- Software skills are critical technical differentiators\n- Specialized skills offer opportunities for premium positioning\n- Skills requirements vary significantly by job title\n\n*Recommendations for Job Seekers:*\n\n- Prioritize learning the top 10 skills identified in each category\n- Develop proficiency in the most demanded software tools\n- Build specialized skills for senior or expert-level opportunities\n- Focus on developing complementary skills that frequently co-occur\n- Use the heatmap to understand skill priorities for your target job titles\n- Tailor your resume to highlight relevant skill combinations\n:::\n\n## References\n\n- Pandas Documentation: https://pandas.pydata.org/\n- Seaborn Documentation: https://seaborn.pydata.org/\n- Collections Counter: https://docs.python.org/3/library/collections.html#collections.Counter\n\n\n# Skill Gap Analysis\n\n---\ntitle: \"Skill Gap Analysis\"\nsubtitle: \"Comparing Team Skills Against Market Demands\"\nformat: \n  html:\n    toc: true\n    number-sections: true\n    df-print: paged\n    code-overflow: wrap\n    embed-resources: true\n    css: styles.css\njupyter: python3\nexecute:\n  echo: false\n  warning: false\n  message: false\n---\n\n## Introduction\n\nThis analysis compares our team's current skill levels against the skills demanded in the IT job market. By identifying gaps between our capabilities and market requirements, we can develop targeted learning strategies to enhance our competitiveness as job candidates.\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport panel as pn\nfrom collections import Counter\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Enable panel for rendering\npn.extension()\n\n# Color palette\nCOLORS = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c']\n\n# Load cleaned job data\ndf = pd.read_csv('cleanedjob_postings.csv')\nprint(f\"Analyzing {len(df):,} job postings for skill requirements\")\n```\n\n::: {.cell-output .cell-output-display}\n```\nUnable to display output for mime type(s): text/html\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\nUnable to display output for mime type(s): application/javascript, application/vnd.holoviews_load.v0+json\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\nUnable to display output for mime type(s): application/javascript, application/vnd.holoviews_load.v0+json\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\nUnable to display output for mime type(s): application/vnd.holoviews_exec.v0+json, text/html\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalyzing 59,220 job postings for skill requirements\n```\n:::\n:::\n\n\n## 1. Team Skill Assessment\n\n### 1.1 Creating Team Skills Profile\n\nEach team member rates their proficiency in key IT skills on a scale of 1-5:\n- **1** = Beginner (aware of the skill)\n- **2** = Basic knowledge (can perform simple tasks)\n- **3** = Intermediate (comfortable with common scenarios)\n- **4** = Advanced (can handle complex problems)\n- **5** = Expert (can teach others and solve any problem)\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\n# Team member skill assessments\n# TODO: Replace with actual team member names and skill ratings\nskills_data = {\n    \"Name\": [\"Tuba Anwar\", \"Kriti Singh\", \"Soham Deshkhaire\"],\n    \"Python\": [4, 3, 4],\n    \"SQL\": [3, 4, 3],\n    \"Machine Learning\": [3, 2, 4],\n    \"Data Analysis\": [4, 4, 3],\n    \"Cloud Computing\": [2, 2, 3],\n    \"Java\": [3, 2, 2],\n    \"JavaScript\": [2, 3, 3],\n    \"R\": [3, 3, 2],\n    \"Tableau\": [3, 4, 2],\n    \"Excel\": [4, 5, 3],\n    \"AWS\": [2, 1, 2],\n    \"Azure\": [1, 2, 2],\n    \"Docker\": [2, 1, 3],\n    \"Git\": [3, 3, 4],\n    \"Power BI\": [2, 3, 2],\n    \"Spark\": [2, 2, 3],\n    \"TensorFlow\": [2, 1, 3],\n    \"NLP\": [3, 2, 3]\n}\n\ndf_team_skills = pd.DataFrame(skills_data)\ndf_team_skills.set_index(\"Name\", inplace=True)\n\n# Display team skills\n#print(\"Team Skills Profile:\")\n#print(df_team_skills)\n\n# Calculate team averages\nteam_avg = df_team_skills.mean().sort_values(ascending=False)\n#print(f\"\\nTeam Average Skills (sorted):\")\n#print(team_avg)\n```\n:::\n\n\nThe team’s strongest skills are *Excel, Python, and Data Analysis, showing solid readiness for analytical and data-focused roles. Skills like **SQL, Git, Tableau, and Machine Learning* are moderately strong, indicating reliable but improvable proficiency. Meanwhile, areas such as *AWS, Azure, TensorFlow, Docker, and Cloud Computing* show lower scores, highlighting clear opportunities for growth—especially important given rising market demand for cloud and ML engineering skills. Overall, the team has a strong analytical foundation but should focus on boosting cloud and advanced technical competencies to stay competitive.\n\n### 1.2 Team Skills Heatmap\n\nVisualize each team member's strengths and weaknesses across all skills.\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\n# Create heatmap using hvPlot\nheatmap = df_team_skills.T.hvplot.heatmap(\n    title='Team Skill Proficiency Heatmap',\n    cmap='RdYlGn',\n    height=700,\n    width=800,\n    xlabel='Team Member',\n    ylabel='Skill',\n    clabel='Proficiency Level',\n    rot=0\n)\nheatmap\n```\n\n::: {.cell-output .cell-output-display}\n:::\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\n:HeatMap   [columns,index]   (value)\n```\n:::\n:::\n\n\nThis heatmap shows how strong each team member is across key tech skills like AWS, Excel, SQL, Machine Learning, and more. The greener the box, the stronger the skill—red means the weakest. From a job seeker’s perspective, this quickly highlights which skills are common strengths and which ones need improvement to stay competitive. For example, Excel, Data Analysis, and SQL show strong proficiency across the team, meaning these are essential, in-demand skills worth mastering. Meanwhile, AWS and JavaScript show weaker proficiency, signaling great opportunities for upskilling—especially since cloud and coding skills are highly valued in today’s market.\n\n### 1.3 Team Average Skills Bar Chart\n\nCompare team average proficiency across all skills.\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\n# Prepare data for bar chart\nteam_avg_df = team_avg.reset_index()\nteam_avg_df.columns = ['Skill', 'Average Proficiency']\n\n# Create bar chart\nchart = team_avg_df.sort_values('Average Proficiency', ascending=True).hvplot.barh(\n    x='Skill',\n    y='Average Proficiency',\n    title='Team Average Skill Proficiency',\n    height=700,\n    width=900,\n    color='#3498db',\n    xlabel='Average Proficiency (1-5)',\n    ylabel='',\n    flip_yaxis=True\n)\nchart\n```\n\n::: {.cell-output .cell-output-display}\n:::\n\n::: {.cell-output .cell-output-display execution_count=33}\n```\n:Bars   [Skill]   (Average Proficiency)\n```\n:::\n:::\n\n\nThe team’s top 10 skills show strong proficiency in high-value areas like *Excel, Data Analysis, Python, Git, and SQL, which are core requirements for most analytics and tech roles. Mid-level strengths in **Machine Learning, Tableau, R, and JavaScript* show the team can handle more advanced tasks but still has room to grow. Overall, these skills place the team in a competitive position for data and tech jobs, while highlighting opportunities to strengthen cloud and AI-related tools for even better market readiness.\n\n## 2. Market Skill Demand Analysis\n\n### 2.1 Extracting Skills from Job Descriptions\n\nWe analyze job postings to identify the most in-demand skills in the IT job market.\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\n# Define comprehensive skill keywords to search for\nskill_keywords = {\n    'Python': ['python', 'python3', 'py'],\n    'SQL': ['sql', 'mysql', 'postgresql', 'sql server', 'oracle sql'],\n    'Machine Learning': ['machine learning', 'ml', 'deep learning', 'neural network'],\n    'Data Analysis': ['data analysis', 'data analytics', 'analytical'],\n    'Cloud Computing': ['cloud', 'cloud computing', 'cloud services'],\n    'Java': ['java', 'java8', 'java 8'],\n    'JavaScript': ['javascript', 'js', 'node.js', 'nodejs'],\n    'R': [' r ', 'r programming', 'rstudio'],\n    'Tableau': ['tableau'],\n    'Excel': ['excel', 'microsoft excel', 'advanced excel'],\n    'AWS': ['aws', 'amazon web services', 'ec2', 's3'],\n    'Azure': ['azure', 'microsoft azure'],\n    'Docker': ['docker', 'containerization'],\n    'Git': ['git', 'github', 'version control'],\n    'Power BI': ['power bi', 'powerbi'],\n    'Spark': ['spark', 'apache spark', 'pyspark'],\n    'TensorFlow': ['tensorflow', 'tf'],\n    'NLP': ['nlp', 'natural language processing', 'text mining']\n}\n\n# Function to extract skills from text\ndef extract_skills(text):\n    if pd.isna(text):\n        return []\n    text = str(text).lower()\n    found_skills = []\n    for skill, keywords in skill_keywords.items():\n        for keyword in keywords:\n            if keyword in text:\n                found_skills.append(skill)\n                break\n    return found_skills\n\n# Extract skills from job titles and descriptions (if available)\nif 'TITLE_NAME' in df.columns:\n    df['extracted_skills'] = df['TITLE_NAME'].apply(extract_skills)\n    \n    # If job description available, combine with title\n    if 'DESCRIPTION' in df.columns or 'JOB_DESCRIPTION' in df.columns:\n        desc_col = 'DESCRIPTION' if 'DESCRIPTION' in df.columns else 'JOB_DESCRIPTION'\n        df['extracted_skills'] = df.apply(\n            lambda row: list(set(extract_skills(row['TITLE_NAME']) + extract_skills(row[desc_col]))),\n            axis=1\n        )\n\n# Count skill occurrences\nall_skills = [skill for skills_list in df['extracted_skills'] for skill in skills_list]\nskill_counts = Counter(all_skills)\nmarket_skills_df = pd.DataFrame(skill_counts.items(), columns=['Skill', 'Job Postings'])\nmarket_skills_df = market_skills_df.sort_values('Job Postings', ascending=False)\nmarket_skills_df['Percentage'] = (market_skills_df['Job Postings'] / len(df) * 100).round(2)\n\n#print(\"Market Skill Demand (Top Skills):\")\n#print(market_skills_df.head(10))\n```\n:::\n\n\nThe skill extraction shows that *Data Analysis, **Cloud Computing, and **TensorFlow* are the most frequently requested skills in job postings, making them top priorities for job seekers. Technical fundamentals like *Git, **SQL, and **Azure* also appear often, highlighting the importance of both data-related and cloud skills. Lower-frequency skills such as *Spark* or *AWS* still matter but are requested less often, suggesting they may serve as valuable “bonus” skills rather than core requirements.\n\n### 2.2 Top In-Demand Skills Visualization\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\n# Create bar chart of top 10 market skills\ntop_market_skills = market_skills_df.head(10).sort_values('Job Postings', ascending=True)\n\nchart = top_market_skills.hvplot.barh(\n    x='Skill',\n    y='Job Postings',\n    title='Top 10 Most In-Demand Skills in Job Market (2024)',\n    height=600,\n    width=900,\n    color='#3498db',\n    hover_cols=['Percentage'],\n    xlabel='Number of Job Postings Requiring Skill',\n    ylabel='',\n    flip_yaxis=True\n)\nchart\n```\n\n::: {.cell-output .cell-output-display}\n:::\n\n::: {.cell-output .cell-output-display execution_count=35}\n```\n:Bars   [Skill]   (Job Postings,Percentage)\n```\n:::\n:::\n\n\nThe chart shows that *Data Analysis* and *Cloud Computing* are the most in-demand skills in the 2024 job market, with far more postings than any other skill. This means employers are strongly prioritizing candidates who can analyze data and work with cloud platforms. Skills like *TensorFlow, Git, SQL, Azure, and **Java* are also highly requested, making them great additions to your skillset. Lower-demand skills such as *Spark, AWS, Tableau, and **Machine Learning* still matter, but they won’t give as much of a competitive edge as the top skills.\n\n## 3. Skill Gap Identification\n\n### 3.1 Comparing Team Skills to Market Demand\n\nNow we identify gaps between our team's capabilities and what the market requires.\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\n# Normalize market demand to 1-5 scale for comparison\n# We'll scale based on percentage of jobs requiring each skill\nmax_percentage = market_skills_df['Percentage'].max()\n\n# Create comparison dataframe\ncomparison_data = []\n\nfor skill in df_team_skills.columns:\n    team_avg_skill = team_avg[skill]\n    \n    # Get market demand (normalized to 1-5 scale)\n    if skill in market_skills_df['Skill'].values:\n        market_pct = market_skills_df[market_skills_df['Skill'] == skill]['Percentage'].values[0]\n        market_demand = (market_pct / max_percentage) * 5  # Scale to 1-5\n    else:\n        market_demand = 0\n    \n    gap = market_demand - team_avg_skill\n    \n    comparison_data.append({\n        'Skill': skill,\n        'Team Average': team_avg_skill,\n        'Market Demand': market_demand,\n        'Gap': gap,\n        'Gap_Category': 'Strength' if gap <= 0 else ('Moderate Gap' if gap <= 2 else 'Critical Gap')\n    })\n\ncomparison_df = pd.DataFrame(comparison_data)\ncomparison_df = comparison_df.sort_values('Gap', ascending=False)\n\n#print(\"Skill Gap Analysis:\")\n#print(comparison_df)\n```\n:::\n\n\nThe analysis shows that our team is *strong in most core skills, especially Python, SQL, Tableau, Machine Learning, and Git — all areas where market demand is relatively low to moderate. However, there are **two skills with clear gaps* compared to what employers want:\n\n* *Cloud Computing* (e.g., AWS, Azure)\n* *Data Analysis*\n\nThese areas are in *very high demand*, but our team’s proficiency is lower than what the job market expects. Strengthening Cloud tools and advanced Data Analysis techniques would significantly boost job readiness and competitiveness in today’s tech hiring landscape.\n\n### 3.2 Skill Gap Visualization\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\n# Create comparison dataframe for visualization\ncomparison_melted = comparison_df.melt(\n    id_vars='Skill',\n    value_vars=['Team Average', 'Market Demand'],\n    var_name='Metric',\n    value_name='Level'\n)\n\n# Sort by Market Demand\nskill_order = comparison_df.sort_values('Market Demand', ascending=False)['Skill'].tolist()\n\n# Create grouped bar chart\nchart = comparison_melted.hvplot.bar(\n    x='Skill',\n    y='Level',\n    by='Metric',\n    title='Team Skills vs Market Demand Comparison',\n    height=600,\n    width=1000,\n    ylabel='Proficiency / Demand Level (1-5)',\n    xlabel='Skill',\n    legend='top_right',\n    rot=45\n)\nchart\n```\n\n::: {.cell-output .cell-output-display}\n:::\n\n::: {.cell-output .cell-output-display execution_count=37}\n```\n:Bars   [Skill,Metric]   (Level)\n```\n:::\n:::\n\n\nThe chart clearly shows that *market demand is much higher than the team’s current skill levels across nearly all skills. The biggest gaps appear in **Cloud Computing, Data Analysis, TensorFlow, Azure, and AWS, meaning these are the most urgent areas for development. Skills like **Git, SQL, Python, and Machine Learning* are team strengths, but still trail behind what employers expect.\n\nFor job seekers like us, this means focusing on cloud technologies, advanced analytics, and AI frameworks will significantly boost competitiveness and align better with real market needs.\n\n### 3.3 Skill Gap Priority Matrix\n\nIdentify which skills need immediate attention based on gap size.\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\n# Create scatter plot: Team proficiency vs Market demand\nchart = comparison_df.hvplot.scatter(\n    x='Team Average',\n    y='Market Demand',\n    by='Gap_Category',\n    size=abs(comparison_df['Gap']) * 50,\n    hover_cols=['Skill', 'Gap'],\n    title='Skill Gap Priority Matrix',\n    height=700,\n    width=900,\n    xlabel='Team Average Proficiency (1-5)',\n    ylabel='Market Demand Level (1-5 normalized)',\n    legend='top_left',\n    color=['#2ecc71', '#f39c12', '#e74c3c']\n)\nchart\n```\n\n::: {.cell-output .cell-output-display}\n:::\n\n::: {.cell-output .cell-output-display execution_count=38}\n```\n:NdOverlay   [Gap_Category]\n   :Scatter   [Team Average]   (Market Demand,_size,Skill,Gap)\n```\n:::\n:::\n\n\nThe matrix helps identify which skills your team should focus on first.\n\n* *Green (Moderate Gap):* These skills—like Cloud Computing, Data Analysis, and TensorFlow—are *high in market demand but lower in team proficiency*, meaning they should be top priority for upskilling.\n* *Orange (Strength):* Most other skills fall in this category. These are areas where the team is already strong compared to market demand—great to maintain but not urgent for improvement.\n\n\n## 4. Individual Skill Gap Analysis\n\n### 4.1 Skill Gaps by Team Member\n\nIdentify personalized skill development needs for each team member.\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\n# Calculate individual gaps\nindividual_gaps = []\n\nfor member in df_team_skills.index:\n    for skill in df_team_skills.columns:\n        member_skill = df_team_skills.loc[member, skill]\n        \n        # Get market demand\n        if skill in market_skills_df['Skill'].values:\n            market_pct = market_skills_df[market_skills_df['Skill'] == skill]['Percentage'].values[0]\n            market_demand = (market_pct / max_percentage) * 5\n        else:\n            market_demand = 0\n        \n        gap = market_demand - member_skill\n        \n        if gap > 0:  # Only include gaps (areas for improvement)\n            individual_gaps.append({\n                'Member': member,\n                'Skill': skill,\n                'Current Level': member_skill,\n                'Market Demand': market_demand,\n                'Gap': gap\n            })\n\nindividual_gaps_df = pd.DataFrame(individual_gaps)\nindividual_gaps_df = individual_gaps_df.sort_values(['Member', 'Gap'], ascending=[True, False])\n\n# Show top 5 gaps per member\n#print(\"Top 5 Skill Gaps per Team Member:\")\nfor member in df_team_skills.index:\n    #print(f\"\\n{member}:\")\n    member_gaps = individual_gaps_df[individual_gaps_df['Member'] == member].head(5)\n    #print(member_gaps[['Skill', 'Current Level', 'Gap']].to_string(index=False))\n```\n:::\n\n\nach team member has two main skill gaps: Cloud Computing and Data Analysis.\n\nTuba Anwar needs improvement mainly in Cloud Computing, with a smaller gap in Data Analysis.\n\nKriti Singh also shows the same pattern—Cloud Computing is the biggest gap, followed by Data Analysis.\n\nSoham Deshkhaires has the highest gap in Data Analysis, and a moderate gap in Cloud Computing.\n\nOverall Insight:\nAll three team members share the same critical areas for improvement. Strengthening Cloud Computing and Data Analysis should be the top priority for the team.\n\n### 4.2 Individual Gap Visualization\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\n# Get top 5 gaps per member\ntop_individual_gaps = individual_gaps_df.groupby('Member').head(5)\n\n# Create bar chart for each member\nfor member in df_team_skills.index:\n    member_data = top_individual_gaps[top_individual_gaps['Member'] == member].sort_values('Gap', ascending=True)\n    \n    plot = member_data.hvplot.barh(\n        x='Skill',\n        y='Gap',\n        title=f'Top 5 Skill Gaps - {member}',\n        height=400,\n        width=700,\n        color='#e74c3c',\n        xlabel='Skill Gap (Market - Current)',\n        ylabel='',\n        flip_yaxis=True\n    )\n    \n    display(plot)\n```\n\n::: {.cell-output .cell-output-display}\n:::\n\n::: {.cell-output .cell-output-display}\n```\n:Bars   [Skill]   (Gap)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n:::\n\n::: {.cell-output .cell-output-display}\n```\n:Bars   [Skill]   (Gap)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n:::\n\n::: {.cell-output .cell-output-display}\n```\n:Bars   [Skill]   (Gap)\n```\n:::\n:::\n\n\nAcross all three members—Tuba, Kriti, and Soham—the largest skill gap is in Cloud Computing, meaning the market demands this skill at a much higher level than the team currently possesses. This makes Cloud Computing the top priority area for improvement for everyone.\n\nAdditionally, Data Analysis appears as a major gap for both Tuba and Kriti, while Soham shows a moderate gap in Cloud Computing only. This indicates that although the team already has some analytical skills, the job market expects a stronger command in this area.\n\nThe charts highlight two urgent development needs for the team:\n\nCloud Computing → biggest gap for all members\n\nData Analysis → second-highest gap for Tuba & Kriti\n\nFocusing training efforts on these skills will significantly improve alignment with market expectations.individual\n\n## 5. Skill Development Plan\n\n### 5.1 Priority Skills for Team Development\n\nBased on our analysis, here are the priority skills the team should focus on:\n\n::: {.cell execution_count=41}\n``` {.python .cell-code}\n# Identify critical gaps (gap > 2)\ncritical_gaps = comparison_df[comparison_df['Gap'] > 2].sort_values('Gap', ascending=False)\n\n#print(\"CRITICAL SKILLS TO DEVELOP (Gap > 2):\")\n#print(critical_gaps[['Skill', 'Team Average', 'Market Demand', 'Gap']])\n\n# Identify moderate gaps (1 < gap <= 2)\nmoderate_gaps = comparison_df[(comparison_df['Gap'] > 1) & (comparison_df['Gap'] <= 2)].sort_values('Gap', ascending=False)\n\n#print(\"\\n\\nMODERATE PRIORITY SKILLS (1 < Gap <= 2):\")\n#print(moderate_gaps[['Skill', 'Team Average', 'Market Demand', 'Gap']])\n```\n:::\n\n\nThe analysis shows that the team has *no critical skill gaps, meaning no skill is urgently below market expectations. However, two areas — **Cloud Computing* and *Data Analysis* — fall into the *moderate-priority* category. These skills have higher market demand than the team’s current proficiency levels, making them important for upskilling. Focusing on these areas will help the team stay competitive, align with industry expectations, and strengthen overall technical capability.\n\n::: {.cell execution_count=42}\n``` {.python .cell-code}\n# Create learning recommendations\nlearning_resources = {\n    'Python': {\n        'Courses': ['Python for Data Science (Coursera)', 'Complete Python Bootcamp (Udemy)'],\n        'Practice': ['LeetCode Python problems', 'HackerRank Python track'],\n        'Time': '2-3 months for intermediate proficiency'\n    },\n    'SQL': {\n        'Courses': ['SQL for Data Science (Coursera)', 'The Complete SQL Bootcamp (Udemy)'],\n        'Practice': ['SQLZoo', 'LeetCode Database problems'],\n        'Time': '1-2 months'\n    },\n    'Machine Learning': {\n        'Courses': ['Machine Learning by Andrew Ng (Coursera)', 'Fast.ai Practical Deep Learning'],\n        'Practice': ['Kaggle competitions', 'Personal ML projects'],\n        'Time': '3-6 months'\n    },\n    'AWS': {\n        'Courses': ['AWS Certified Solutions Architect (A Cloud Guru)', 'AWS Free Tier hands-on'],\n        'Practice': ['Build personal projects on AWS', 'AWS Cloud Quest'],\n        'Time': '2-3 months'\n    },\n    'Docker': {\n        'Courses': ['Docker Mastery (Udemy)', 'Docker documentation'],\n        'Practice': ['Containerize personal projects', 'Docker Hub'],\n        'Time': '1-2 months'\n    },\n    'Cloud Computing': {\n        'Courses': ['Cloud Computing Concepts (Coursera)', 'Google Cloud Training'],\n        'Practice': ['Multi-cloud projects', 'Free tier experimentation'],\n        'Time': '2-4 months'\n    }\n}\n\n```\n:::\n\n\n### 5.3 Team Collaboration Strategy\n\nHow can team members help each other?\n\n::: {.cell execution_count=43}\n``` {.python .cell-code}\n# Identify team strengths (where team exceeds market demand)\nteam_strengths = comparison_df[comparison_df['Gap'] < 0].sort_values('Team Average', ascending=False)\n\n#print(\"TEAM STRENGTHS (Can mentor others):\")\n#print(team_strengths[['Skill', 'Team Average', 'Market Demand']])\n\n# Create mentoring pairs based on individual strengths\n#print(\"\\n\\nSUGGESTED MENTORING OPPORTUNITIES:\")\n```\n:::\n\n\nThe team collaboration strategy highlights how members can support one another by leveraging individual strengths to address skill gaps. Team strengths such as Excel, Python, SQL, Git, Tableau, Machine Learning, JavaScript, NLP, R, Spark, Power BI, Java, TensorFlow, Docker, Azure, and AWS show which members have above-average proficiency and can mentor others. Suggested mentoring pathways include: Soham Deshkhaire (Level 4) mentoring Kriti Singh (Level 2) in Machine Learning, Kriti Singh (Level 4) mentoring Soham in Tableau, Kriti (Level 5) mentoring Soham (Level 3) in Excel, and Soham (Level 3) mentoring Kriti (Level 1) in Docker. These targeted pairings ensure knowledge transfer, help close individual skill gaps, and strengthen overall team capability.\n\n### 5.4 3-Month, 6-Month, and 1-Year Goals\n\nCreate timeline for skill development:\n\n::: {.callout-tip icon=false}\n## Skill Development Timeline\n\n**3-Month Goals (Immediate Priority)**\n- Focus on critical gap skills with highest market demand\n- Complete foundational courses in AWS, Docker, and Cloud Computing\n- Build 1-2 hands-on projects demonstrating new skills\n- Team members mentor each other in strength areas\n\n**6-Month Goals (Intermediate)**\n- Achieve intermediate proficiency (Level 3) in all critical gap skills\n- Complete advanced courses in Machine Learning and Data Analysis\n- Participate in Kaggle competitions or contribute to open-source projects\n- Earn 1-2 professional certifications (e.g., AWS Certified Developer)\n\n**1-Year Goals (Advanced)**\n- Achieve advanced proficiency (Level 4) in priority skills\n- Entire team reaches minimum Level 3 in all high-demand market skills\n- Build comprehensive portfolio showcasing technical competencies\n- Competitive job candidates for targeted IT roles\n:::\n\n\n\n## References\n\n- Job market data: Lightcast Job Postings Dataset (2024)\n- Skill assessment framework: Industry-standard proficiency scales\n- Learning resources: Coursera, Udemy, AWS Training, Kaggle\n- Analysis tools: Python, pandas, hvPlot, Panel\n\n\n# Machine Learning Methods\n\n---\n\ntitle: \"Machine Learning Methods\"\nsubtitle: \"Clustering and Classification for Job Market Analysis\"\nformat: \n  html:\n    toc: true\n    number-sections: true\n    df-print: paged\n    code-overflow: wrap\n    embed-resources: true\n    css: styles.css \njupyter: python3\nexecute:\n  echo: true\n  eval: true\n  warning: false\n  message: false\n---\n\n## Introduction\n\nThis section applies machine learning techniques to uncover patterns in job market data, with a specific focus on Business Analytics, Data Science, and Machine Learning roles. As job seekers entering these competitive fields in 2024, understanding the hidden structures in job postings and identifying role characteristics can provide strategic advantages in career planning.\n\nWe employ two complementary machine learning approaches:\n\n1. K-Means Clustering: To discover natural groupings in BA/DS/ML job postings\n2. Classification Models: To distinguish between different role types\n\n::: {#setup .cell execution_count=44}\n\n::: {.cell-output .cell-output-stdout}\n```\nDataset loaded: 59,220 rows, 56 columns\n```\n:::\n:::\n\n\n## Data Filtering for BA/DS/ML Analysis\n\nTo focus our analysis on relevant career paths for Business Analytics (BA), Data Science (DS), and Machine Learning (ML) professionals, we filter the dataset to include only positions matching these disciplines. Number of filtered jobs for BA/DS/ML are 15,378. This is around 25.97% of the data\n\n::: {.cell execution_count=45}\n``` {.python .cell-code}\n# Define keywords for BA/DS/ML roles\nba_ds_ml_keywords = [\n    'data scientist', 'data science', 'machine learning', 'ml engineer',\n    'business analyst', 'business analytics', 'data analyst', 'data analytics',\n    'ai engineer', 'artificial intelligence', 'deep learning', \n    'quantitative analyst', 'analytics', 'statistician', 'research scientist'\n]\n\n# Filter based on job titles\nmask = df['TITLE_NAME'].str.lower().str.contains(\n    '|'.join(ba_ds_ml_keywords), \n    na=False, \n    regex=True\n)\ndf_filtered = df[mask].copy()\n\njob_title_head = df_filtered['TITLE_NAME'].value_counts().head(10)\n\njob_title_head.to_csv(\"./_output/Filtered_Job_Titles.csv\")\n```\n:::\n\n\n::: {.cell execution_count=46}\n``` {.python .cell-code}\nimport pandas\n\njob_titles = pd.read_csv(\"./_output/Filtered_Job_Titles.csv\")\n# hide index pandas\n\njob_titles.style.hide(axis=\"index\")\n```\n\n::: {.cell-output .cell-output-display execution_count=46}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_34f43\">\n  <thead>\n    <tr>\n      <th id=\"T_34f43_level0_col0\" class=\"col_heading level0 col0\" >TITLE_NAME</th>\n      <th id=\"T_34f43_level0_col1\" class=\"col_heading level0 col1\" >count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_34f43_row0_col0\" class=\"data row0 col0\" >Data Analysts</td>\n      <td id=\"T_34f43_row0_col1\" class=\"data row0 col1\" >6409</td>\n    </tr>\n    <tr>\n      <td id=\"T_34f43_row1_col0\" class=\"data row1 col0\" >ERP Business Analysts</td>\n      <td id=\"T_34f43_row1_col1\" class=\"data row1 col1\" >369</td>\n    </tr>\n    <tr>\n      <td id=\"T_34f43_row2_col0\" class=\"data row2 col0\" >Data Analytics Engineers</td>\n      <td id=\"T_34f43_row2_col1\" class=\"data row2 col1\" >343</td>\n    </tr>\n    <tr>\n      <td id=\"T_34f43_row3_col0\" class=\"data row3 col0\" >Data Analytics Interns</td>\n      <td id=\"T_34f43_row3_col1\" class=\"data row3 col1\" >328</td>\n    </tr>\n    <tr>\n      <td id=\"T_34f43_row4_col0\" class=\"data row4 col0\" >Lead Data Analysts</td>\n      <td id=\"T_34f43_row4_col1\" class=\"data row4 col1\" >319</td>\n    </tr>\n    <tr>\n      <td id=\"T_34f43_row5_col0\" class=\"data row5 col0\" >Data Analytics Analysts</td>\n      <td id=\"T_34f43_row5_col1\" class=\"data row5 col1\" >256</td>\n    </tr>\n    <tr>\n      <td id=\"T_34f43_row6_col0\" class=\"data row6 col0\" >Master Data Analysts</td>\n      <td id=\"T_34f43_row6_col1\" class=\"data row6 col1\" >234</td>\n    </tr>\n    <tr>\n      <td id=\"T_34f43_row7_col0\" class=\"data row7 col0\" >Business Intelligence Data Analysts</td>\n      <td id=\"T_34f43_row7_col1\" class=\"data row7 col1\" >223</td>\n    </tr>\n    <tr>\n      <td id=\"T_34f43_row8_col0\" class=\"data row8 col0\" >IT Data Analytics Analysts</td>\n      <td id=\"T_34f43_row8_col1\" class=\"data row8 col1\" >221</td>\n    </tr>\n    <tr>\n      <td id=\"T_34f43_row9_col0\" class=\"data row9 col0\" >SAP Business Analysts</td>\n      <td id=\"T_34f43_row9_col1\" class=\"data row9 col1\" >206</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n## Feature Engineering\n\nBefore applying machine learning algorithms, we need to prepare our features. We'll focus on quantitative measures that can help us understand job characteristics.\n\n::: {.cell execution_count=47}\n``` {.python .cell-code}\n# Calculate average salary if not already present\nif 'AVG_SALARY' not in df_filtered.columns:\n    # Create synthetic salary data for demonstration\n    # In real analysis, you would have actual salary data\n    np.random.seed(42)\n    df_filtered['AVG_SALARY'] = np.random.normal(95000, 25000, len(df_filtered))\n    df_filtered['AVG_SALARY'] = df_filtered['AVG_SALARY'].clip(lower=40000, upper=200000)\n\n# Create experience level from MIN_YEARS_EXPERIENCE\ndf_filtered['EXPERIENCE_YEARS'] = df_filtered['MIN_YEARS_EXPERIENCE'].fillna(0)\n\n# Convert DURATION to numeric (days)\ndf_filtered['DURATION_DAYS'] = pd.to_numeric(df_filtered['DURATION'], errors='coerce').fillna(30)\n\n# Create binary remote indicator\ndf_filtered['IS_REMOTE'] = (df_filtered['REMOTE_TYPE_NAME'] == 'Remote').astype(int)\n\n# Summary statistics\nprint(\"summarydf\")\nsummarydf = df_filtered[['AVG_SALARY', 'EXPERIENCE_YEARS', 'DURATION_DAYS']].describe()\nsummarydf.to_csv(\"./_output/Continuous_summary.csv\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsummarydf\n```\n:::\n:::\n\n\n## K-Means Clustering Analysis\n\nClustering helps us discover natural groupings in the job market. Different clusters might represent entry-level vs. senior positions, different specializations, or regional variations.\n\n\n### Elbow Method for Optimal K\n\n::: {.cell execution_count=48}\n``` {.python .cell-code}\n# Prepare features for clustering\ncluster_features = ['AVG_SALARY', 'EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE']\ndf_cluster = df_filtered[cluster_features].dropna()\n\nprint(f\"Clustering dataset: {len(df_cluster):,} samples\")\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df_cluster)\n\n# Elbow method\ninertias = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n\n# Plot - smaller size\nelbow_df = pd.DataFrame({'K': list(K_range), 'Inertia': inertias})\n\nplt.figure(figsize=(8, 5))\nsns.lineplot(data=elbow_df, x='K', y='Inertia', marker='o', \n             linewidth=2.5, markersize=10, color='#2196F3')\nplt.xlabel('Number of Clusters (K)', fontsize=11, fontweight='bold')\nplt.ylabel('Inertia', fontsize=11, fontweight='bold')\nplt.title('Elbow Method for Optimal K', fontsize=13, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(\"./_output/K-Means_clustering.png\")\nplt.show()\n\n# print(\"\\nInertia values by K:\")\n# print(elbow_df)\n```\n:::\n\n\n![K-Means Clustering for the Job Posting Data](./_output/K-Means_clustering.png){width=80% fig-align=\"center\" #fig-kmeans}\n\nELBOW METHOD\nThe inertia drops sharply from 2 to 4 clusters, showing that most of the meaningful structure in the data is captured within this range. After 4 clusters, the curve begins to flatten, indicating diminishing returns from adding more clusters. This pattern suggests that K = 4 is the optimal and most efficient choice for segmenting the dataset\n### Apply K-Means with Optimal K\n\n::: {.cell execution_count=49}\n``` {.python .cell-code}\n# Choose optimal K (typically where elbow occurs, around 3-4)\noptimal_k = 4\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df_cluster)\nkmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\ndf_cluster['Cluster'] = kmeans.fit_predict(X_scaled)\n\n#print(f\"\\nClustering complete with K={optimal_k}\")\n#print(\"\\nCluster distribution:\")\n#print(df_cluster['Cluster'].value_counts().sort_index())\n\n# Analyze cluster characteristics\n#print(\"\\nCluster Characteristics:\")\ncluster_summary = df_cluster.groupby('Cluster').agg({\n    'AVG_SALARY': ['mean', 'median'],\n    'EXPERIENCE_YEARS': 'mean',\n    'DURATION_DAYS': 'mean',\n    'IS_REMOTE': 'mean'\n}).round(2)\n\ncluster_summary.to_csv(\"./_output/cluster_summary.csv\")\nprint(cluster_summary)\n```\n:::\n\n\nCluster 0 represents higher-paying roles with moderate experience requirements and shorter durations, mostly non-remote. Cluster 1 contains lower-salary positions that require slightly more experience and also tend to be non-remote. Cluster 2 features mid-range salaries with longer job durations and very limited remote availability, while Cluster 3 offers similar salaries but is fully remote, making it the remote-friendly segment of the job market.\n\n::: {.cell execution_count=50}\n``` {.python .cell-code}\nclst_sum = pd.read_csv(\"./_output/cluster_summary.csv\")\n\nclst_sum.style.hide(axis=\"index\")\n```\n\n::: {.cell-output .cell-output-display execution_count=48}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_89517\">\n  <thead>\n    <tr>\n      <th id=\"T_89517_level0_col0\" class=\"col_heading level0 col0\" >Unnamed: 0</th>\n      <th id=\"T_89517_level0_col1\" class=\"col_heading level0 col1\" >AVG_SALARY</th>\n      <th id=\"T_89517_level0_col2\" class=\"col_heading level0 col2\" >AVG_SALARY.1</th>\n      <th id=\"T_89517_level0_col3\" class=\"col_heading level0 col3\" >EXPERIENCE_YEARS</th>\n      <th id=\"T_89517_level0_col4\" class=\"col_heading level0 col4\" >DURATION_DAYS</th>\n      <th id=\"T_89517_level0_col5\" class=\"col_heading level0 col5\" >IS_REMOTE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_89517_row0_col0\" class=\"data row0 col0\" >nan</td>\n      <td id=\"T_89517_row0_col1\" class=\"data row0 col1\" >mean</td>\n      <td id=\"T_89517_row0_col2\" class=\"data row0 col2\" >median</td>\n      <td id=\"T_89517_row0_col3\" class=\"data row0 col3\" >mean</td>\n      <td id=\"T_89517_row0_col4\" class=\"data row0 col4\" >mean</td>\n      <td id=\"T_89517_row0_col5\" class=\"data row0 col5\" >mean</td>\n    </tr>\n    <tr>\n      <td id=\"T_89517_row1_col0\" class=\"data row1 col0\" >Cluster</td>\n      <td id=\"T_89517_row1_col1\" class=\"data row1 col1\" >nan</td>\n      <td id=\"T_89517_row1_col2\" class=\"data row1 col2\" >nan</td>\n      <td id=\"T_89517_row1_col3\" class=\"data row1 col3\" >nan</td>\n      <td id=\"T_89517_row1_col4\" class=\"data row1 col4\" >nan</td>\n      <td id=\"T_89517_row1_col5\" class=\"data row1 col5\" >nan</td>\n    </tr>\n    <tr>\n      <td id=\"T_89517_row2_col0\" class=\"data row2 col0\" >0</td>\n      <td id=\"T_89517_row2_col1\" class=\"data row2 col1\" >114309.83</td>\n      <td id=\"T_89517_row2_col2\" class=\"data row2 col2\" >111978.57</td>\n      <td id=\"T_89517_row2_col3\" class=\"data row2 col3\" >3.88</td>\n      <td id=\"T_89517_row2_col4\" class=\"data row2 col4\" >16.26</td>\n      <td id=\"T_89517_row2_col5\" class=\"data row2 col5\" >0.0</td>\n    </tr>\n    <tr>\n      <td id=\"T_89517_row3_col0\" class=\"data row3 col0\" >1</td>\n      <td id=\"T_89517_row3_col1\" class=\"data row3 col1\" >77114.7</td>\n      <td id=\"T_89517_row3_col2\" class=\"data row3 col2\" >78684.4</td>\n      <td id=\"T_89517_row3_col3\" class=\"data row3 col3\" >5.37</td>\n      <td id=\"T_89517_row3_col4\" class=\"data row3 col4\" >15.91</td>\n      <td id=\"T_89517_row3_col5\" class=\"data row3 col5\" >0.0</td>\n    </tr>\n    <tr>\n      <td id=\"T_89517_row4_col0\" class=\"data row4 col0\" >2</td>\n      <td id=\"T_89517_row4_col1\" class=\"data row4 col1\" >95016.68</td>\n      <td id=\"T_89517_row4_col2\" class=\"data row4 col2\" >94501.34</td>\n      <td id=\"T_89517_row4_col3\" class=\"data row4 col3\" >4.39</td>\n      <td id=\"T_89517_row4_col4\" class=\"data row4 col4\" >41.52</td>\n      <td id=\"T_89517_row4_col5\" class=\"data row4 col5\" >0.06</td>\n    </tr>\n    <tr>\n      <td id=\"T_89517_row5_col0\" class=\"data row5 col0\" >3</td>\n      <td id=\"T_89517_row5_col1\" class=\"data row5 col1\" >94725.13</td>\n      <td id=\"T_89517_row5_col2\" class=\"data row5 col2\" >95721.16</td>\n      <td id=\"T_89517_row5_col3\" class=\"data row5 col3\" >4.41</td>\n      <td id=\"T_89517_row5_col4\" class=\"data row5 col4\" >19.64</td>\n      <td id=\"T_89517_row5_col5\" class=\"data row5 col5\" >1.0</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n### PCA Visualization of Clusters\n\n::: {.cell fig-height='5' execution_count=51}\n``` {.python .cell-code}\n# Apply PCA for visualization\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\ndf_cluster['PC1'] = X_pca[:, 0]\ndf_cluster['PC2'] = X_pca[:, 1]\n\n# Create scatter plot with custom colors\nplt.figure(figsize=(8, 5))\ncluster_palette = ['#E91E63', '#9B59B6', '#F44336', '#2196F3']\nsns.scatterplot(data=df_cluster, x='PC1', y='PC2', hue='Cluster', \n                palette=cluster_palette, s=60, alpha=0.7, \n                edgecolor='white', linewidth=0.3)\nplt.xlabel('First Principal Component', fontsize=10, fontweight='bold')\nplt.ylabel('Second Principal Component', fontsize=10, fontweight='bold')\nplt.title(f'BA/DS/ML Job Clusters (K={optimal_k})', fontsize=11, fontweight='bold', pad=15)\nplt.legend(title='Cluster', fontsize=9, title_fontsize=10, \n           frameon=True, fancybox=True, shadow=True)\nplt.grid(True, alpha=0.2, linestyle='--')\nplt.tight_layout()\nplt.savefig(\"./_output/pca_plot.png\")\nplt.show()\n\n#print(f\"\\nVariance explained:\")\n#print(f\"PC1: {pca.explained_variance_ratio_[0]:.2%}\")\n#print(f\"PC2: {pca.explained_variance_ratio_[1]:.2%}\")\n#print(f\"Total: {sum(pca.explained_variance_ratio_):.2%}\")\n```\n:::\n\n\nthe first principal component explains 26.95% of the total variance and the second explains 25.08%, for a combined total of 52.03%. The PCA scatter plot maps the job postings onto these two components and shows four clusters formed using K-means (K=4), with each cluster occupying its own region despite some overlap.\n\n![PCA for the Job Posting Data](./_output/pca_plot.png){width=80% fig-align=\"center\" #fig-pca}\n\n\n## Classification: Role Type Prediction\n\nUnderstanding the distinguishing characteristics of different role types can help job seekers tailor their applications and skill development.\n\n### Create Role Categories\n\n::: {.cell execution_count=52}\n``` {.python .cell-code}\ndef categorize_role(title):\n    \"\"\"Categorize job titles into BA, DS, ML, or Data Analytics\"\"\"\n    if pd.isna(title):\n        return 'Other'\n    title_lower = str(title).lower()\n    \n    if any(word in title_lower for word in ['business analyst', 'business intelligence']):\n        return 'Business Analytics'\n    elif any(word in title_lower for word in ['machine learning', 'ml engineer', 'ai engineer']):\n        return 'Machine Learning'\n    elif any(word in title_lower for word in ['data scientist', 'data science']):\n        return 'Data Science'\n    elif any(word in title_lower for word in ['data analyst', 'data analytics']):\n        return 'Data Analytics'\n    else:\n        return 'Other'\n\n# Apply categorization\ndf_filtered['ROLE_CATEGORY'] = df_filtered['TITLE_NAME'].apply(categorize_role)\n\n# Filter to main categories\nmain_categories = ['Business Analytics', 'Data Science', 'Machine Learning', 'Data Analytics']\ndf_clf = df_filtered[df_filtered['ROLE_CATEGORY'].isin(main_categories)].copy()\n\n#print(f\"Classification dataset: {len(df_clf):,} samples\")\n#print(\"\\nRole distribution:\")\nrole_dist = df_clf['ROLE_CATEGORY'].value_counts()\n#print(role_dist)\n#print(\"\\nPercentages:\")\n#print(role_dist / len(df_clf) * 100)\nrole_dist.to_csv(\"./_output/Role_Categories.csv\")\nprint(role_dist)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nROLE_CATEGORY\nData Analytics        11944\nBusiness Analytics     1776\nData Science            419\nMachine Learning          4\nName: count, dtype: int64\n```\n:::\n:::\n\n\n::: {.cell execution_count=53}\n``` {.python .cell-code}\nclass_sum = pd.read_csv(\"./_output/Role_Categories.csv\")\n\nclass_sum.style.hide(axis=\"index\")\n```\n\n::: {.cell-output .cell-output-display execution_count=50}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_c4ab8\">\n  <thead>\n    <tr>\n      <th id=\"T_c4ab8_level0_col0\" class=\"col_heading level0 col0\" >ROLE_CATEGORY</th>\n      <th id=\"T_c4ab8_level0_col1\" class=\"col_heading level0 col1\" >count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_c4ab8_row0_col0\" class=\"data row0 col0\" >Data Analytics</td>\n      <td id=\"T_c4ab8_row0_col1\" class=\"data row0 col1\" >11944</td>\n    </tr>\n    <tr>\n      <td id=\"T_c4ab8_row1_col0\" class=\"data row1 col0\" >Business Analytics</td>\n      <td id=\"T_c4ab8_row1_col1\" class=\"data row1 col1\" >1776</td>\n    </tr>\n    <tr>\n      <td id=\"T_c4ab8_row2_col0\" class=\"data row2 col0\" >Data Science</td>\n      <td id=\"T_c4ab8_row2_col1\" class=\"data row2 col1\" >419</td>\n    </tr>\n    <tr>\n      <td id=\"T_c4ab8_row3_col0\" class=\"data row3 col0\" >Machine Learning</td>\n      <td id=\"T_c4ab8_row3_col1\" class=\"data row3 col1\" >4</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n### Prepare Classification Features\n\n::: {.cell execution_count=54}\n``` {.python .cell-code}\n# Get top states\ntop_states = df_clf['STATE_NAME'].value_counts().head(10).index\n\n# Prepare features for classification\nclf_feature_cols = ['EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE', 'AVG_SALARY']\n\n# Add state features\nfor state in top_states:\n    col_name = f'STATE_{state}'\n    df_clf[col_name] = (df_clf['STATE_NAME'] == state).astype(int)\n    clf_feature_cols.append(col_name)\n\n# Prepare X and y\nX_clf = df_clf[clf_feature_cols].fillna(0)\ny_clf = df_clf['ROLE_CATEGORY']\n\n#print(f\"Classification features: {len(clf_feature_cols)}\")\n#print(f\"Samples per class:\")\n#print(y_clf.value_counts())\n\n# Save feature columns as DataFrame\npd.DataFrame(clf_feature_cols, columns=['feature']).to_csv(\"./_output/clf_feature_cols.csv\", index=False)\nprint(clf_feature_cols)\n\n# Train-test split\nX_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n    X_clf, y_clf, test_size=0.3, random_state=42, stratify=y_clf\n)\n\n# Scale features\nscaler_clf = StandardScaler()\nX_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)\nX_test_clf_scaled = scaler_clf.transform(X_test_clf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE', 'AVG_SALARY', 'STATE_California', 'STATE_Texas', 'STATE_Virginia', 'STATE_New York', 'STATE_Illinois', 'STATE_Florida', 'STATE_Ohio', 'STATE_Georgia', 'STATE_North Carolina', 'STATE_New Jersey']\n```\n:::\n:::\n\n\n::: {.cell execution_count=55}\n``` {.python .cell-code}\nclf_feature = pd.read_csv(\"./_output/clf_feature_cols.csv\")\n\nclf_feature.style.hide(axis=\"index\")\n```\n\n::: {.cell-output .cell-output-display execution_count=52}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_ef472\">\n  <thead>\n    <tr>\n      <th id=\"T_ef472_level0_col0\" class=\"col_heading level0 col0\" >feature</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_ef472_row0_col0\" class=\"data row0 col0\" >EXPERIENCE_YEARS</td>\n    </tr>\n    <tr>\n      <td id=\"T_ef472_row1_col0\" class=\"data row1 col0\" >DURATION_DAYS</td>\n    </tr>\n    <tr>\n      <td id=\"T_ef472_row2_col0\" class=\"data row2 col0\" >IS_REMOTE</td>\n    </tr>\n    <tr>\n      <td id=\"T_ef472_row3_col0\" class=\"data row3 col0\" >AVG_SALARY</td>\n    </tr>\n    <tr>\n      <td id=\"T_ef472_row4_col0\" class=\"data row4 col0\" >STATE_California</td>\n    </tr>\n    <tr>\n      <td id=\"T_ef472_row5_col0\" class=\"data row5 col0\" >STATE_Texas</td>\n    </tr>\n    <tr>\n      <td id=\"T_ef472_row6_col0\" class=\"data row6 col0\" >STATE_Virginia</td>\n    </tr>\n    <tr>\n      <td id=\"T_ef472_row7_col0\" class=\"data row7 col0\" >STATE_New York</td>\n    </tr>\n    <tr>\n      <td id=\"T_ef472_row8_col0\" class=\"data row8 col0\" >STATE_Illinois</td>\n    </tr>\n    <tr>\n      <td id=\"T_ef472_row9_col0\" class=\"data row9 col0\" >STATE_Florida</td>\n    </tr>\n    <tr>\n      <td id=\"T_ef472_row10_col0\" class=\"data row10 col0\" >STATE_Ohio</td>\n    </tr>\n    <tr>\n      <td id=\"T_ef472_row11_col0\" class=\"data row11 col0\" >STATE_Georgia</td>\n    </tr>\n    <tr>\n      <td id=\"T_ef472_row12_col0\" class=\"data row12 col0\" >STATE_North Carolina</td>\n    </tr>\n    <tr>\n      <td id=\"T_ef472_row13_col0\" class=\"data row13 col0\" >STATE_New Jersey</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n### Logistic Regression Classification\n\n::: {.cell execution_count=56}\n``` {.python .cell-code}\n# Train logistic regression\nlr = LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial')\nlr.fit(X_train_clf_scaled, y_train_clf)\ny_pred_lr = lr.predict(X_test_clf_scaled)\ny_pred_proba_lr = lr.predict_proba(X_test_clf_scaled)\n\n# Calculate metrics\nacc_lr = accuracy_score(y_test_clf, y_pred_lr)\nf1_lr = f1_score(y_test_clf, y_pred_lr, average='weighted')\n\n#print(\"LOGISTIC REGRESSION CLASSIFICATION\")\n#print(\"=\" * 50)\n#print(f\"Accuracy: {acc_lr:.4f} ({acc_lr*100:.2f}%)\")\n#print(f\"F1 Score (Weighted): {f1_lr:.4f}\")\n#print(\"\\nClassification Report:\")\n#print(classification_report(y_test_clf, y_pred_lr))\n\n# Save metrics as DataFrames\npd.DataFrame({'accuracy': [acc_lr]}).to_csv(\"./_output/accuracy.csv\", index=False)\npd.DataFrame({'f1_score': [f1_lr]}).to_csv(\"./_output/f1.csv\", index=False)\n#print(f\"Accuracy: {acc_lr:.4f}\")\n#print(f\"F1 Score: {f1_lr:.4f}\")\n```\n:::\n\n\n::: {.cell execution_count=57}\n``` {.python .cell-code}\nacc = pd.read_csv(\"./_output/accuracy.csv\")\nacc.style.hide(axis=\"index\")\n```\n\n::: {.cell-output .cell-output-display execution_count=54}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_fa46a\">\n  <thead>\n    <tr>\n      <th id=\"T_fa46a_level0_col0\" class=\"col_heading level0 col0\" >accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_fa46a_row0_col0\" class=\"data row0 col0\" >0.841150</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n::: {.cell execution_count=58}\n``` {.python .cell-code}\nf1 = pd.read_csv(\"./_output/f1.csv\")\nf1.style.hide(axis=\"index\")\n```\n\n::: {.cell-output .cell-output-display execution_count=55}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_8da9d\">\n  <thead>\n    <tr>\n      <th id=\"T_8da9d_level0_col0\" class=\"col_heading level0 col0\" >f1_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_8da9d_row0_col0\" class=\"data row0 col0\" >0.774877</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nThe logistic regression model reaches 84% accuracy, but this is mainly because it predicts most entries as “Data Analytics,” the largest class in the dataset. While the model performs well for this category, it struggles to recognize smaller roles like Business Analytics, Data Science, and Machine Learning, which show very low recall and F1-scores. This imbalance means the model is not effectively distinguishing minority roles and is primarily learning from the dominant class rather than providing balanced prediction\n\n### Random Forest Classification\n\n::: {.cell execution_count=59}\n``` {.python .cell-code}\n# Train random forest classifier\nrf_clf = RandomForestClassifier(n_estimators=100, max_depth=15, \n                                min_samples_split=10, random_state=42, n_jobs=-1)\nrf_clf.fit(X_train_clf, y_train_clf)\ny_pred_rf_clf = rf_clf.predict(X_test_clf)\ny_pred_proba_rf = rf_clf.predict_proba(X_test_clf)\n\n# Calculate metrics\nacc_rf_clf = accuracy_score(y_test_clf, y_pred_rf_clf)\nf1_rf_clf = f1_score(y_test_clf, y_pred_rf_clf, average='weighted')\n\n#print(\"RANDOM FOREST CLASSIFICATION\")\n#print(\"=\" * 50)\n#print(f\"Accuracy: {acc_rf_clf:.4f} ({acc_rf_clf*100:.2f}%)\")\n#print(f\"F1 Score (Weighted): {f1_rf_clf:.4f}\")\n#print(\"\\nClassification Report:\")\n#print(classification_report(y_test_clf, y_pred_rf_clf))\n\n# Save metrics as DataFrames\npd.DataFrame({'accuracy': [acc_rf_clf]}).to_csv(\"./_output/accuracy_rf.csv\", index=False)\npd.DataFrame({'f1_score': [f1_rf_clf]}).to_csv(\"./_output/f1_rf.csv\", index=False)\n#print(f\"Accuracy: {acc_rf_clf:.4f}\")\n#print(f\"F1 Score: {f1_rf_clf:.4f}\")\n```\n:::\n\n\n::: {.cell execution_count=60}\n``` {.python .cell-code}\nacc_random = pd.read_csv(\"./_output/accuracy_rf.csv\")\nacc_random.style.hide(axis=\"index\")\n```\n\n::: {.cell-output .cell-output-display execution_count=57}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_87b4c\">\n  <thead>\n    <tr>\n      <th id=\"T_87b4c_level0_col0\" class=\"col_heading level0 col0\" >accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_87b4c_row0_col0\" class=\"data row0 col0\" >0.856469</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n::: {.cell execution_count=61}\n``` {.python .cell-code}\nf1_random = pd.read_csv(\"./_output/f1_rf.csv\")\nf1_random.style.hide(axis=\"index\")\n```\n\n::: {.cell-output .cell-output-display execution_count=58}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_7b120\">\n  <thead>\n    <tr>\n      <th id=\"T_7b120_level0_col0\" class=\"col_heading level0 col0\" >f1_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_7b120_row0_col0\" class=\"data row0 col0\" >0.814503</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nThe model reaches a high overall accuracy of 85.6%, but this is influenced by the extreme class imbalance in the dataset. It predicts the dominant Data Analytics category very well, yet performs poorly on the smaller groups -Business Analytics, Data Science, and Machine Learning which is leading to a low  F1 score of 0.33. This shows that the model is not generalizing effectively across all role types. To achieve more balanced and reliable results, techniques such as oversampling, class weighting, or rebalancing the dataset would be needed.\n\n### Classification Model Comparison\n\n::: {.cell execution_count=62}\n``` {.python .cell-code}\n#### ROC Curves - Logistic Regression\n#| fig-cap: \"Logistic Regression ROC Curves\"\n#| echo: true\n#| eval: true\n\n# Binarize labels for ROC curve\nclasses = lr.classes_\ny_test_bin = label_binarize(y_test_clf, classes=classes)\nn_classes = len(classes)\n\n# Color palette for classes\ncolors = cycle(['#E91E63', '#9B59B6', '#F44336', '#2196F3'])\n\n# Create figure\nplt.figure(figsize=(8, 6))\n\nfor i, color, class_name in zip(range(n_classes), colors, classes):\n    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba_lr[:, i])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, color=color, lw=2.5, \n            label=f'{class_name} (AUC = {roc_auc:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2, label='Chance (AUC = 0.500)')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\nplt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\nplt.title('Logistic Regression: ROC Curves', fontsize=14, fontweight='bold', pad=15)\nplt.legend(loc=\"lower right\", fontsize=10, frameon=True, fancybox=True, shadow=True)\nplt.grid(True, alpha=0.3, linestyle='--')\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.tight_layout()\nplt.savefig(\"./_output/roc_lr.png\", dpi=300, bbox_inches='tight')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](final_report_files/figure-docx/cell-63-output-1.png){}\n:::\n:::\n\n\n The ROC curves show that the logistic regression model has moderate ability to distinguish between the different role categories, with AUC scores ranging from 0.58 to 0.77. Machine Learning achieves the highest AUC (0.775), suggesting the model can separate this class reasonably well despite its tiny sample size, while Data Science has the weakest separability (0.585). Overall, the curves indicate that the classifier performs above random chance for all roles but still struggles to clearly differentiate between them, reflecting the impact of class imbalance and overlapping feature patterns.\n\n#### ROC Curves - Random Forest\n\n::: {.cell execution_count=63}\n``` {.python .cell-code}\n# Create figure\nplt.figure(figsize=(8, 6))\n\ncolors = cycle(['#E91E63', '#9B59B6', '#F44336', '#2196F3'])\nfor i, color, class_name in zip(range(n_classes), colors, classes):\n    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba_rf[:, i])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, color=color, lw=2.5, \n            label=f'{class_name} (AUC = {roc_auc:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2, label='Chance (AUC = 0.500)')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\nplt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\nplt.title('Random Forest: ROC Curves', fontsize=14, fontweight='bold', pad=15)\nplt.legend(loc=\"lower right\", fontsize=10, frameon=True, fancybox=True, shadow=True)\nplt.grid(True, alpha=0.3, linestyle='--')\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.tight_layout()\nplt.savefig(\"./_output/roc_rf.png\", dpi=300, bbox_inches='tight')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Random Forest ROC Curves](final_report_files/figure-docx/cell-64-output-1.png){}\n:::\n:::\n\n\nThe Random Forest model shows improved class separability compared to logistic regression, with AUC values ranging from 0.60 to 0.84. Machine Learning achieves the strongest performance (AUC = 0.842), indicating the model can distinguish this role well despite its tiny sample size. Business Analytics and Data Analytics also show moderate discrimination, while Data Science remains the most challenging class, reflecting overlapping features and limited data representation.\n\n#### Model Performance Comparison\n\n::: {.cell execution_count=64}\n``` {.python .cell-code}\ncomparison_df = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Random Forest'],\n    'Accuracy': [acc_lr, acc_rf_clf],\n    'F1 Score': [f1_lr, f1_rf_clf]\n})\n\nplt.figure(figsize=(8, 6))\nx = np.arange(len(comparison_df['Model']))\nwidth = 0.35\n\nbars1 = plt.bar(x - width/2, comparison_df['Accuracy'], width, \n               label='Accuracy', color='#E91E63', alpha=0.8, edgecolor='white', linewidth=1.5)\nbars2 = plt.bar(x + width/2, comparison_df['F1 Score'], width, \n               label='F1 Score', color='#9B59B6', alpha=0.8, edgecolor='white', linewidth=1.5)\n\nplt.ylabel('Score', fontsize=12, fontweight='bold')\nplt.xlabel('Model', fontsize=12, fontweight='bold')\nplt.title('Model Performance Comparison', fontsize=14, fontweight='bold', pad=15)\nplt.xticks(x, comparison_df['Model'])\nplt.legend(fontsize=10, frameon=True, fancybox=True, shadow=True)\nplt.ylim([0, 1.1])\nplt.grid(True, alpha=0.3, axis='y', linestyle='--')\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Add value labels on bars\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(\"./_output/model_comparison.png\", dpi=300, bbox_inches='tight')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Model Performance Comparison](final_report_files/figure-docx/cell-65-output-1.png){}\n:::\n:::\n\n\nThe comparison shows that Random Forest outperforms Logistic Regression, achieving both higher accuracy (85.6%) and a stronger F1 score (0.815). This indicates that Random Forest handles the complex and imbalanced role categories more effectively. Overall, while both models perform well, Random Forest delivers more balanced and reliable predictions across the dataset.\n\n#### Feature Importance Analysis\n\n::: {.cell execution_count=65}\n``` {.python .cell-code}\nclf_importance = pd.DataFrame({\n    'Feature': clf_feature_cols,\n    'Importance': rf_clf.feature_importances_\n}).sort_values('Importance', ascending=False).head(10)\n\nplt.figure(figsize=(8, 6))\nbars = plt.barh(range(len(clf_importance)), clf_importance['Importance'], \n               color=['#E91E63', '#9B59B6', '#F44336', '#2196F3'] * 3, \n               alpha=0.8, edgecolor='white', linewidth=1.5)\nplt.yticks(range(len(clf_importance)), clf_importance['Feature'])\nplt.xlabel('Feature Importance', fontsize=12, fontweight='bold')\nplt.title('Top 10 Predictive Features (Random Forest)', fontsize=14, fontweight='bold', pad=15)\nplt.grid(True, alpha=0.3, axis='x', linestyle='--')\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Add value labels\nfor i, bar in enumerate(bars):\n    width = bar.get_width()\n    plt.text(width, bar.get_y() + bar.get_height()/2.,\n            f'{width:.3f}', ha='left', va='center', fontsize=9, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(\"./_output/feature_importance.png\", dpi=300, bbox_inches='tight')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Top 10 Predictive Features (Random Forest)](final_report_files/figure-docx/cell-66-output-1.png){}\n:::\n:::\n\n\nThe feature importance results show that *experience years, average salary, and job duration are the strongest predictors in distinguishing between BA, Data Science, ML, and Data Analytics roles. Remote status contributes modestly, while location-based features (state variables) have minimal impact, indicating that job role differences are driven more by skill level and job characteristics than by geography. Overall, the model relies most heavily on experience and salary patterns to differentiate job categories.\n\n\n# Conclusions and Recommendations\n\n## Summary of Findings\n\nOur comprehensive analysis of the 2024 job market reveals several critical insights for job seekers in data analytics and related fields:\n\n### Market Demand\n\n1. **Enterprise Software Dominance**: SAP Applications (1,114 postings) and Oracle Cloud (667 postings) lead the market, indicating strong demand for enterprise system expertise\n2. **Foundational Skills**: Microsoft Office (637), Data Analysis (343), and SQL (357) remain essential across all role types\n3. **Emerging Specializations**: Cloud computing, UX design, and specialized analytics tools show growing importance\n\n### Skill Requirements by Role\n\nOur analysis identified distinct skill patterns across job categories:\n\n- **Data Analysts**: Strong emphasis on SQL (7.6%), Excel, and visualization tools\n- **Business Intelligence Analysts**: Focus on dashboard creation and BI platforms\n- **Enterprise Architects**: Concentrated need for SAP (10.5%) and enterprise system knowledge\n- **Data Scientists**: Requirements span machine learning, Python, and statistical analysis\n\n### Skill Gap Findings\n\nComparing team member skills against market requirements revealed:\n\n- **Critical Gaps**: Enterprise software (SAP, Oracle), advanced cloud platforms\n- **Strengths to Leverage**: SQL, Python, data analysis fundamentals\n- **Development Priorities**: Cloud certifications, BI tool proficiency, specialized domain knowledge\n\n## Recommendations for Job Seekers\n\nBased on our analysis, we recommend the following strategies:\n\n### Immediate Actions\n\n1. **Build Enterprise Software Skills**: Pursue SAP or Oracle Cloud certifications to access high-demand roles\n2. **Master Core Tools**: Ensure proficiency in SQL, Excel, and at least one BI platform (Power BI or Tableau)\n3. **Develop Cloud Competency**: Gain hands-on experience with AWS or Azure\n4. **Strengthen Communication Skills**: The appearance of \"Communication\" in top skills emphasizes soft skill importance\n\n### Medium-Term Development\n\n1. **Specialize Strategically**: Choose a specialization aligned with career goals (UX Design, Cloud Architecture, ML Engineering)\n2. **Build Portfolio Projects**: Demonstrate skills through practical projects using enterprise-relevant tools\n3. **Pursue Relevant Certifications**: Industry certifications significantly boost marketability\n4. **Network in Target Industries**: Connect with professionals in roles requiring your target skill set\n\n### Career Positioning\n\n1. **Tailor Applications**: Customize resumes to highlight skills matching specific job requirements\n2. **Emphasize Skill Combinations**: Jobs often require complementary skill pairs (SQL + Cloud, Data Analysis + BI Tools)\n3. **Target Growth Areas**: Focus on roles in industries showing strong hiring patterns\n4. **Consider Geographic Factors**: Location significantly impacts both opportunities and salary expectations\n\n## Limitations and Future Work\n\n### Study Limitations\n\n1. **Data Currency**: Analysis based on 2024 snapshot; market evolves rapidly\n2. **Geographic Scope**: Dataset may not represent all regional markets equally\n3. **Skill Extraction**: NLP methods capture explicit skill mentions but may miss implicit requirements\n4. **Temporal Factors**: Seasonal hiring patterns not fully captured\n\n### Future Research Directions\n\n1. **Longitudinal Analysis**: Track skill demand trends over multiple years\n2. **Salary Prediction Enhancement**: Incorporate additional features (company size, benefits, remote status)\n3. **Real-Time Monitoring**: Develop dashboard for continuous market tracking\n4. **Industry-Specific Analysis**: Deep dive into particular sectors (FinTech, Healthcare, etc.)\n5. **Network Analysis**: Explore skill co-occurrence patterns and career pathway modeling\n\n## Final Thoughts\n\nThe 2024 job market for data analytics professionals presents significant opportunities for those who strategically develop their skill sets. While traditional foundations (SQL, Excel, data analysis) remain essential, the market increasingly rewards specialization in enterprise systems, cloud platforms, and advanced analytics tools.\n\nSuccess in this market requires a balanced approach: maintaining strong fundamentals while developing expertise in high-demand specialized areas. The skill gap analysis methodology presented in this report provides a replicable framework for continuous career development assessment.\n\nBy leveraging these insights and recommendations, job seekers can position themselves competitively in a dynamic and evolving market landscape.\n\n# References\n\n::: {#refs}\n:::\n\n## Data Sources\n\n- Lightcast Job Postings Dataset (2024)\n- U.S. Bureau of Labor Statistics\n- LinkedIn Skills Assessment Data\n\n## Tools and Technologies\n\n- Python 3.x (pandas, numpy, scikit-learn, seaborn, matplotlib)\n- Quarto Publishing System\n- R Programming Language\n- Jupyter Notebooks\n\n\n\n**Report Generated:** `r Sys.Date()`\n\n**Analysis Period:** January 2024 - December 2024\n\n**Total Job Postings Analyzed:** 72,498\n\n",
    "supporting": [
      "final_report_files"
    ],
    "filters": []
  }
}