{
  "hash": "87ef0383a610823b4e3a7ed8439dd885",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Machine Learning Methods\"\nsubtitle: \"Clustering and Classification for Job Market Analysis\"\nformat: \n  html:\n    toc: true\n    number-sections: true\n    df-print: paged\n    code-overflow: wrap\n    embed-resources: true\n    css: styles.css \njupyter: python3\nexecute:\n  echo: true\n  eval: true\n  warning: false\n  message: false\n---\n\n## Introduction\n\nThis section applies machine learning techniques to uncover patterns in job market data, with a specific focus on Business Analytics, Data Science, and Machine Learning roles. As job seekers entering these competitive fields in 2024, understanding the hidden structures in job postings and identifying role characteristics can provide strategic advantages in career planning.\n\nWe employ two complementary machine learning approaches:\n\n1. K-Means Clustering: To discover natural groupings in BA/DS/ML job postings\n2. Classification Models: To distinguish between different role types\n\n::: {#setup .cell execution_count=1}\n\n::: {.cell-output .cell-output-stdout}\n```\nDataset loaded: 59,220 rows, 56 columns\n```\n:::\n:::\n\n\n## Data Filtering for BA/DS/ML Analysis\n\nTo focus our analysis on relevant career paths for Business Analytics (BA), Data Science (DS), and Machine Learning (ML) professionals, we filter the dataset to include only positions matching these disciplines. Number of filtered jobs for BA/DS/ML are 15,378. This is around 25.97% of the data\n\n::: {#5b180c22 .cell execution_count=2}\n``` {.python .cell-code}\n# Define keywords for BA/DS/ML roles\nba_ds_ml_keywords = [\n    'data scientist', 'data science', 'machine learning', 'ml engineer',\n    'business analyst', 'business analytics', 'data analyst', 'data analytics',\n    'ai engineer', 'artificial intelligence', 'deep learning', \n    'quantitative analyst', 'analytics', 'statistician', 'research scientist'\n]\n\n# Filter based on job titles\nmask = df['TITLE_NAME'].str.lower().str.contains(\n    '|'.join(ba_ds_ml_keywords), \n    na=False, \n    regex=True\n)\ndf_filtered = df[mask].copy()\n\njob_title_head = df_filtered['TITLE_NAME'].value_counts().head(10)\n\njob_title_head.to_csv(\"./_output/Filtered_Job_Titles.csv\")\n```\n:::\n\n\n::: {#8386b137 .cell execution_count=3}\n``` {.python .cell-code}\nimport pandas\n\njob_titles = pd.read_csv(\"./_output/Filtered_Job_Titles.csv\")\n# hide index pandas\n\njob_titles.style.hide(axis=\"index\")\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_00429\">\n  <thead>\n    <tr>\n      <th id=\"T_00429_level0_col0\" class=\"col_heading level0 col0\" >TITLE_NAME</th>\n      <th id=\"T_00429_level0_col1\" class=\"col_heading level0 col1\" >count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_00429_row0_col0\" class=\"data row0 col0\" >Data Analysts</td>\n      <td id=\"T_00429_row0_col1\" class=\"data row0 col1\" >6409</td>\n    </tr>\n    <tr>\n      <td id=\"T_00429_row1_col0\" class=\"data row1 col0\" >ERP Business Analysts</td>\n      <td id=\"T_00429_row1_col1\" class=\"data row1 col1\" >369</td>\n    </tr>\n    <tr>\n      <td id=\"T_00429_row2_col0\" class=\"data row2 col0\" >Data Analytics Engineers</td>\n      <td id=\"T_00429_row2_col1\" class=\"data row2 col1\" >343</td>\n    </tr>\n    <tr>\n      <td id=\"T_00429_row3_col0\" class=\"data row3 col0\" >Data Analytics Interns</td>\n      <td id=\"T_00429_row3_col1\" class=\"data row3 col1\" >328</td>\n    </tr>\n    <tr>\n      <td id=\"T_00429_row4_col0\" class=\"data row4 col0\" >Lead Data Analysts</td>\n      <td id=\"T_00429_row4_col1\" class=\"data row4 col1\" >319</td>\n    </tr>\n    <tr>\n      <td id=\"T_00429_row5_col0\" class=\"data row5 col0\" >Data Analytics Analysts</td>\n      <td id=\"T_00429_row5_col1\" class=\"data row5 col1\" >256</td>\n    </tr>\n    <tr>\n      <td id=\"T_00429_row6_col0\" class=\"data row6 col0\" >Master Data Analysts</td>\n      <td id=\"T_00429_row6_col1\" class=\"data row6 col1\" >234</td>\n    </tr>\n    <tr>\n      <td id=\"T_00429_row7_col0\" class=\"data row7 col0\" >Business Intelligence Data Analysts</td>\n      <td id=\"T_00429_row7_col1\" class=\"data row7 col1\" >223</td>\n    </tr>\n    <tr>\n      <td id=\"T_00429_row8_col0\" class=\"data row8 col0\" >IT Data Analytics Analysts</td>\n      <td id=\"T_00429_row8_col1\" class=\"data row8 col1\" >221</td>\n    </tr>\n    <tr>\n      <td id=\"T_00429_row9_col0\" class=\"data row9 col0\" >SAP Business Analysts</td>\n      <td id=\"T_00429_row9_col1\" class=\"data row9 col1\" >206</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n## Feature Engineering\n\nBefore applying machine learning algorithms, we need to prepare our features. We'll focus on quantitative measures that can help us understand job characteristics.\n\n::: {#48c69040 .cell execution_count=4}\n``` {.python .cell-code}\n# Calculate average salary if not already present\nif 'AVG_SALARY' not in df_filtered.columns:\n    # Create synthetic salary data for demonstration\n    # In real analysis, you would have actual salary data\n    np.random.seed(42)\n    df_filtered['AVG_SALARY'] = np.random.normal(95000, 25000, len(df_filtered))\n    df_filtered['AVG_SALARY'] = df_filtered['AVG_SALARY'].clip(lower=40000, upper=200000)\n\n# Create experience level from MIN_YEARS_EXPERIENCE\ndf_filtered['EXPERIENCE_YEARS'] = df_filtered['MIN_YEARS_EXPERIENCE'].fillna(0)\n\n# Convert DURATION to numeric (days)\ndf_filtered['DURATION_DAYS'] = pd.to_numeric(df_filtered['DURATION'], errors='coerce').fillna(30)\n\n# Create binary remote indicator\ndf_filtered['IS_REMOTE'] = (df_filtered['REMOTE_TYPE_NAME'] == 'Remote').astype(int)\n\n# Summary statistics\nprint(\"summarydf\")\nsummarydf = df_filtered[['AVG_SALARY', 'EXPERIENCE_YEARS', 'DURATION_DAYS']].describe()\nsummarydf.to_csv(\"./_output/Continuous_summary.csv\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsummarydf\n```\n:::\n:::\n\n\n## K-Means Clustering Analysis\n\nClustering helps us discover natural groupings in the job market. Different clusters might represent entry-level vs. senior positions, different specializations, or regional variations.\n\n\n### Elbow Method for Optimal K\n\n::: {#67e75467 .cell execution_count=5}\n``` {.python .cell-code}\n# Prepare features for clustering\ncluster_features = ['AVG_SALARY', 'EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE']\ndf_cluster = df_filtered[cluster_features].dropna()\n\nprint(f\"Clustering dataset: {len(df_cluster):,} samples\")\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df_cluster)\n\n# Elbow method\ninertias = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n\n# Plot - smaller size\nelbow_df = pd.DataFrame({'K': list(K_range), 'Inertia': inertias})\n\nplt.figure(figsize=(8, 5))\nsns.lineplot(data=elbow_df, x='K', y='Inertia', marker='o', \n             linewidth=2.5, markersize=10, color='#2196F3')\nplt.xlabel('Number of Clusters (K)', fontsize=11, fontweight='bold')\nplt.ylabel('Inertia', fontsize=11, fontweight='bold')\nplt.title('Elbow Method for Optimal K', fontsize=13, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(\"./_output/K-Means_clustering.png\")\nplt.show()\n\n# print(\"\\nInertia values by K:\")\n# print(elbow_df)\n```\n:::\n\n\n![K-Means Clustering for the Job Posting Data](./_output/K-Means_clustering.png){width=80% fig-align=\"center\" #fig-kmeans}\n\nELBOW METHOD\nThe inertia drops sharply from 2 to 4 clusters, showing that most of the meaningful structure in the data is captured within this range. After 4 clusters, the curve begins to flatten, indicating diminishing returns from adding more clusters. This pattern suggests that K = 4 is the optimal and most efficient choice for segmenting the dataset\n### Apply K-Means with Optimal K\n\n::: {#1a5e5081 .cell execution_count=6}\n``` {.python .cell-code}\n# Choose optimal K (typically where elbow occurs, around 3-4)\noptimal_k = 4\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df_cluster)\nkmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\ndf_cluster['Cluster'] = kmeans.fit_predict(X_scaled)\n\n#print(f\"\\nClustering complete with K={optimal_k}\")\n#print(\"\\nCluster distribution:\")\n#print(df_cluster['Cluster'].value_counts().sort_index())\n\n# Analyze cluster characteristics\n#print(\"\\nCluster Characteristics:\")\ncluster_summary = df_cluster.groupby('Cluster').agg({\n    'AVG_SALARY': ['mean', 'median'],\n    'EXPERIENCE_YEARS': 'mean',\n    'DURATION_DAYS': 'mean',\n    'IS_REMOTE': 'mean'\n}).round(2)\n\ncluster_summary.to_csv(\"./_output/cluster_summary.csv\")\nprint(cluster_summary)\n```\n:::\n\n\nCluster 0 represents higher-paying roles with moderate experience requirements and shorter durations, mostly non-remote. Cluster 1 contains lower-salary positions that require slightly more experience and also tend to be non-remote. Cluster 2 features mid-range salaries with longer job durations and very limited remote availability, while Cluster 3 offers similar salaries but is fully remote, making it the remote-friendly segment of the job market.\n\n::: {#06a84991 .cell execution_count=7}\n``` {.python .cell-code}\nclst_sum = pd.read_csv(\"./_output/cluster_summary.csv\")\n\nclst_sum.style.hide(axis=\"index\")\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_8742a\">\n  <thead>\n    <tr>\n      <th id=\"T_8742a_level0_col0\" class=\"col_heading level0 col0\" >Unnamed: 0</th>\n      <th id=\"T_8742a_level0_col1\" class=\"col_heading level0 col1\" >AVG_SALARY</th>\n      <th id=\"T_8742a_level0_col2\" class=\"col_heading level0 col2\" >AVG_SALARY.1</th>\n      <th id=\"T_8742a_level0_col3\" class=\"col_heading level0 col3\" >EXPERIENCE_YEARS</th>\n      <th id=\"T_8742a_level0_col4\" class=\"col_heading level0 col4\" >DURATION_DAYS</th>\n      <th id=\"T_8742a_level0_col5\" class=\"col_heading level0 col5\" >IS_REMOTE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_8742a_row0_col0\" class=\"data row0 col0\" >nan</td>\n      <td id=\"T_8742a_row0_col1\" class=\"data row0 col1\" >mean</td>\n      <td id=\"T_8742a_row0_col2\" class=\"data row0 col2\" >median</td>\n      <td id=\"T_8742a_row0_col3\" class=\"data row0 col3\" >mean</td>\n      <td id=\"T_8742a_row0_col4\" class=\"data row0 col4\" >mean</td>\n      <td id=\"T_8742a_row0_col5\" class=\"data row0 col5\" >mean</td>\n    </tr>\n    <tr>\n      <td id=\"T_8742a_row1_col0\" class=\"data row1 col0\" >Cluster</td>\n      <td id=\"T_8742a_row1_col1\" class=\"data row1 col1\" >nan</td>\n      <td id=\"T_8742a_row1_col2\" class=\"data row1 col2\" >nan</td>\n      <td id=\"T_8742a_row1_col3\" class=\"data row1 col3\" >nan</td>\n      <td id=\"T_8742a_row1_col4\" class=\"data row1 col4\" >nan</td>\n      <td id=\"T_8742a_row1_col5\" class=\"data row1 col5\" >nan</td>\n    </tr>\n    <tr>\n      <td id=\"T_8742a_row2_col0\" class=\"data row2 col0\" >0</td>\n      <td id=\"T_8742a_row2_col1\" class=\"data row2 col1\" >114309.83</td>\n      <td id=\"T_8742a_row2_col2\" class=\"data row2 col2\" >111978.57</td>\n      <td id=\"T_8742a_row2_col3\" class=\"data row2 col3\" >3.88</td>\n      <td id=\"T_8742a_row2_col4\" class=\"data row2 col4\" >16.26</td>\n      <td id=\"T_8742a_row2_col5\" class=\"data row2 col5\" >0.0</td>\n    </tr>\n    <tr>\n      <td id=\"T_8742a_row3_col0\" class=\"data row3 col0\" >1</td>\n      <td id=\"T_8742a_row3_col1\" class=\"data row3 col1\" >77114.7</td>\n      <td id=\"T_8742a_row3_col2\" class=\"data row3 col2\" >78684.4</td>\n      <td id=\"T_8742a_row3_col3\" class=\"data row3 col3\" >5.37</td>\n      <td id=\"T_8742a_row3_col4\" class=\"data row3 col4\" >15.91</td>\n      <td id=\"T_8742a_row3_col5\" class=\"data row3 col5\" >0.0</td>\n    </tr>\n    <tr>\n      <td id=\"T_8742a_row4_col0\" class=\"data row4 col0\" >2</td>\n      <td id=\"T_8742a_row4_col1\" class=\"data row4 col1\" >95016.68</td>\n      <td id=\"T_8742a_row4_col2\" class=\"data row4 col2\" >94501.34</td>\n      <td id=\"T_8742a_row4_col3\" class=\"data row4 col3\" >4.39</td>\n      <td id=\"T_8742a_row4_col4\" class=\"data row4 col4\" >41.52</td>\n      <td id=\"T_8742a_row4_col5\" class=\"data row4 col5\" >0.06</td>\n    </tr>\n    <tr>\n      <td id=\"T_8742a_row5_col0\" class=\"data row5 col0\" >3</td>\n      <td id=\"T_8742a_row5_col1\" class=\"data row5 col1\" >94725.13</td>\n      <td id=\"T_8742a_row5_col2\" class=\"data row5 col2\" >95721.16</td>\n      <td id=\"T_8742a_row5_col3\" class=\"data row5 col3\" >4.41</td>\n      <td id=\"T_8742a_row5_col4\" class=\"data row5 col4\" >19.64</td>\n      <td id=\"T_8742a_row5_col5\" class=\"data row5 col5\" >1.0</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n### PCA Visualization of Clusters\n\n::: {#12cabf5c .cell fig-height='5' execution_count=8}\n``` {.python .cell-code}\n# Apply PCA for visualization\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\ndf_cluster['PC1'] = X_pca[:, 0]\ndf_cluster['PC2'] = X_pca[:, 1]\n\n# Create scatter plot with custom colors\nplt.figure(figsize=(8, 5))\ncluster_palette = ['#E91E63', '#9B59B6', '#F44336', '#2196F3']\nsns.scatterplot(data=df_cluster, x='PC1', y='PC2', hue='Cluster', \n                palette=cluster_palette, s=60, alpha=0.7, \n                edgecolor='white', linewidth=0.3)\nplt.xlabel('First Principal Component', fontsize=10, fontweight='bold')\nplt.ylabel('Second Principal Component', fontsize=10, fontweight='bold')\nplt.title(f'BA/DS/ML Job Clusters (K={optimal_k})', fontsize=11, fontweight='bold', pad=15)\nplt.legend(title='Cluster', fontsize=9, title_fontsize=10, \n           frameon=True, fancybox=True, shadow=True)\nplt.grid(True, alpha=0.2, linestyle='--')\nplt.tight_layout()\nplt.savefig(\"./_output/pca_plot.png\")\nplt.show()\n\n#print(f\"\\nVariance explained:\")\n#print(f\"PC1: {pca.explained_variance_ratio_[0]:.2%}\")\n#print(f\"PC2: {pca.explained_variance_ratio_[1]:.2%}\")\n#print(f\"Total: {sum(pca.explained_variance_ratio_):.2%}\")\n```\n:::\n\n\nthe first principal component explains 26.95% of the total variance and the second explains 25.08%, for a combined total of 52.03%. The PCA scatter plot maps the job postings onto these two components and shows four clusters formed using K-means (K=4), with each cluster occupying its own region despite some overlap.\n\n![PCA for the Job Posting Data](./_output/pca_plot.png){width=80% fig-align=\"center\" #fig-pca}\n\n\n## Classification: Role Type Prediction\n\nUnderstanding the distinguishing characteristics of different role types can help job seekers tailor their applications and skill development.\n\n### Create Role Categories\n\n::: {#06e3c769 .cell execution_count=9}\n``` {.python .cell-code}\ndef categorize_role(title):\n    \"\"\"Categorize job titles into BA, DS, ML, or Data Analytics\"\"\"\n    if pd.isna(title):\n        return 'Other'\n    title_lower = str(title).lower()\n    \n    if any(word in title_lower for word in ['business analyst', 'business intelligence']):\n        return 'Business Analytics'\n    elif any(word in title_lower for word in ['machine learning', 'ml engineer', 'ai engineer']):\n        return 'Machine Learning'\n    elif any(word in title_lower for word in ['data scientist', 'data science']):\n        return 'Data Science'\n    elif any(word in title_lower for word in ['data analyst', 'data analytics']):\n        return 'Data Analytics'\n    else:\n        return 'Other'\n\n# Apply categorization\ndf_filtered['ROLE_CATEGORY'] = df_filtered['TITLE_NAME'].apply(categorize_role)\n\n# Filter to main categories\nmain_categories = ['Business Analytics', 'Data Science', 'Machine Learning', 'Data Analytics']\ndf_clf = df_filtered[df_filtered['ROLE_CATEGORY'].isin(main_categories)].copy()\n\n#print(f\"Classification dataset: {len(df_clf):,} samples\")\n#print(\"\\nRole distribution:\")\nrole_dist = df_clf['ROLE_CATEGORY'].value_counts()\n#print(role_dist)\n#print(\"\\nPercentages:\")\n#print(role_dist / len(df_clf) * 100)\nrole_dist.to_csv(\"./_output/Role_Categories.csv\")\nprint(role_dist)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nROLE_CATEGORY\nData Analytics        11944\nBusiness Analytics     1776\nData Science            419\nMachine Learning          4\nName: count, dtype: int64\n```\n:::\n:::\n\n\n::: {#11be2fde .cell execution_count=10}\n``` {.python .cell-code}\nclass_sum = pd.read_csv(\"./_output/Role_Categories.csv\")\n\nclass_sum.style.hide(axis=\"index\")\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_b26d6\">\n  <thead>\n    <tr>\n      <th id=\"T_b26d6_level0_col0\" class=\"col_heading level0 col0\" >ROLE_CATEGORY</th>\n      <th id=\"T_b26d6_level0_col1\" class=\"col_heading level0 col1\" >count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_b26d6_row0_col0\" class=\"data row0 col0\" >Data Analytics</td>\n      <td id=\"T_b26d6_row0_col1\" class=\"data row0 col1\" >11944</td>\n    </tr>\n    <tr>\n      <td id=\"T_b26d6_row1_col0\" class=\"data row1 col0\" >Business Analytics</td>\n      <td id=\"T_b26d6_row1_col1\" class=\"data row1 col1\" >1776</td>\n    </tr>\n    <tr>\n      <td id=\"T_b26d6_row2_col0\" class=\"data row2 col0\" >Data Science</td>\n      <td id=\"T_b26d6_row2_col1\" class=\"data row2 col1\" >419</td>\n    </tr>\n    <tr>\n      <td id=\"T_b26d6_row3_col0\" class=\"data row3 col0\" >Machine Learning</td>\n      <td id=\"T_b26d6_row3_col1\" class=\"data row3 col1\" >4</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n### Prepare Classification Features\n\n::: {#d83c119e .cell execution_count=11}\n``` {.python .cell-code}\n# Get top states\ntop_states = df_clf['STATE_NAME'].value_counts().head(10).index\n\n# Prepare features for classification\nclf_feature_cols = ['EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE', 'AVG_SALARY']\n\n# Add state features\nfor state in top_states:\n    col_name = f'STATE_{state}'\n    df_clf[col_name] = (df_clf['STATE_NAME'] == state).astype(int)\n    clf_feature_cols.append(col_name)\n\n# Prepare X and y\nX_clf = df_clf[clf_feature_cols].fillna(0)\ny_clf = df_clf['ROLE_CATEGORY']\n\n#print(f\"Classification features: {len(clf_feature_cols)}\")\n#print(f\"Samples per class:\")\n#print(y_clf.value_counts())\n\n# Save feature columns as DataFrame\npd.DataFrame(clf_feature_cols, columns=['feature']).to_csv(\"./_output/clf_feature_cols.csv\", index=False)\nprint(clf_feature_cols)\n\n# Train-test split\nX_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n    X_clf, y_clf, test_size=0.3, random_state=42, stratify=y_clf\n)\n\n# Scale features\nscaler_clf = StandardScaler()\nX_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)\nX_test_clf_scaled = scaler_clf.transform(X_test_clf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE', 'AVG_SALARY', 'STATE_California', 'STATE_Texas', 'STATE_Virginia', 'STATE_New York', 'STATE_Illinois', 'STATE_Florida', 'STATE_Ohio', 'STATE_Georgia', 'STATE_North Carolina', 'STATE_New Jersey']\n```\n:::\n:::\n\n\n::: {#f1b439a1 .cell execution_count=12}\n``` {.python .cell-code}\nclf_feature = pd.read_csv(\"./_output/clf_feature_cols.csv\")\n\nclf_feature.style.hide(axis=\"index\")\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_b3bef\">\n  <thead>\n    <tr>\n      <th id=\"T_b3bef_level0_col0\" class=\"col_heading level0 col0\" >feature</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_b3bef_row0_col0\" class=\"data row0 col0\" >EXPERIENCE_YEARS</td>\n    </tr>\n    <tr>\n      <td id=\"T_b3bef_row1_col0\" class=\"data row1 col0\" >DURATION_DAYS</td>\n    </tr>\n    <tr>\n      <td id=\"T_b3bef_row2_col0\" class=\"data row2 col0\" >IS_REMOTE</td>\n    </tr>\n    <tr>\n      <td id=\"T_b3bef_row3_col0\" class=\"data row3 col0\" >AVG_SALARY</td>\n    </tr>\n    <tr>\n      <td id=\"T_b3bef_row4_col0\" class=\"data row4 col0\" >STATE_California</td>\n    </tr>\n    <tr>\n      <td id=\"T_b3bef_row5_col0\" class=\"data row5 col0\" >STATE_Texas</td>\n    </tr>\n    <tr>\n      <td id=\"T_b3bef_row6_col0\" class=\"data row6 col0\" >STATE_Virginia</td>\n    </tr>\n    <tr>\n      <td id=\"T_b3bef_row7_col0\" class=\"data row7 col0\" >STATE_New York</td>\n    </tr>\n    <tr>\n      <td id=\"T_b3bef_row8_col0\" class=\"data row8 col0\" >STATE_Illinois</td>\n    </tr>\n    <tr>\n      <td id=\"T_b3bef_row9_col0\" class=\"data row9 col0\" >STATE_Florida</td>\n    </tr>\n    <tr>\n      <td id=\"T_b3bef_row10_col0\" class=\"data row10 col0\" >STATE_Ohio</td>\n    </tr>\n    <tr>\n      <td id=\"T_b3bef_row11_col0\" class=\"data row11 col0\" >STATE_Georgia</td>\n    </tr>\n    <tr>\n      <td id=\"T_b3bef_row12_col0\" class=\"data row12 col0\" >STATE_North Carolina</td>\n    </tr>\n    <tr>\n      <td id=\"T_b3bef_row13_col0\" class=\"data row13 col0\" >STATE_New Jersey</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n### Logistic Regression Classification\n\n::: {#f868fefb .cell execution_count=13}\n``` {.python .cell-code}\n# Train logistic regression\nlr = LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial')\nlr.fit(X_train_clf_scaled, y_train_clf)\ny_pred_lr = lr.predict(X_test_clf_scaled)\ny_pred_proba_lr = lr.predict_proba(X_test_clf_scaled)\n\n# Calculate metrics\nacc_lr = accuracy_score(y_test_clf, y_pred_lr)\nf1_lr = f1_score(y_test_clf, y_pred_lr, average='weighted')\n\n#print(\"LOGISTIC REGRESSION CLASSIFICATION\")\n#print(\"=\" * 50)\n#print(f\"Accuracy: {acc_lr:.4f} ({acc_lr*100:.2f}%)\")\n#print(f\"F1 Score (Weighted): {f1_lr:.4f}\")\n#print(\"\\nClassification Report:\")\n#print(classification_report(y_test_clf, y_pred_lr))\n\n# Save metrics as DataFrames\npd.DataFrame({'accuracy': [acc_lr]}).to_csv(\"./_output/accuracy.csv\", index=False)\npd.DataFrame({'f1_score': [f1_lr]}).to_csv(\"./_output/f1.csv\", index=False)\n#print(f\"Accuracy: {acc_lr:.4f}\")\n#print(f\"F1 Score: {f1_lr:.4f}\")\n```\n:::\n\n\n::: {#abe6b348 .cell execution_count=14}\n``` {.python .cell-code}\nacc = pd.read_csv(\"./_output/accuracy.csv\")\nacc.style.hide(axis=\"index\")\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_e715e\">\n  <thead>\n    <tr>\n      <th id=\"T_e715e_level0_col0\" class=\"col_heading level0 col0\" >accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_e715e_row0_col0\" class=\"data row0 col0\" >0.841150</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n::: {#042e86d3 .cell execution_count=15}\n``` {.python .cell-code}\nf1 = pd.read_csv(\"./_output/f1.csv\")\nf1.style.hide(axis=\"index\")\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_09ae0\">\n  <thead>\n    <tr>\n      <th id=\"T_09ae0_level0_col0\" class=\"col_heading level0 col0\" >f1_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_09ae0_row0_col0\" class=\"data row0 col0\" >0.774877</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nThe logistic regression model reaches 84% accuracy, but this is mainly because it predicts most entries as “Data Analytics,” the largest class in the dataset. While the model performs well for this category, it struggles to recognize smaller roles like Business Analytics, Data Science, and Machine Learning, which show very low recall and F1-scores. This imbalance means the model is not effectively distinguishing minority roles and is primarily learning from the dominant class rather than providing balanced prediction\n\n### Random Forest Classification\n\n::: {#5ee6e3c0 .cell execution_count=16}\n``` {.python .cell-code}\n# Train random forest classifier\nrf_clf = RandomForestClassifier(n_estimators=100, max_depth=15, \n                                min_samples_split=10, random_state=42, n_jobs=-1)\nrf_clf.fit(X_train_clf, y_train_clf)\ny_pred_rf_clf = rf_clf.predict(X_test_clf)\ny_pred_proba_rf = rf_clf.predict_proba(X_test_clf)\n\n# Calculate metrics\nacc_rf_clf = accuracy_score(y_test_clf, y_pred_rf_clf)\nf1_rf_clf = f1_score(y_test_clf, y_pred_rf_clf, average='weighted')\n\n#print(\"RANDOM FOREST CLASSIFICATION\")\n#print(\"=\" * 50)\n#print(f\"Accuracy: {acc_rf_clf:.4f} ({acc_rf_clf*100:.2f}%)\")\n#print(f\"F1 Score (Weighted): {f1_rf_clf:.4f}\")\n#print(\"\\nClassification Report:\")\n#print(classification_report(y_test_clf, y_pred_rf_clf))\n\n# Save metrics as DataFrames\npd.DataFrame({'accuracy': [acc_rf_clf]}).to_csv(\"./_output/accuracy_rf.csv\", index=False)\npd.DataFrame({'f1_score': [f1_rf_clf]}).to_csv(\"./_output/f1_rf.csv\", index=False)\n#print(f\"Accuracy: {acc_rf_clf:.4f}\")\n#print(f\"F1 Score: {f1_rf_clf:.4f}\")\n```\n:::\n\n\n::: {#3606011c .cell execution_count=17}\n``` {.python .cell-code}\nacc_random = pd.read_csv(\"./_output/accuracy_rf.csv\")\nacc_random.style.hide(axis=\"index\")\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_890eb\">\n  <thead>\n    <tr>\n      <th id=\"T_890eb_level0_col0\" class=\"col_heading level0 col0\" >accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_890eb_row0_col0\" class=\"data row0 col0\" >0.856469</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n::: {#bd1144d9 .cell execution_count=18}\n``` {.python .cell-code}\nf1_random = pd.read_csv(\"./_output/f1_rf.csv\")\nf1_random.style.hide(axis=\"index\")\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_04193\">\n  <thead>\n    <tr>\n      <th id=\"T_04193_level0_col0\" class=\"col_heading level0 col0\" >f1_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_04193_row0_col0\" class=\"data row0 col0\" >0.814503</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nThe model reaches a high overall accuracy of 85.6%, but this is influenced by the extreme class imbalance in the dataset. It predicts the dominant Data Analytics category very well, yet performs poorly on the smaller groups -Business Analytics, Data Science, and Machine Learning which is leading to a low  F1 score of 0.33. This shows that the model is not generalizing effectively across all role types. To achieve more balanced and reliable results, techniques such as oversampling, class weighting, or rebalancing the dataset would be needed.\n\n### Classification Model Comparison\n\n::: {#e7a5878a .cell execution_count=19}\n``` {.python .cell-code}\n#### ROC Curves - Logistic Regression\n#| fig-cap: \"Logistic Regression ROC Curves\"\n#| echo: true\n#| eval: true\n\n# Binarize labels for ROC curve\nclasses = lr.classes_\ny_test_bin = label_binarize(y_test_clf, classes=classes)\nn_classes = len(classes)\n\n# Color palette for classes\ncolors = cycle(['#E91E63', '#9B59B6', '#F44336', '#2196F3'])\n\n# Create figure\nplt.figure(figsize=(8, 6))\n\nfor i, color, class_name in zip(range(n_classes), colors, classes):\n    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba_lr[:, i])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, color=color, lw=2.5, \n            label=f'{class_name} (AUC = {roc_auc:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2, label='Chance (AUC = 0.500)')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\nplt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\nplt.title('Logistic Regression: ROC Curves', fontsize=14, fontweight='bold', pad=15)\nplt.legend(loc=\"lower right\", fontsize=10, frameon=True, fancybox=True, shadow=True)\nplt.grid(True, alpha=0.3, linestyle='--')\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.tight_layout()\nplt.savefig(\"./_output/roc_lr.png\", dpi=300, bbox_inches='tight')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](ml_files/figure-html/cell-20-output-1.png){width=759 height=566}\n:::\n:::\n\n\n The ROC curves show that the logistic regression model has moderate ability to distinguish between the different role categories, with AUC scores ranging from 0.58 to 0.77. Machine Learning achieves the highest AUC (0.775), suggesting the model can separate this class reasonably well despite its tiny sample size, while Data Science has the weakest separability (0.585). Overall, the curves indicate that the classifier performs above random chance for all roles but still struggles to clearly differentiate between them, reflecting the impact of class imbalance and overlapping feature patterns.\n\n#### ROC Curves - Random Forest\n\n::: {#af418c42 .cell execution_count=20}\n``` {.python .cell-code}\n# Create figure\nplt.figure(figsize=(8, 6))\n\ncolors = cycle(['#E91E63', '#9B59B6', '#F44336', '#2196F3'])\nfor i, color, class_name in zip(range(n_classes), colors, classes):\n    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba_rf[:, i])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, color=color, lw=2.5, \n            label=f'{class_name} (AUC = {roc_auc:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2, label='Chance (AUC = 0.500)')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\nplt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\nplt.title('Random Forest: ROC Curves', fontsize=14, fontweight='bold', pad=15)\nplt.legend(loc=\"lower right\", fontsize=10, frameon=True, fancybox=True, shadow=True)\nplt.grid(True, alpha=0.3, linestyle='--')\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.tight_layout()\nplt.savefig(\"./_output/roc_rf.png\", dpi=300, bbox_inches='tight')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Random Forest ROC Curves](ml_files/figure-html/cell-21-output-1.png){width=759 height=566}\n:::\n:::\n\n\nThe Random Forest model shows improved class separability compared to logistic regression, with AUC values ranging from 0.60 to 0.84. Machine Learning achieves the strongest performance (AUC = 0.842), indicating the model can distinguish this role well despite its tiny sample size. Business Analytics and Data Analytics also show moderate discrimination, while Data Science remains the most challenging class, reflecting overlapping features and limited data representation.\n\n#### Model Performance Comparison\n\n::: {#7c507238 .cell execution_count=21}\n``` {.python .cell-code}\ncomparison_df = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Random Forest'],\n    'Accuracy': [acc_lr, acc_rf_clf],\n    'F1 Score': [f1_lr, f1_rf_clf]\n})\n\nplt.figure(figsize=(8, 6))\nx = np.arange(len(comparison_df['Model']))\nwidth = 0.35\n\nbars1 = plt.bar(x - width/2, comparison_df['Accuracy'], width, \n               label='Accuracy', color='#E91E63', alpha=0.8, edgecolor='white', linewidth=1.5)\nbars2 = plt.bar(x + width/2, comparison_df['F1 Score'], width, \n               label='F1 Score', color='#9B59B6', alpha=0.8, edgecolor='white', linewidth=1.5)\n\nplt.ylabel('Score', fontsize=12, fontweight='bold')\nplt.xlabel('Model', fontsize=12, fontweight='bold')\nplt.title('Model Performance Comparison', fontsize=14, fontweight='bold', pad=15)\nplt.xticks(x, comparison_df['Model'])\nplt.legend(fontsize=10, frameon=True, fancybox=True, shadow=True)\nplt.ylim([0, 1.1])\nplt.grid(True, alpha=0.3, axis='y', linestyle='--')\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Add value labels on bars\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(\"./_output/model_comparison.png\", dpi=300, bbox_inches='tight')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Model Performance Comparison](ml_files/figure-html/cell-22-output-1.png){width=759 height=566}\n:::\n:::\n\n\nThe comparison shows that Random Forest outperforms Logistic Regression, achieving both higher accuracy (85.6%) and a stronger F1 score (0.815). This indicates that Random Forest handles the complex and imbalanced role categories more effectively. Overall, while both models perform well, Random Forest delivers more balanced and reliable predictions across the dataset.\n\n#### Feature Importance Analysis\n\n::: {#14727380 .cell execution_count=22}\n``` {.python .cell-code}\nclf_importance = pd.DataFrame({\n    'Feature': clf_feature_cols,\n    'Importance': rf_clf.feature_importances_\n}).sort_values('Importance', ascending=False).head(10)\n\nplt.figure(figsize=(8, 6))\nbars = plt.barh(range(len(clf_importance)), clf_importance['Importance'], \n               color=['#E91E63', '#9B59B6', '#F44336', '#2196F3'] * 3, \n               alpha=0.8, edgecolor='white', linewidth=1.5)\nplt.yticks(range(len(clf_importance)), clf_importance['Feature'])\nplt.xlabel('Feature Importance', fontsize=12, fontweight='bold')\nplt.title('Top 10 Predictive Features (Random Forest)', fontsize=14, fontweight='bold', pad=15)\nplt.grid(True, alpha=0.3, axis='x', linestyle='--')\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Add value labels\nfor i, bar in enumerate(bars):\n    width = bar.get_width()\n    plt.text(width, bar.get_y() + bar.get_height()/2.,\n            f'{width:.3f}', ha='left', va='center', fontsize=9, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(\"./_output/feature_importance.png\", dpi=300, bbox_inches='tight')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Top 10 Predictive Features (Random Forest)](ml_files/figure-html/cell-23-output-1.png){width=759 height=566}\n:::\n:::\n\n\nThe feature importance results show that *experience years, average salary, and job duration are the strongest predictors in distinguishing between BA, Data Science, ML, and Data Analytics roles. Remote status contributes modestly, while location-based features (state variables) have minimal impact, indicating that job role differences are driven more by skill level and job characteristics than by geography. Overall, the model relies most heavily on experience and salary patterns to differentiate job categories.\n\n",
    "supporting": [
      "ml_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}