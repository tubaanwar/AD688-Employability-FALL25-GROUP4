---
title: "Machine Learning Methods"
subtitle: "Clustering and Predictive Modeling for Job Market Analysis"
author:
  - name: Tuba Anwar
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
  - name: Kriti Singh
    affiliations:
      - ref: bu
  - name: Soham Deshkhaire
    affiliations:
      - ref: bu
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
    code-overflow: wrap
    embed-resources: true
    theme: 
      light: [cosmo, custom.scss]
    css: styles.css
jupyter: python3
execute:
  echo: false
  warning: false
  message: false
---

## Introduction

This section applies machine learning techniques to uncover patterns in job market data. We use clustering algorithms to group similar job postings and predictive models to forecast salary trends and job categories.

```{python}
import pandas as pd
import numpy as np
import hvplot.pandas
import panel as pn
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import os
import warnings
warnings.filterwarnings('ignore')

# Enable panel for rendering
pn.extension()

# Color palette
COLORS = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c']

# Load data - check which file exists
if os.path.exists('cleaned_job_posting.csv'):
    df = pd.read_csv('cleaned_job_posting.csv')
    print(f"Dataset loaded from: cleaned_job_posting.csv")
elif os.path.exists('cleaned_job_postings.csv'):
    df = pd.read_csv('cleaned_job_postings.csv')
    print(f"Dataset loaded from: cleaned_job_postings.csv")
elif os.path.exists('lightcast_job_postings.csv'):
    df = pd.read_csv('lightcast_job_postings.csv')
    print(f"Dataset loaded from: lightcast_job_postings.csv")
else:
    raise FileNotFoundError("Could not find data file")

print(f"Shape: {df.shape[0]:,} rows, {df.shape[1]} columns")
print(f"Columns available: {', '.join(df.columns[:10])}...")
```

## 1. Feature Engineering

### 1.1 Preparing Data for Machine Learning

Before applying ML algorithms, we need to prepare our features.

```{python}
# Select relevant numerical features for clustering
# TODO: Customize based on available columns in your dataset

# Example features - adjust based on your actual columns
ml_features = []

# Check which numerical features exist
potential_features = ['SALARY', 'DURATION', 'EXPERIENCE_REQUIRED']
for feature in potential_features:
    if feature in df.columns:
        ml_features.append(feature)

print(f"Features selected for ML: {ml_features}")

# Create feature dataframe
if len(ml_features) > 0:
    df_ml = df[ml_features].dropna()
    print(f"\nML dataset shape: {df_ml.shape}")
    print("\nFeature Summary Statistics:")
    print(df_ml.describe())
else:
    print("No numerical features available for ML. Please add feature engineering.")
```

## 2. K-Means Clustering

### 2.1 Determining Optimal Number of Clusters

Use the elbow method to find the optimal number of clusters.

```{python}
# Implement elbow method
import matplotlib.pyplot as plt

if len(ml_features) > 0:
    # Standardize features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df_ml)
    
    # Calculate inertia for different k values
    inertias = []
    K_range = range(2, 11)
    
    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X_scaled)
        inertias.append(kmeans.inertia_)
    
    # Create dataframe for plotting
    elbow_df = pd.DataFrame({
        'K': list(K_range),
        'Inertia': inertias
    })
    
    # Plot elbow curve with matplotlib
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.plot(elbow_df['K'], elbow_df['Inertia'], marker='o', linewidth=2, 
            markersize=8, color='#3498db', label='Inertia')
    ax.set_xlabel('Number of Clusters (K)', fontsize=12)
    ax.set_ylabel('Inertia', fontsize=12)
    ax.set_title('Elbow Method for Optimal K', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3)
    ax.legend()
    plt.tight_layout()
    plt.show()
    
    print("Review the elbow curve to determine optimal K")
```

::: {.callout-note icon=false}
## Choosing Optimal K
Look for the "elbow" point where the inertia decrease begins to slow down. This represents the optimal balance between cluster compactness and the number of clusters.
:::

### 2.2 Applying K-Means Clustering

```{python}
# Apply K-Means with optimal K

if len(ml_features) > 0:
    # Use k=4 as example - adjust based on elbow method
    optimal_k = 4
    
    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
    df_ml['Cluster'] = kmeans.fit_predict(X_scaled)
    
    print(f"Clustering complete with K={optimal_k}")
    print("\nCluster distribution:")
    print(df_ml['Cluster'].value_counts().sort_index())
    
    # Calculate cluster centers in original scale
    cluster_centers = scaler.inverse_transform(kmeans.cluster_centers_)
    centers_df = pd.DataFrame(cluster_centers, columns=ml_features)
    centers_df['Cluster'] = range(optimal_k)
    
    print("\nCluster Centers (Original Scale):")
    print(centers_df)
```

### 2.3 Visualizing Clusters

```{python}
# Visualize clusters using PCA

if len(ml_features) >= 2:
    # Apply PCA for visualization
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)
    
    df_ml['PC1'] = X_pca[:, 0]
    df_ml['PC2'] = X_pca[:, 1]
    
    # Create scatter plot with matplotlib
    fig, ax = plt.subplots(figsize=(12, 8))
    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c']
    
    for cluster in range(optimal_k):
        mask = df_ml['Cluster'] == cluster
        ax.scatter(df_ml[mask]['PC1'], df_ml[mask]['PC2'], 
                   label=f'Cluster {cluster}', s=50, alpha=0.6, 
                   color=colors[cluster % len(colors)])
    
    ax.set_xlabel('First Principal Component', fontsize=12)
    ax.set_ylabel('Second Principal Component', fontsize=12)
    ax.set_title(f'Job Clusters (K-Means, K={optimal_k})', fontsize=14, fontweight='bold')
    ax.legend(loc='best', fontsize=10)
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    # Print variance explained
    variance_explained = pca.explained_variance_ratio_
    print(f"\nVariance explained by PC1: {variance_explained[0]:.2%}")
    print(f"Variance explained by PC2: {variance_explained[1]:.2%}")
    print(f"Total variance explained: {variance_explained.sum():.2%}")
```

::: {.callout-tip icon=false}
## Understanding PCA Visualization
The scatter plot shows job postings projected onto two principal components. Points of the same color belong to the same cluster, revealing natural groupings in the data.
:::

## 3. Cluster Interpretation

### 3.1 Analyzing Cluster Characteristics

```{python}
# Analyze what each cluster represents

if len(ml_features) > 0:
    print("Cluster Characteristics:\n")
    
    for cluster_id in range(optimal_k):
        cluster_data = df_ml[df_ml['Cluster'] == cluster_id]
        print(f"{'='*60}")
        print(f"CLUSTER {cluster_id} (n={len(cluster_data):,} jobs)")
        print(f"{'='*60}")
        
        for feature in ml_features:
            mean_val = cluster_data[feature].mean()
            median_val = cluster_data[feature].median()
            print(f"  {feature}:")
            print(f"    Mean: {mean_val:.2f}")
            print(f"    Median: {median_val:.2f}")
        print()
```

### 3.2 Cluster Comparison Heatmap

```{python}
# Create heatmap comparing cluster characteristics
import matplotlib.pyplot as plt
import seaborn as sns

if len(ml_features) > 0:
    # Calculate mean values for each cluster
    cluster_summary = df_ml.groupby('Cluster')[ml_features].mean()
    
    # Normalize for better visualization
    cluster_summary_norm = (cluster_summary - cluster_summary.min()) / (cluster_summary.max() - cluster_summary.min())
    
    # Create matplotlib heatmap
    fig, ax = plt.subplots(figsize=(10, 6))
    sns.heatmap(cluster_summary_norm.T, annot=True, fmt='.2f', cmap='RdYlGn', 
                cbar_kws={'label': 'Normalized Value'}, ax=ax, linewidths=0.5)
    ax.set_title('Normalized Cluster Characteristics', fontsize=14, fontweight='bold')
    ax.set_xlabel('Cluster', fontsize=12)
    ax.set_ylabel('Feature', fontsize=12)
    plt.tight_layout()
    plt.show()
```

### 3.3 Cluster Size Distribution

```{python}
# Visualize cluster sizes

if len(ml_features) > 0:
    cluster_sizes = df_ml['Cluster'].value_counts().sort_index().reset_index()
    cluster_sizes.columns = ['Cluster', 'Count']
    
    # Create bar chart with matplotlib
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.bar(cluster_sizes['Cluster'].astype(str), cluster_sizes['Count'], 
           color='#3498db', alpha=0.7, edgecolor='black', linewidth=1.5)
    ax.set_xlabel('Cluster', fontsize=12)
    ax.set_ylabel('Number of Jobs', fontsize=12)
    ax.set_title('Job Distribution Across Clusters', fontsize=14, fontweight='bold')
    ax.grid(True, alpha=0.3, axis='y')
    
    # Add value labels on bars
    for i, v in enumerate(cluster_sizes['Count']):
        ax.text(i, v + 50, str(v), ha='center', fontweight='bold')
    
    plt.tight_layout()
    plt.show()
```

## 4. Predictive Modeling (Optional)

### 4.1 Salary Prediction Model

```{python}
# Build a regression model to predict salary
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score

# Check if salary data is available
if 'SALARY' in df.columns and len(ml_features) > 1:
    # Prepare features (excluding salary)
    feature_cols = [col for col in ml_features if col != 'SALARY']
    
    if len(feature_cols) > 0:
        # Prepare data
        df_pred = df[feature_cols + ['SALARY']].dropna()
        X = df_pred[feature_cols]
        y = df_pred['SALARY']
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # Train model
        rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
        rf_model.fit(X_train, y_train)
        
        # Predictions
        y_pred = rf_model.predict(X_test)
        
        # Evaluate
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        print(f"Salary Prediction Model Performance:")
        print(f"  Mean Absolute Error: ${mae:,.2f}")
        print(f"  R² Score: {r2:.3f}")
        
        # Feature importance
        importance_df = pd.DataFrame({
            'Feature': feature_cols,
            'Importance': rf_model.feature_importances_
        }).sort_values('Importance', ascending=False)
        
        print("\nFeature Importance:")
        print(importance_df)
        
        # Plot feature importance with matplotlib
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.barh(importance_df['Feature'], importance_df['Importance'], 
                color='#2ecc71', alpha=0.7, edgecolor='black', linewidth=1.5)
        ax.set_xlabel('Importance', fontsize=12)
        ax.set_title('Feature Importance for Salary Prediction', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3, axis='x')
        plt.tight_layout()
        plt.show()
        
        # Actual vs Predicted
        pred_df = pd.DataFrame({
            'Actual': y_test,
            'Predicted': y_pred
        })
        
        # Create scatter plot with diagonal line
        fig, ax = plt.subplots(figsize=(10, 8))
        ax.scatter(pred_df['Actual'], pred_df['Predicted'], alpha=0.5, 
                   color='#3498db', s=50, edgecolors='black', linewidth=0.5)
        
        # Add diagonal reference line
        min_val = min(pred_df['Actual'].min(), pred_df['Predicted'].min())
        max_val = max(pred_df['Actual'].max(), pred_df['Predicted'].max())
        ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
        
        ax.set_xlabel('Actual Salary', fontsize=12)
        ax.set_ylabel('Predicted Salary', fontsize=12)
        ax.set_title(f'Actual vs Predicted Salary (R²={r2:.3f})', fontsize=14, fontweight='bold')
        ax.legend(fontsize=10)
        ax.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
    else:
        print("Not enough features for salary prediction")
else:
    print("Salary data not available for predictive modeling")
```

::: {.callout-important icon=false}
## Model Interpretation
- **R² Score** closer to 1.0 indicates better predictions
- **Feature Importance** shows which factors most influence salary
- Points closer to the diagonal line represent more accurate predictions
:::

## Summary

::: {.callout-important icon=false}
## Machine Learning Insights

**Key findings from ML analysis:**
- Clustering reveals distinct job market segments based on numerical features
- K-Means algorithm successfully groups similar job postings
- PCA visualization helps understand cluster separation
- Predictive models can estimate salaries based on job characteristics

**Recommendations:**
- Target jobs in clusters with favorable characteristics
- Focus on features with high importance for salary optimization
- Use cluster insights to refine job search strategy
:::

## Next Steps

Proceed to NLP Methods to extract insights from job descriptions using text analysis techniques.

## References

- Scikit-learn documentation
- K-Means clustering best practices
- PCA for dimensionality reduction
- Random Forest for regression