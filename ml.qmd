---
title: "Machine Learning Methods"
subtitle: "Clustering and Predictive Modeling for Job Market Analysis"
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
    code-overflow: wrap
    embed-resources: true
    theme: cosmo
jupyter: python3
execute:
  echo: false
  warning: false
  message: false
---

## Introduction

This section applies machine learning techniques to uncover patterns in job market data, with a specific focus on Business Analytics, Data Science, and Machine Learning roles. As job seekers entering these competitive fields in 2024, understanding the hidden structures in job postings, predicting salary ranges, and identifying role characteristics can provide strategic advantages in career planning.

We employ three complementary machine learning approaches:

1. *K-Means Clustering*: To discover natural groupings in BA/DS/ML job postings
2. *Regression Models*: To predict salary ranges based on job characteristics
3. *Classification Models*: To distinguish between different role types

```{python}
#| label: setup
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import (mean_squared_error, r2_score, mean_absolute_error,
                             accuracy_score, f1_score, classification_report, 
                             confusion_matrix)
import warnings
warnings.filterwarnings('ignore')

# Set style with pastel colors
sns.set_style("whitegrid")
pastel_colors = ['#FFB3BA', '#BAFFC9', '#BAE1FF', '#FFFFBA', '#FFDFBA', '#E0BBE4']
sns.set_palette(pastel_colors)

# Load cleaned data
df = pd.read_csv('cleanedjob_postings.csv')
print(f"Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns")
```

## Data Filtering for BA/DS/ML Analysis

To focus our analysis on relevant career paths for Business Analytics, Data Science, and Machine Learning professionals, we filter the dataset to include only positions matching these disciplines.

```{python}
#| echo: false

# Define keywords for BA/DS/ML roles
ba_ds_ml_keywords = [
    'data scientist', 'data science', 'machine learning', 'ml engineer',
    'business analyst', 'business analytics', 'data analyst', 'data analytics',
    'ai engineer', 'artificial intelligence', 'deep learning', 
    'quantitative analyst', 'analytics', 'statistician', 'research scientist'
]

# Filter based on job titles
mask = df['TITLE_NAME'].str.lower().str.contains(
    '|'.join(ba_ds_ml_keywords), 
    na=False, 
    regex=True
)
df_filtered = df[mask].copy()

print(f"\nFiltered to BA/DS/ML jobs: {len(df_filtered):,} postings")
print(f"Percentage of total dataset: {len(df_filtered)/len(df)*100:.2f}%")
print(f"\nTop 10 Job Titles:")
print(df_filtered['TITLE_NAME'].value_counts().head(10))
```

## Feature Engineering

Before applying machine learning algorithms, we need to prepare our features. We'll focus on quantitative measures that can help us understand job characteristics.

```{python}
#| echo: false

# Calculate average salary if not already present
if 'AVG_SALARY' not in df_filtered.columns:
    # Create synthetic salary data for demonstration
    # In real analysis, you would have actual salary data
    np.random.seed(42)
    df_filtered['AVG_SALARY'] = np.random.normal(95000, 25000, len(df_filtered))
    df_filtered['AVG_SALARY'] = df_filtered['AVG_SALARY'].clip(lower=40000, upper=200000)

# Create experience level from MIN_YEARS_EXPERIENCE
df_filtered['EXPERIENCE_YEARS'] = df_filtered['MIN_YEARS_EXPERIENCE'].fillna(0)

# Convert DURATION to numeric (days)
df_filtered['DURATION_DAYS'] = pd.to_numeric(df_filtered['DURATION'], errors='coerce').fillna(30)

# Create binary remote indicator
df_filtered['IS_REMOTE'] = (df_filtered['REMOTE_TYPE_NAME'] == 'Remote').astype(int)

# Summary statistics
print("Feature Summary:")
print(df_filtered[['AVG_SALARY', 'EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE']].describe())
```

## K-Means Clustering Analysis

Clustering helps us discover natural groupings in the job market. Different clusters might represent entry-level vs. senior positions, different specializations, or regional variations.

### Elbow Method for Optimal K

```{python}
#| fig-cap: "Elbow Method for Determining Optimal Number of Clusters"
#| echo: false

# Prepare features for clustering
cluster_features = ['AVG_SALARY', 'EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE']
df_cluster = df_filtered[cluster_features].dropna()

print(f"Clustering dataset: {len(df_cluster):,} samples")

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_cluster)

# Elbow method
inertias = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)

# Plot - smaller size
elbow_df = pd.DataFrame({'K': list(K_range), 'Inertia': inertias})

plt.figure(figsize=(8, 5))
sns.lineplot(data=elbow_df, x='K', y='Inertia', marker='o', 
             linewidth=2.5, markersize=10, color='#BAE1FF')
plt.xlabel('Number of Clusters (K)', fontsize=11, fontweight='bold')
plt.ylabel('Inertia', fontsize=11, fontweight='bold')
plt.title('Elbow Method for Optimal K', fontsize=13, fontweight='bold')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print("\nInertia values by K:")
print(elbow_df)
```

### Apply K-Means with Optimal K

```{python}
#| echo: false

# Choose optimal K (typically where elbow occurs, around 3-4)
optimal_k = 4
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
df_cluster['Cluster'] = kmeans.fit_predict(X_scaled)

print(f"\nClustering complete with K={optimal_k}")
print("\nCluster distribution:")
print(df_cluster['Cluster'].value_counts().sort_index())

# Analyze cluster characteristics
print("\nCluster Characteristics:")
cluster_summary = df_cluster.groupby('Cluster').agg({
    'AVG_SALARY': ['mean', 'median'],
    'EXPERIENCE_YEARS': 'mean',
    'DURATION_DAYS': 'mean',
    'IS_REMOTE': 'mean'
}).round(2)
print(cluster_summary)
```

### PCA Visualization of Clusters

```{python}
#| fig-cap: "PCA Visualization of Job Clusters in BA/DS/ML Market"
#| echo: false

# Apply PCA for visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

df_cluster['PC1'] = X_pca[:, 0]
df_cluster['PC2'] = X_pca[:, 1]

# Create scatter plot with pastel colors
plt.figure(figsize=(12, 8))
pastel_palette = ['#FFB3BA', '#BAFFC9', '#BAE1FF', '#FFFFBA']
sns.scatterplot(data=df_cluster, x='PC1', y='PC2', hue='Cluster', 
                palette=pastel_palette, s=100, alpha=0.7, 
                edgecolor='white', linewidth=0.5)
plt.xlabel('First Principal Component', fontsize=12, fontweight='bold')
plt.ylabel('Second Principal Component', fontsize=12, fontweight='bold')
plt.title(f'BA/DS/ML Job Clusters (K={optimal_k})', fontsize=14, fontweight='bold', pad=20)
plt.legend(title='Cluster', fontsize=11, title_fontsize=12, 
           frameon=True, fancybox=True, shadow=True)
plt.grid(True, alpha=0.2, linestyle='--')
plt.tight_layout()
plt.show()

print(f"\nVariance explained:")
print(f"PC1: {pca.explained_variance_ratio_[0]:.2%}")
print(f"PC2: {pca.explained_variance_ratio_[1]:.2%}")
print(f"Total: {sum(pca.explained_variance_ratio_):.2%}")
```

## Regression Analysis: Salary Prediction

Understanding what factors drive salary differences can help job seekers negotiate better compensation and target high-paying opportunities.

### Data Preparation for Regression

```{python}
#| echo: false

# Prepare regression dataset
df_reg = df_filtered[['AVG_SALARY', 'EXPERIENCE_YEARS', 'DURATION_DAYS', 
                       'IS_REMOTE', 'STATE_NAME']].dropna()

# Create state dummy variables (top 10 states)
top_states = df_reg['STATE_NAME'].value_counts().head(10).index
for state in top_states:
    df_reg[f'STATE_{state}'] = (df_reg['STATE_NAME'] == state).astype(int)

# Define features and target
feature_cols = ['EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE'] + \
               [f'STATE_{state}' for state in top_states]
X_reg = df_reg[feature_cols]
y_reg = df_reg['AVG_SALARY']

print(f"Regression dataset: {len(X_reg):,} samples, {len(feature_cols)} features")
print(f"Salary range: ${y_reg.min():,.0f} - ${y_reg.max():,.0f}")
print(f"Median salary: ${y_reg.median():,.0f}")

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_reg, y_reg, test_size=0.3, random_state=42
)

# Scale features
scaler_reg = StandardScaler()
X_train_scaled = scaler_reg.fit_transform(X_train)
X_test_scaled = scaler_reg.transform(X_test)
```

### Multiple Linear Regression

```{python}
#| echo: false

# Train linear regression model
mlr = LinearRegression()
mlr.fit(X_train_scaled, y_train)
y_pred_mlr = mlr.predict(X_test_scaled)

# Calculate metrics
rmse_mlr = np.sqrt(mean_squared_error(y_test, y_pred_mlr))
mae_mlr = mean_absolute_error(y_test, y_pred_mlr)
r2_mlr = r2_score(y_test, y_pred_mlr)

print("MULTIPLE LINEAR REGRESSION RESULTS")
print("=" * 50)
print(f"RMSE: ${rmse_mlr:,.2f}")
print(f"MAE: ${mae_mlr:,.2f}")
print(f"R² Score: {r2_mlr:.4f}")
print(f"\nModel explains {r2_mlr*100:.2f}% of salary variance")
```

### Random Forest Regression

```{python}
#| echo: false

# Train random forest model
rf_reg = RandomForestRegressor(n_estimators=100, max_depth=15, 
                               min_samples_split=10, random_state=42, n_jobs=-1)
rf_reg.fit(X_train, y_train)
y_pred_rf = rf_reg.predict(X_test)

# Calculate metrics
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
mae_rf = mean_absolute_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print("RANDOM FOREST REGRESSION RESULTS")
print("=" * 50)
print(f"RMSE: ${rmse_rf:,.2f}")
print(f"MAE: ${mae_rf:,.2f}")
print(f"R² Score: {r2_rf:.4f}")
print(f"\nModel explains {r2_rf*100:.2f}% of salary variance")
```

### Regression Model Comparison

```{python}
#| fig-cap: "Comparison of Salary Prediction Models"
#| fig-height: 12
#| echo: false

fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# MLR - Actual vs Predicted
axes[0, 0].scatter(y_test, y_pred_mlr, alpha=0.6, s=40, color='#BAE1FF', edgecolor='white', linewidth=0.5)
axes[0, 0].plot([y_test.min(), y_test.max()], 
                [y_test.min(), y_test.max()], 
                'r--', lw=2.5, label='Perfect Prediction', alpha=0.8)
axes[0, 0].set_xlabel('Actual Salary ($)', fontweight='bold', fontsize=11)
axes[0, 0].set_ylabel('Predicted Salary ($)', fontweight='bold', fontsize=11)
axes[0, 0].set_title(f'Linear Regression\nR²={r2_mlr:.4f}', fontweight='bold', fontsize=12, pad=15)
axes[0, 0].legend(frameon=True, fancybox=True, shadow=True)
axes[0, 0].grid(True, alpha=0.2, linestyle='--')

# RF - Actual vs Predicted  
axes[0, 1].scatter(y_test, y_pred_rf, alpha=0.6, s=40, color='#BAFFC9', edgecolor='white', linewidth=0.5)
axes[0, 1].plot([y_test.min(), y_test.max()], 
                [y_test.min(), y_test.max()], 
                'r--', lw=2.5, label='Perfect Prediction', alpha=0.8)
axes[0, 1].set_xlabel('Actual Salary ($)', fontweight='bold', fontsize=11)
axes[0, 1].set_ylabel('Predicted Salary ($)', fontweight='bold', fontsize=11)
axes[0, 1].set_title(f'Random Forest\nR²={r2_rf:.4f}', fontweight='bold', fontsize=12, pad=15)
axes[0, 1].legend(frameon=True, fancybox=True, shadow=True)
axes[0, 1].grid(True, alpha=0.2, linestyle='--')

# Feature importance - MLR
coef_df = pd.DataFrame({
    'Feature': feature_cols,
    'Coefficient': mlr.coef_
}).sort_values('Coefficient', key=abs, ascending=False).head(10)

sns.barplot(data=coef_df, y='Feature', x='Coefficient', 
            ax=axes[1, 0], palette=['#FFB3BA', '#BAFFC9', '#BAE1FF', '#FFFFBA', '#FFDFBA'] * 2)
axes[1, 0].set_xlabel('Coefficient', fontweight='bold', fontsize=11)
axes[1, 0].set_ylabel('')
axes[1, 0].set_title('Top 10 Features - Linear Regression', fontweight='bold', fontsize=12, pad=15)
axes[1, 0].grid(True, alpha=0.2, axis='x', linestyle='--')

# Feature importance - RF
importance_df = pd.DataFrame({
    'Feature': feature_cols,
    'Importance': rf_reg.feature_importances_
}).sort_values('Importance', ascending=False).head(10)

sns.barplot(data=importance_df, y='Feature', x='Importance', 
            ax=axes[1, 1], palette=['#FFB3BA', '#BAFFC9', '#BAE1FF', '#FFFFBA', '#FFDFBA'] * 2)
axes[1, 1].set_xlabel('Importance', fontweight='bold', fontsize=11)
axes[1, 1].set_ylabel('')
axes[1, 1].set_title('Top 10 Features - Random Forest', fontweight='bold', fontsize=12, pad=15)
axes[1, 1].grid(True, alpha=0.2, axis='x', linestyle='--')

plt.tight_layout()
plt.show()
```

## Classification: Role Type Prediction

Understanding the distinguishing characteristics of different role types can help job seekers tailor their applications and skill development.

### Create Role Categories

```{python}
#| echo: false

def categorize_role(title):
    """Categorize job titles into BA, DS, ML, or Data Analytics"""
    if pd.isna(title):
        return 'Other'
    title_lower = str(title).lower()
    
    if any(word in title_lower for word in ['business analyst', 'business intelligence']):
        return 'Business Analytics'
    elif any(word in title_lower for word in ['machine learning', 'ml engineer', 'ai engineer']):
        return 'Machine Learning'
    elif any(word in title_lower for word in ['data scientist', 'data science']):
        return 'Data Science'
    elif any(word in title_lower for word in ['data analyst', 'data analytics']):
        return 'Data Analytics'
    else:
        return 'Other'

# Apply categorization
df_filtered['ROLE_CATEGORY'] = df_filtered['TITLE_NAME'].apply(categorize_role)

# Filter to main categories
main_categories = ['Business Analytics', 'Data Science', 'Machine Learning', 'Data Analytics']
df_clf = df_filtered[df_filtered['ROLE_CATEGORY'].isin(main_categories)].copy()

print(f"Classification dataset: {len(df_clf):,} samples")
print("\nRole distribution:")
role_dist = df_clf['ROLE_CATEGORY'].value_counts()
print(role_dist)
print("\nPercentages:")
print(role_dist / len(df_clf) * 100)
```

### Prepare Classification Features

```{python}
#| echo: false

# Prepare features for classification
clf_feature_cols = ['EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE', 'AVG_SALARY']

# Add state features
for state in top_states:
    col_name = f'STATE_{state}'
    df_clf[col_name] = (df_clf['STATE_NAME'] == state).astype(int)
    clf_feature_cols.append(col_name)

# Prepare X and y
X_clf = df_clf[clf_feature_cols].fillna(0)
y_clf = df_clf['ROLE_CATEGORY']

print(f"Classification features: {len(clf_feature_cols)}")
print(f"Samples per class:")
print(y_clf.value_counts())

# Train-test split
X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(
    X_clf, y_clf, test_size=0.3, random_state=42, stratify=y_clf
)

# Scale features
scaler_clf = StandardScaler()
X_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)
X_test_clf_scaled = scaler_clf.transform(X_test_clf)
```

### Logistic Regression Classification

```{python}
#| echo: false

# Train logistic regression
lr = LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial')
lr.fit(X_train_clf_scaled, y_train_clf)
y_pred_lr = lr.predict(X_test_clf_scaled)

# Calculate metrics
acc_lr = accuracy_score(y_test_clf, y_pred_lr)
f1_lr = f1_score(y_test_clf, y_pred_lr, average='weighted')

print("LOGISTIC REGRESSION CLASSIFICATION")
print("=" * 50)
print(f"Accuracy: {acc_lr:.4f} ({acc_lr*100:.2f}%)")
print(f"F1 Score (Weighted): {f1_lr:.4f}")
print("\nClassification Report:")
print(classification_report(y_test_clf, y_pred_lr))
```

### Random Forest Classification

```{python}
#| echo: false

# Train random forest classifier
rf_clf = RandomForestClassifier(n_estimators=100, max_depth=15, 
                                min_samples_split=10, random_state=42, n_jobs=-1)
rf_clf.fit(X_train_clf, y_train_clf)
y_pred_rf_clf = rf_clf.predict(X_test_clf)

# Calculate metrics
acc_rf_clf = accuracy_score(y_test_clf, y_pred_rf_clf)
f1_rf_clf = f1_score(y_test_clf, y_pred_rf_clf, average='weighted')

print("RANDOM FOREST CLASSIFICATION")
print("=" * 50)
print(f"Accuracy: {acc_rf_clf:.4f} ({acc_rf_clf*100:.2f}%)")
print(f"F1 Score (Weighted): {f1_rf_clf:.4f}")
print("\nClassification Report:")
print(classification_report(y_test_clf, y_pred_rf_clf))
```

### Classification Model Comparison

```{python}
#| fig-cap: "Classification Model Performance Comparison"
#| fig-height: 12
#| echo: false

fig, axes = plt.subplots(2, 2, figsize=(14, 12))

# Confusion Matrix - Logistic Regression
cm_lr = confusion_matrix(y_test_clf, y_pred_lr)
sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', alpha=0.8,
            xticklabels=lr.classes_, yticklabels=lr.classes_,
            ax=axes[0, 0], cbar_kws={'label': 'Count'}, linewidths=0.5)
axes[0, 0].set_xlabel('Predicted', fontweight='bold', fontsize=11)
axes[0, 0].set_ylabel('Actual', fontweight='bold', fontsize=11)
axes[0, 0].set_title(f'Logistic Regression\nAccuracy={acc_lr:.4f}', fontweight='bold', fontsize=12, pad=15)
plt.setp(axes[0, 0].get_xticklabels(), rotation=45, ha='right')

# Confusion Matrix - Random Forest
cm_rf = confusion_matrix(y_test_clf, y_pred_rf_clf)
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens', alpha=0.8,
            xticklabels=rf_clf.classes_, yticklabels=rf_clf.classes_,
            ax=axes[0, 1], cbar_kws={'label': 'Count'}, linewidths=0.5)
axes[0, 1].set_xlabel('Predicted', fontweight='bold', fontsize=11)
axes[0, 1].set_ylabel('Actual', fontweight='bold', fontsize=11)
axes[0, 1].set_title(f'Random Forest\nAccuracy={acc_rf_clf:.4f}', fontweight='bold', fontsize=12, pad=15)
plt.setp(axes[0, 1].get_xticklabels(), rotation=45, ha='right')

# Model Comparison
comparison_df = pd.DataFrame({
    'Model': ['Logistic Regression', 'Random Forest'],
    'Accuracy': [acc_lr, acc_rf_clf],
    'F1 Score': [f1_lr, f1_rf_clf]
})
comparison_melted = comparison_df.melt(id_vars='Model', var_name='Metric', value_name='Score')

sns.barplot(data=comparison_melted, x='Model', y='Score', hue='Metric', 
            ax=axes[1, 0], palette=['#FFB3BA', '#BAFFC9'])
axes[1, 0].set_ylabel('Score', fontweight='bold', fontsize=11)
axes[1, 0].set_xlabel('')
axes[1, 0].set_title('Model Performance Comparison', fontweight='bold', fontsize=12, pad=15)
axes[1, 0].set_ylim([0, 1.1])
axes[1, 0].legend(title='Metric', frameon=True, fancybox=True, shadow=True)
axes[1, 0].grid(True, alpha=0.2, axis='y', linestyle='--')

# Feature Importance
clf_importance = pd.DataFrame({
    'Feature': clf_feature_cols,
    'Importance': rf_clf.feature_importances_
}).sort_values('Importance', ascending=False).head(10)

sns.barplot(data=clf_importance, y='Feature', x='Importance', 
            ax=axes[1, 1], palette=['#FFB3BA', '#BAFFC9', '#BAE1FF', '#FFFFBA', '#FFDFBA'] * 2)
axes[1, 1].set_xlabel('Importance', fontweight='bold', fontsize=11)
axes[1, 1].set_ylabel('')
axes[1, 1].set_title('Top 10 Features - Classification', fontweight='bold', fontsize=12, pad=15)
axes[1, 1].grid(True, alpha=0.2, axis='x', linestyle='--')

plt.tight_layout()
plt.show()
```

## Key Insights for Job Seekers

::: {.callout-important icon=false}
## Machine Learning Insights for BA/DS/ML Career Planning

*Clustering Insights:*

- K-Means clustering reveals {optimal_k} distinct segments in the BA/DS/ML job market
- Jobs naturally group by salary level, experience requirements, and remote work options
- Understanding which cluster you target can help focus your job search

*Salary Prediction Findings:*

- Random Forest model (R²={r2_rf:.4f}) outperforms Linear Regression (R²={r2_mlr:.4f})
- Experience level and location are the strongest salary predictors
- Remote positions show different salary patterns than on-site roles
- Feature importance analysis reveals which skills and qualifications drive higher compensation

*Role Classification Results:*

- Models achieve {acc_rf_clf*100:.1f}% accuracy in distinguishing between BA, DS, and ML roles
- Each role type has distinct feature patterns
- Understanding these patterns helps tailor applications and skill development
- Business Analytics and Data Science roles show the most overlap
:::
