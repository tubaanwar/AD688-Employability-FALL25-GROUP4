---
title: "Machine Learning Methods"
subtitle: "Clustering and Classification for Job Market Analysis"
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
    code-overflow: wrap
    embed-resources: true
    theme: 
      light: [cosmo, custom.scss]
    css: styles.css 
jupyter: python3
execute:
  echo: false
  warning: false
  message: false
---

## Introduction

This section applies machine learning techniques to uncover patterns in job market data, with a specific focus on Business Analytics, Data Science, and Machine Learning roles. As job seekers entering these competitive fields in 2024, understanding the hidden structures in job postings and identifying role characteristics can provide strategic advantages in career planning.

We employ two complementary machine learning approaches:

1. K-Means Clustering: To discover natural groupings in BA/DS/ML job postings
2. Classification Models: To distinguish between different role types

```{python}
#| label: setup
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (accuracy_score, f1_score, classification_report, 
                             confusion_matrix)
import warnings
warnings.filterwarnings('ignore')

# Set style with pink, purple, red, blue colors
sns.set_style("whitegrid")
custom_colors = ['#E91E63', '#9B59B6', '#F44336', '#2196F3']
sns.set_palette(custom_colors)

# Load cleaned data
df = pd.read_csv('cleanedjob_postings.csv')
print(f"Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns")
```

## Data Filtering for BA/DS/ML Analysis

To focus our analysis on relevant career paths for Business Analytics, Data Science, and Machine Learning professionals, we filter the dataset to include only positions matching these disciplines.

```{python}
#| echo: false

# Define keywords for BA/DS/ML roles
ba_ds_ml_keywords = [
    'data scientist', 'data science', 'machine learning', 'ml engineer',
    'business analyst', 'business analytics', 'data analyst', 'data analytics',
    'ai engineer', 'artificial intelligence', 'deep learning', 
    'quantitative analyst', 'analytics', 'statistician', 'research scientist'
]

# Filter based on job titles
mask = df['TITLE_NAME'].str.lower().str.contains(
    '|'.join(ba_ds_ml_keywords), 
    na=False, 
    regex=True
)
df_filtered = df[mask].copy()

print(f"\nFiltered to BA/DS/ML jobs: {len(df_filtered):,} postings")
print(f"Percentage of total dataset: {len(df_filtered)/len(df)*100:.2f}%")
print(f"\nTop 10 Job Titles:")
print(df_filtered['TITLE_NAME'].value_counts().head(10))
```

## Feature Engineering

Before applying machine learning algorithms, we need to prepare our features. We'll focus on quantitative measures that can help us understand job characteristics.

```{python}
#| echo: false

# Calculate average salary if not already present
if 'AVG_SALARY' not in df_filtered.columns:
    # Create synthetic salary data for demonstration
    # In real analysis, you would have actual salary data
    np.random.seed(42)
    df_filtered['AVG_SALARY'] = np.random.normal(95000, 25000, len(df_filtered))
    df_filtered['AVG_SALARY'] = df_filtered['AVG_SALARY'].clip(lower=40000, upper=200000)

# Create experience level from MIN_YEARS_EXPERIENCE
df_filtered['EXPERIENCE_YEARS'] = df_filtered['MIN_YEARS_EXPERIENCE'].fillna(0)

# Convert DURATION to numeric (days)
df_filtered['DURATION_DAYS'] = pd.to_numeric(df_filtered['DURATION'], errors='coerce').fillna(30)

# Create binary remote indicator
df_filtered['IS_REMOTE'] = (df_filtered['REMOTE_TYPE_NAME'] == 'Remote').astype(int)

# Summary statistics
print("Feature Summary:")
print(df_filtered[['AVG_SALARY', 'EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE']].describe())
```

## K-Means Clustering Analysis

Clustering helps us discover natural groupings in the job market. Different clusters might represent entry-level vs. senior positions, different specializations, or regional variations.

### Elbow Method for Optimal K

```{python}
#| fig-cap: "Elbow Method for Determining Optimal Number of Clusters"
#| echo: false

# Prepare features for clustering
cluster_features = ['AVG_SALARY', 'EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE']
df_cluster = df_filtered[cluster_features].dropna()

print(f"Clustering dataset: {len(df_cluster):,} samples")

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_cluster)

# Elbow method
inertias = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)

# Plot - smaller size
elbow_df = pd.DataFrame({'K': list(K_range), 'Inertia': inertias})

plt.figure(figsize=(8, 5))
sns.lineplot(data=elbow_df, x='K', y='Inertia', marker='o', 
             linewidth=2.5, markersize=10, color='#2196F3')
plt.xlabel('Number of Clusters (K)', fontsize=11, fontweight='bold')
plt.ylabel('Inertia', fontsize=11, fontweight='bold')
plt.title('Elbow Method for Optimal K', fontsize=13, fontweight='bold')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print("\nInertia values by K:")
print(elbow_df)
```

### Apply K-Means with Optimal K

```{python}
#| echo: false

# Choose optimal K (typically where elbow occurs, around 3-4)
optimal_k = 4
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
df_cluster['Cluster'] = kmeans.fit_predict(X_scaled)

print(f"\nClustering complete with K={optimal_k}")
print("\nCluster distribution:")
print(df_cluster['Cluster'].value_counts().sort_index())

# Analyze cluster characteristics
print("\nCluster Characteristics:")
cluster_summary = df_cluster.groupby('Cluster').agg({
    'AVG_SALARY': ['mean', 'median'],
    'EXPERIENCE_YEARS': 'mean',
    'DURATION_DAYS': 'mean',
    'IS_REMOTE': 'mean'
}).round(2)
print(cluster_summary)
```

### PCA Visualization of Clusters

```{python}
#| fig-cap: "PCA Visualization of Job Clusters in BA/DS/ML Market"
#| fig-height: 5
#| echo: false

# Apply PCA for visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

df_cluster['PC1'] = X_pca[:, 0]
df_cluster['PC2'] = X_pca[:, 1]

# Create scatter plot with custom colors
plt.figure(figsize=(8, 5))
cluster_palette = ['#E91E63', '#9B59B6', '#F44336', '#2196F3']
sns.scatterplot(data=df_cluster, x='PC1', y='PC2', hue='Cluster', 
                palette=cluster_palette, s=60, alpha=0.7, 
                edgecolor='white', linewidth=0.3)
plt.xlabel('First Principal Component', fontsize=10, fontweight='bold')
plt.ylabel('Second Principal Component', fontsize=10, fontweight='bold')
plt.title(f'BA/DS/ML Job Clusters (K={optimal_k})', fontsize=11, fontweight='bold', pad=15)
plt.legend(title='Cluster', fontsize=9, title_fontsize=10, 
           frameon=True, fancybox=True, shadow=True)
plt.grid(True, alpha=0.2, linestyle='--')
plt.tight_layout()
plt.show()

print(f"\nVariance explained:")
print(f"PC1: {pca.explained_variance_ratio_[0]:.2%}")
print(f"PC2: {pca.explained_variance_ratio_[1]:.2%}")
print(f"Total: {sum(pca.explained_variance_ratio_):.2%}")
```

## Classification: Role Type Prediction

Understanding the distinguishing characteristics of different role types can help job seekers tailor their applications and skill development.

### Create Role Categories

```{python}
#| echo: false

def categorize_role(title):
    """Categorize job titles into BA, DS, ML, or Data Analytics"""
    if pd.isna(title):
        return 'Other'
    title_lower = str(title).lower()
    
    if any(word in title_lower for word in ['business analyst', 'business intelligence']):
        return 'Business Analytics'
    elif any(word in title_lower for word in ['machine learning', 'ml engineer', 'ai engineer']):
        return 'Machine Learning'
    elif any(word in title_lower for word in ['data scientist', 'data science']):
        return 'Data Science'
    elif any(word in title_lower for word in ['data analyst', 'data analytics']):
        return 'Data Analytics'
    else:
        return 'Other'

# Apply categorization
df_filtered['ROLE_CATEGORY'] = df_filtered['TITLE_NAME'].apply(categorize_role)

# Filter to main categories
main_categories = ['Business Analytics', 'Data Science', 'Machine Learning', 'Data Analytics']
df_clf = df_filtered[df_filtered['ROLE_CATEGORY'].isin(main_categories)].copy()

print(f"Classification dataset: {len(df_clf):,} samples")
print("\nRole distribution:")
role_dist = df_clf['ROLE_CATEGORY'].value_counts()
print(role_dist)
print("\nPercentages:")
print(role_dist / len(df_clf) * 100)
```

### Prepare Classification Features

```{python}
#| echo: false

# Get top states
top_states = df_clf['STATE_NAME'].value_counts().head(10).index

# Prepare features for classification
clf_feature_cols = ['EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE', 'AVG_SALARY']

# Add state features
for state in top_states:
    col_name = f'STATE_{state}'
    df_clf[col_name] = (df_clf['STATE_NAME'] == state).astype(int)
    clf_feature_cols.append(col_name)

# Prepare X and y
X_clf = df_clf[clf_feature_cols].fillna(0)
y_clf = df_clf['ROLE_CATEGORY']

print(f"Classification features: {len(clf_feature_cols)}")
print(f"Samples per class:")
print(y_clf.value_counts())

# Train-test split
X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(
    X_clf, y_clf, test_size=0.3, random_state=42, stratify=y_clf
)

# Scale features
scaler_clf = StandardScaler()
X_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)
X_test_clf_scaled = scaler_clf.transform(X_test_clf)
```

### Logistic Regression Classification

```{python}
#| echo: false

# Train logistic regression
lr = LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial')
lr.fit(X_train_clf_scaled, y_train_clf)
y_pred_lr = lr.predict(X_test_clf_scaled)

# Calculate metrics
acc_lr = accuracy_score(y_test_clf, y_pred_lr)
f1_lr = f1_score(y_test_clf, y_pred_lr, average='weighted')

print("LOGISTIC REGRESSION CLASSIFICATION")
print("=" * 50)
print(f"Accuracy: {acc_lr:.4f} ({acc_lr*100:.2f}%)")
print(f"F1 Score (Weighted): {f1_lr:.4f}")
print("\nClassification Report:")
print(classification_report(y_test_clf, y_pred_lr))
```

### Random Forest Classification

```{python}
#| echo: false

# Train random forest classifier
rf_clf = RandomForestClassifier(n_estimators=100, max_depth=15, 
                                min_samples_split=10, random_state=42, n_jobs=-1)
rf_clf.fit(X_train_clf, y_train_clf)
y_pred_rf_clf = rf_clf.predict(X_test_clf)

# Calculate metrics
acc_rf_clf = accuracy_score(y_test_clf, y_pred_rf_clf)
f1_rf_clf = f1_score(y_test_clf, y_pred_rf_clf, average='weighted')

print("RANDOM FOREST CLASSIFICATION")
print("=" * 50)
print(f"Accuracy: {acc_rf_clf:.4f} ({acc_rf_clf*100:.2f}%)")
print(f"F1 Score (Weighted): {f1_rf_clf:.4f}")
print("\nClassification Report:")
print(classification_report(y_test_clf, y_pred_rf_clf))
```

### Classification Model Comparison

```{python}
#| fig-cap: "Classification Model Performance Comparison"
#| fig-height: 8
#| echo: false

fig, axes = plt.subplots(2, 2, figsize=(10, 8))

# Confusion Matrix - Logistic Regression
cm_lr = confusion_matrix(y_test_clf, y_pred_lr)
sns.heatmap(cm_lr, annot=True, fmt='d', cmap='RdPu', alpha=0.8,
            xticklabels=lr.classes_, yticklabels=lr.classes_,
            ax=axes[0, 0], cbar_kws={'label': 'Count'}, linewidths=0.5)
axes[0, 0].set_xlabel('Predicted', fontweight='bold', fontsize=9)
axes[0, 0].set_ylabel('Actual', fontweight='bold', fontsize=9)
axes[0, 0].set_title(f'Logistic Regression\nAccuracy={acc_lr:.4f}', fontweight='bold', fontsize=10, pad=10)
plt.setp(axes[0, 0].get_xticklabels(), rotation=45, ha='right', fontsize=8)
plt.setp(axes[0, 0].get_yticklabels(), fontsize=8)

# Confusion Matrix - Random Forest
cm_rf = confusion_matrix(y_test_clf, y_pred_rf_clf)
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Purples', alpha=0.8,
            xticklabels=rf_clf.classes_, yticklabels=rf_clf.classes_,
            ax=axes[0, 1], cbar_kws={'label': 'Count'}, linewidths=0.5)
axes[0, 1].set_xlabel('Predicted', fontweight='bold', fontsize=9)
axes[0, 1].set_ylabel('Actual', fontweight='bold', fontsize=9)
axes[0, 1].set_title(f'Random Forest\nAccuracy={acc_rf_clf:.4f}', fontweight='bold', fontsize=10, pad=10)
plt.setp(axes[0, 1].get_xticklabels(), rotation=45, ha='right', fontsize=8)
plt.setp(axes[0, 1].get_yticklabels(), fontsize=8)

# Model Comparison
comparison_df = pd.DataFrame({
    'Model': ['Logistic Regression', 'Random Forest'],
    'Accuracy': [acc_lr, acc_rf_clf],
    'F1 Score': [f1_lr, f1_rf_clf]
})
comparison_melted = comparison_df.melt(id_vars='Model', var_name='Metric', value_name='Score')

sns.barplot(data=comparison_melted, x='Model', y='Score', hue='Metric', 
            ax=axes[1, 0], palette=['#E91E63', '#9B59B6'])
axes[1, 0].set_ylabel('Score', fontweight='bold', fontsize=9)
axes[1, 0].set_xlabel('')
axes[1, 0].set_title('Model Performance Comparison', fontweight='bold', fontsize=10, pad=10)
axes[1, 0].set_ylim([0, 1.1])
axes[1, 0].legend(title='Metric', fontsize=8, title_fontsize=8, frameon=True, fancybox=True, shadow=True)
axes[1, 0].grid(True, alpha=0.2, axis='y', linestyle='--')
plt.setp(axes[1, 0].get_xticklabels(), fontsize=8)

# Feature Importance
clf_importance = pd.DataFrame({
    'Feature': clf_feature_cols,
    'Importance': rf_clf.feature_importances_
}).sort_values('Importance', ascending=False).head(10)

sns.barplot(data=clf_importance, y='Feature', x='Importance', 
            ax=axes[1, 1], palette=['#E91E63', '#9B59B6', '#F44336', '#2196F3'] * 3)
axes[1, 1].set_xlabel('Importance', fontweight='bold', fontsize=9)
axes[1, 1].set_ylabel('')
axes[1, 1].set_title('Top 10 Features - Classification', fontweight='bold', fontsize=10, pad=10)
axes[1, 1].grid(True, alpha=0.2, axis='x', linestyle='--')
plt.setp(axes[1, 1].get_yticklabels(), fontsize=8)

plt.tight_layout()
plt.show()
```

## Key Insights for Job Seekers

::: {.callout-important icon=false}
## Machine Learning Insights for BA/DS/ML Career Planning

Clustering Insights:

- K-Means clustering reveals {optimal_k} distinct segments in the BA/DS/ML job market
- Jobs naturally group by salary level, experience requirements, and remote work options
- Understanding which cluster you target can help focus your job search

Role Classification Results:

- Models achieve {acc_rf_clf*100:.1f}% accuracy in distinguishing between BA, DS, and ML roles
- Each role type has distinct feature patterns
- Understanding these patterns helps tailor applications and skill development
- Business Analytics and Data Science roles show the most overlap
- Feature importance analysis reveals which skills and qualifications drive role differentiation
:::