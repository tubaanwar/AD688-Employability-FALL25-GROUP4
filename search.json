[
  {
    "objectID": "final_report.html",
    "href": "final_report.html",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "This report presents a comprehensive analysis of the job market prospects for data analytics and related roles in 2024. Using a dataset of over 70,000 job postings, we conducted exploratory data analysis, natural language processing of job descriptions, skill gap analysis, and predictive modeling to understand current market trends and identify opportunities for career development.\nKey Findings:\n\nSAP Applications, Oracle Cloud, and Microsoft Office are the most in-demand software skills\nData Analysis capabilities appear in over 25,000 job postings\nSignificant skill gaps exist in enterprise software (SAP, Oracle) and cloud platforms (AWS, Azure)\nMachine learning models achieved XX% accuracy in predicting job categories\nSalary predictions show strong correlation with experience level and technical skill proficiency"
  },
  {
    "objectID": "final_report.html#project-overview",
    "href": "final_report.html#project-overview",
    "title": "Skill Gap Analysis",
    "section": "3.1 Project Overview",
    "text": "3.1 Project Overview\nThis research explores the evolving landscape of Business Analytics, Data Science and Machine Learning (ML) careers in 2024. As organizations increasingly integrate AI in every function from operations to strategic decision-making, the demand for professionals with strong data and analytics skills is growing rapidly. This study explores four interconnected questions: the most sought-after skills for analytics and ML roles, how job descriptions are changing to require AI literacy, which industries are leading data-driven hiring, and what the short- to medium-term career outlook looks like for analytics professionals. By combining insights from labor market data and recent academic research, this project provides a holistic view of how AI and automation are transforming workforce expectations and employability strategies for students and professionals."
  },
  {
    "objectID": "final_report.html#data-source",
    "href": "final_report.html#data-source",
    "title": "Skill Gap Analysis",
    "section": "7.1 Data Source",
    "text": "7.1 Data Source\nWe will utilize the Lightcast 2024 dataset, which includes: - Job posting volumes for analytics, data science, and ML roles by industry and location - Salary data across data-related positions and geographies - Skill requirements extracted from job descriptions - Hiring trends across time periods - Company size and industry classifications - Emerging role titles and job requirements"
  },
  {
    "objectID": "final_report.html#analysis-approach",
    "href": "final_report.html#analysis-approach",
    "title": "Skill Gap Analysis",
    "section": "7.2 Analysis Approach",
    "text": "7.2 Analysis Approach\nOur team will:\n\nClean and preprocess the Lightcast data using Python (pandas, NumPy)\nExtract and categorize skills mentioned in job descriptions for analytics and ML roles\nCompare trends across industries, geographies, and job levels\nAnalyze salary patterns to understand compensation for different skill combinations\nIdentify emerging roles and how job requirements are evolving\nVisualize findings through interactive dashboards using Plotly and Matplotlib\nDevelop career strategy recommendations based on market insights"
  },
  {
    "objectID": "final_report.html#expected-findings",
    "href": "final_report.html#expected-findings",
    "title": "Skill Gap Analysis",
    "section": "7.3 Expected Findings",
    "text": "7.3 Expected Findings\nWe anticipate discovering that: - Python, SQL, and cloud platforms (AWS, GCP, Azure) remain foundational skills - Generative AI and prompt engineering are emerging as newly valued competencies - Finance, healthcare, and technology sectors lead in analytics hiring - Soft skills like communication and domain expertise are increasingly emphasized - Analytics roles offer strong job security and career growth potential - Salary premiums exist for ML/AI-specialized roles compared to traditional business analytics"
  },
  {
    "objectID": "final_report.html#deliverables-future-phases",
    "href": "final_report.html#deliverables-future-phases",
    "title": "Skill Gap Analysis",
    "section": "7.4 Deliverables (Future Phases)",
    "text": "7.4 Deliverables (Future Phases)\n\nExploratory Data Analysis (EDA) with visualizations\nInteractive dashboards showing skill trends, industry hiring patterns, and salary insights\nCareer pathway recommendations for different specializations\nPersonal career action plans for each team member"
  },
  {
    "objectID": "final_report.html#introduction-2",
    "href": "final_report.html#introduction-2",
    "title": "Skill Gap Analysis",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nThis document details the comprehensive data cleaning and preprocessing steps applied to the 2024 job market dataset.\n\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load raw data\ndf = pd.read_csv('lightcast_job_postings.csv')\n\nprint(f\"Initial dataset shape: {df.shape}\")\nprint(f\"Total rows: {df.shape[0]:,}\")\nprint(f\"Total columns: {df.shape[1]}\")\n\nInitial dataset shape: (72498, 131)\nTotal rows: 72,498\nTotal columns: 131"
  },
  {
    "objectID": "final_report.html#step-1-removing-redundant-columns",
    "href": "final_report.html#step-1-removing-redundant-columns",
    "title": "Skill Gap Analysis",
    "section": "9.2 Step 1: Removing Redundant Columns",
    "text": "9.2 Step 1: Removing Redundant Columns\nWe remove redundant columns to improve dataset quality and analysis efficiency.\n\n# List of columns to drop\ncolumns_to_drop = [\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n    \"BODY\", \"TITLE_RAW\", \"COMPANY_RAW\", \"ACTIVE_SOURCES_INFO\",\n    \"NAICS2\", \"NAICS2_NAME\", \"NAICS3\", \"NAICS3_NAME\", \n    \"NAICS4\", \"NAICS4_NAME\", \"NAICS5\", \"NAICS5_NAME\", \n    \"NAICS6\", \"NAICS6_NAME\",\n    \"SOC_2\", \"SOC_2_NAME\", \"SOC_3\", \"SOC_3_NAME\", \n    \"SOC_4\", \"SOC_4_NAME\", \"SOC_5\", \"SOC_5_NAME\",\n    \"SOC_2021_2\", \"SOC_2021_2_NAME\", \"SOC_2021_3\", \"SOC_2021_3_NAME\", \n    \"SOC_2021_5\", \"SOC_2021_5_NAME\",\n    \"LOT_CAREER_AREA\", \"LOT_CAREER_AREA_NAME\", \"LOT_OCCUPATION\", \"LOT_OCCUPATION_NAME\",\n    \"LOT_SPECIALIZED_OCCUPATION\", \"LOT_SPECIALIZED_OCCUPATION_NAME\", \n    \"LOT_OCCUPATION_GROUP\", \"LOT_OCCUPATION_GROUP_NAME\",\n    \"LOT_V6_SPECIALIZED_OCCUPATION\", \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\", \n    \"LOT_V6_OCCUPATION\", \"LOT_V6_OCCUPATION_NAME\",\n    \"LOT_V6_OCCUPATION_GROUP\", \"LOT_V6_OCCUPATION_GROUP_NAME\", \n    \"LOT_V6_CAREER_AREA\", \"LOT_V6_CAREER_AREA_NAME\",\n    \"ONET_2019\", \"ONET_2019_NAME\", \"CIP6\", \"CIP6_NAME\", \"CIP2\", \"CIP2_NAME\",\n    \"COUNTY\", \"COUNTY_NAME\", \"COUNTY_OUTGOING\", \"COUNTY_NAME_OUTGOING\", \n    \"COUNTY_INCOMING\", \"COUNTY_NAME_INCOMING\",\n    \"MSA\", \"MSA_OUTGOING\", \"MSA_INCOMING\",\n    \"SALARY_TO\", \"SALARY_FROM\", \"ORIGINAL_PAY_PERIOD\",\n    \"MODELED_EXPIRED\", \"MODELED_DURATION\"\n]\n\n# Drop only columns that exist\ncolumns_to_drop = [col for col in columns_to_drop if col in df.columns]\ndf_cleaned = df.drop(columns=columns_to_drop, inplace=False)\n\nprint(f\"Columns removed: {len(columns_to_drop)}\")\nprint(f\"New dataset shape: {df_cleaned.shape}\")\n\nColumns removed: 69\nNew dataset shape: (72498, 62)"
  },
  {
    "objectID": "final_report.html#step-2-handling-missing-values",
    "href": "final_report.html#step-2-handling-missing-values",
    "title": "Skill Gap Analysis",
    "section": "9.3 Step 2: Handling Missing Values",
    "text": "9.3 Step 2: Handling Missing Values\n\n# Missing value statistics\nmissing_stats = pd.DataFrame({\n    'Column': df_cleaned.columns,\n    'Missing_Count': df_cleaned.isnull().sum().values,\n    'Missing_Percentage': (df_cleaned.isnull().sum().values / len(df_cleaned) * 100).round(2)\n})\nmissing_stats = missing_stats[missing_stats['Missing_Count'] &gt; 0].sort_values('Missing_Percentage', ascending=False)\n\nprint(f\"Columns with missing values: {len(missing_stats)}\")\nprint(\"\\nTop 10 columns with highest missing percentages:\")\nprint(missing_stats.head(10))\n\nColumns with missing values: 62\n\nTop 10 columns with highest missing percentages:\n                    Column  Missing_Count  Missing_Percentage\n18    MAX_YEARS_EXPERIENCE          64068               88.37\n13           MAX_EDULEVELS          56183               77.50\n14      MAX_EDULEVELS_NAME          56183               77.50\n50       LIGHTCAST_SECTORS          54711               75.47\n51  LIGHTCAST_SECTORS_NAME          54711               75.47\n20                  SALARY          41690               57.51\n3                 DURATION          27316               37.68\n17    MIN_YEARS_EXPERIENCE          23146               31.93\n2                  EXPIRED           7844               10.82\n30       MSA_NAME_INCOMING           3962                5.46\n\n\n\n# Drop columns with &gt;50% missing values\nthreshold = len(df_cleaned) * 0.5\ncols_before = len(df_cleaned.columns)\ndf_cleaned = df_cleaned.dropna(thresh=threshold, axis=1)\ncols_after = len(df_cleaned.columns)\n\nprint(f\"Columns dropped due to &gt;50% missing values: {cols_before - cols_after}\")\n\n# Fill numerical columns with median\nnumerical_cols = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\nfor col in numerical_cols:\n    if df_cleaned[col].isnull().sum() &gt; 0:\n        median_val = df_cleaned[col].median()\n        df_cleaned[col].fillna(median_val, inplace=True)\n\n# Fill categorical columns with \"Unknown\"\ncategorical_cols = df_cleaned.select_dtypes(include=['object']).columns.tolist()\nfor col in categorical_cols:\n    if df_cleaned[col].isnull().sum() &gt; 0:\n        df_cleaned[col].fillna(\"Unknown\", inplace=True)\n\nprint(f\"Total missing values after imputation: {df_cleaned.isnull().sum().sum()}\")\n\nColumns dropped due to &gt;50% missing values: 6\nTotal missing values after imputation: 0"
  },
  {
    "objectID": "final_report.html#step-3-removing-duplicates",
    "href": "final_report.html#step-3-removing-duplicates",
    "title": "Skill Gap Analysis",
    "section": "9.4 Step 3: Removing Duplicates",
    "text": "9.4 Step 3: Removing Duplicates\n\n# Check for duplicates\ninitial_rows = len(df_cleaned)\nduplicates_count = df_cleaned.duplicated(subset=['TITLE_NAME', 'COMPANY_NAME', 'LOCATION']).sum()\n\nprint(f\"Initial rows: {initial_rows:,}\")\nprint(f\"Duplicate rows detected: {duplicates_count:,}\")\n\n# Remove duplicates\ndf_cleaned = df_cleaned.drop_duplicates(\n    subset=['TITLE_NAME', 'COMPANY_NAME', 'LOCATION'], \n    keep='first'\n)\n\nfinal_rows = len(df_cleaned)\nprint(f\"Final rows after duplicate removal: {final_rows:,}\")\nprint(f\"Rows removed: {initial_rows - final_rows:,}\")\n\nInitial rows: 72,498\nDuplicate rows detected: 13,278\nFinal rows after duplicate removal: 59,220\nRows removed: 13,278"
  },
  {
    "objectID": "final_report.html#step-4-final-summary",
    "href": "final_report.html#step-4-final-summary",
    "title": "Skill Gap Analysis",
    "section": "9.5 Step 4: Final Summary",
    "text": "9.5 Step 4: Final Summary\n\n# Create summary statistics\nsummary_stats = pd.DataFrame({\n    'Metric': [\n        'Total Rows',\n        'Total Columns',\n        'Numerical Columns',\n        'Categorical Columns',\n        'Missing Values'\n    ],\n    'Value': [\n        f\"{len(df_cleaned):,}\",\n        f\"{len(df_cleaned.columns)}\",\n        f\"{len(df_cleaned.select_dtypes(include=[np.number]).columns)}\",\n        f\"{len(df_cleaned.select_dtypes(include=['object']).columns)}\",\n        f\"{df_cleaned.isnull().sum().sum()}\"\n    ]\n})\n\nprint(summary_stats.to_string(index=False))\n\n# Save cleaned dataset\ndf_cleaned.to_csv('cleanedjob_postings.csv', index=False)\nprint(\"\\nCleaned dataset saved as 'cleanedjob_postings.csv'\")\n\n             Metric  Value\n         Total Rows 59,220\n      Total Columns     56\n  Numerical Columns     12\nCategorical Columns     44\n     Missing Values      0\n\nCleaned dataset saved as 'cleanedjob_postings.csv'"
  },
  {
    "objectID": "final_report.html#summary",
    "href": "final_report.html#summary",
    "title": "Skill Gap Analysis",
    "section": "9.6 Summary",
    "text": "9.6 Summary\nThe data cleaning process has successfully prepared the job market dataset:\n\nRemoved redundant columns\nHandled missing values strategically\nRemoved duplicate postings\nFinal clean dataset ready for analysis"
  },
  {
    "objectID": "final_report.html#introduction-3",
    "href": "final_report.html#introduction-3",
    "title": "Skill Gap Analysis",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\nThis document presents a comprehensive exploratory data analysis of the 2024 job market. We examine hiring trends, employment types, geographic patterns, and remote work opportunities to provide actionable insights for job seekers and market analysts.\n\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport panel as pn\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Enable panel for rendering\npn.extension()\n\n# Updated color palette - blue, pink, purple, red\nCOLORS = ['#2196F3', '#E91E63', '#9B59B6', '#F44336', '#00BCD4', '#FF4081', '#7E57C2', '#EF5350']\n\n# Load cleaned data\ndf = pd.read_csv('cleanedjob_postings.csv')\n\n#print(f\"Dataset shape: {df.shape}\")\n#print(f\"Analysis period: 2024 Job Market\")\n#print(f\"Total job postings analyzed: {len(df):,}\")"
  },
  {
    "objectID": "final_report.html#job-title-analysis",
    "href": "final_report.html#job-title-analysis",
    "title": "Skill Gap Analysis",
    "section": "10.2 1. Job Title Analysis",
    "text": "10.2 1. Job Title Analysis\n\n10.2.1 1.1 Top In-Demand Job Titles\nUnderstanding which job titles dominate the market helps identify high-demand roles and emerging career opportunities.\n\n# Get top 10 job titles \ntitle_counts = df[df['TITLE_NAME'].notna()]['TITLE_NAME'].value_counts().head(10)\ntitle_df = pd.DataFrame({\n    'Job Title': title_counts.index, \n    'Count': title_counts.values,\n    'Percentage': (title_counts.values / len(df[df['TITLE_NAME'].notna()]) * 100).round(2)\n})\n\nprint(f\"\\nTop job titles : {len(title_counts)} titles\")\n\n# Create interactive horizontal bar chart\ntitle_df.hvplot.barh(\n    x='Job Title',\n    y='Count',\n    title='Top 10 Most In-Demand Job Titles (2024)',\n    height=500,\n    width=800,\n    color='#2196F3',\n    hover_cols=['Percentage'],\n    ylabel='',\n    xlabel='Number of Job Postings',\n    flip_yaxis=True\n).opts(xformatter='%.0f')\n\n\nTop job titles : 10 titles\n\n\n\n\n\n\n  \n\n\n\n\nThe Top 10 most in-demand jobs are dominated by Data Analyst roles, which appear far more frequently than any other title. Business Intelligence Analysts, Enterprise Architects, and Data Modelers also rank highly, highlighting strong employer demand for data-focused and technical architecture skills. Overall, the top roles show a clear market emphasis on analytics, data management, and solution-oriented positions in 2024."
  },
  {
    "objectID": "final_report.html#employment-type-distribution",
    "href": "final_report.html#employment-type-distribution",
    "title": "Skill Gap Analysis",
    "section": "10.3 2. Employment Type Distribution",
    "text": "10.3 2. Employment Type Distribution\n\n10.3.1 2.1 Full-Time vs Part-Time vs Contract\nUnderstanding employment type distribution helps job seekers target positions matching their career preferences.\n\n# Count jobs by employment type \ndf_employment = df[df['EMPLOYMENT_TYPE_NAME'].notna()].copy()\n\n# Remove \"Unknown\" values\ndf_employment = df_employment[df_employment['EMPLOYMENT_TYPE_NAME'] != 'Unknown']\n\nemployment_counts = df_employment['EMPLOYMENT_TYPE_NAME'].value_counts()\nemployment_df = pd.DataFrame({\n    'Employment Type': employment_counts.index, \n    'Count': employment_counts.values,\n    'Percentage': (employment_counts.values / employment_counts.sum() * 100).round(1)\n})\n\n#print(f\"\\nEmployment types : {len(df_employment):,} jobs\")\n#print(employment_df.to_string(index=False))\n\n# Create bar chart\nemployment_df.hvplot.bar(\n    x='Employment Type',\n    y='Count',\n    title='Job Market Distribution by Employment Type (2024)',\n    height=400,\n    width=700,\n    color='Employment Type',\n    cmap=COLORS,\n    hover_cols=['Percentage'],\n    ylabel='Number of Job Postings',\n    rot=45\n).opts(yformatter='%.0f')\n\n\n\n\n\n  \n\n\n\n\nMost job postings in 2024 require around 5 years of experience, making it the dominant requirement across roles. Entry-level opportunities (0–2 years) exist but are significantly fewer, showing that employers prioritize mid-level talent with proven skills. Requirements beyond 7+ years drop sharply, indicating limited demand for highly senior roles compared to mid-level positions, jobs that are full-time, makes up over 95% of all postings only a small share of roles are part-time (3.3%) or mixed part-time/full-time (1.4%). This shows that employers mainly prefer hiring for stable, full-time positions."
  },
  {
    "objectID": "final_report.html#remote-work-analysis",
    "href": "final_report.html#remote-work-analysis",
    "title": "Skill Gap Analysis",
    "section": "10.4 3. Remote Work Analysis",
    "text": "10.4 3. Remote Work Analysis\n\n10.4.1 3.1 Remote vs Hybrid vs On-Site\nThe prevalence of remote work reflects post-pandemic hiring trends and employer flexibility.\n\n# Count jobs by remote type and remove not specified values\ndf_remote = df.copy()\n\n# Replace None, NaN, Unknown, empty strings with NaN\ndf_remote['REMOTE_TYPE_NAME'] = df_remote['REMOTE_TYPE_NAME'].replace({\n    '[None]': np.nan,\n    'None': np.nan,\n    'Unknown': np.nan,\n    '': np.nan,\n    'Not Specified': np.nan\n})\n\n# Drop rows with NaN in REMOTE_TYPE_NAME\ndf_remote = df_remote[df_remote['REMOTE_TYPE_NAME'].notna()]\n\n#print(f\"\\nRemote type distribution (excluding not specified):\")\n#print(f\"Total jobs with remote info: {len(df_remote):,}\")\n\n# Count jobs by remote type\nremote_counts = df_remote['REMOTE_TYPE_NAME'].value_counts()\nremote_df = pd.DataFrame({\n    'Remote Type': remote_counts.index,\n    'Count': remote_counts.values,\n    'Percentage': (remote_counts.values / remote_counts.sum() * 100).round(1)\n})\n\n#print(remote_df.to_string(index=False))\n\n# Create bar chart with custom colors\nremote_df.hvplot.bar(\n    x='Remote Type',\n    y='Count',\n    title='Job Market Distribution: Remote, Hybrid & On-Site Opportunities (2024)',\n    height=400,\n    width=700,\n    color='Remote Type',\n    cmap=['#2196F3', '#E91E63', '#9B59B6', '#F44336'],\n    hover_cols=['Percentage'],\n    ylabel='Number of Job Postings',\n    xlabel='Work Location Type',\n    rot=0\n).opts(yformatter='%.0f')\n\n\n\n\n\n  \n\n\n\n\nThe dominance of full-time roles in 2024 reflects a strong post-COVID recovery, as companies shift back toward stable, long-term hiring after years of uncertainty. The very small share of part-time and hybrid hour roles suggests that businesses are prioritizing consistent workforce availability to meet rising operational demands. Overall, the trend indicates increased employer confidence and a return to pre-pandemic hiring patterns focused on full-time talent."
  },
  {
    "objectID": "final_report.html#geographic-analysis",
    "href": "final_report.html#geographic-analysis",
    "title": "Skill Gap Analysis",
    "section": "10.5 4. Geographic Analysis",
    "text": "10.5 4. Geographic Analysis\n\n10.5.1 4.1 Top States for Job Opportunities\nGeographic distribution shows where job opportunities are concentrated.\n\n# Get top 10 states by job postings \ndf_states = df[df['STATE_NAME'].notna()].copy()\n\nstate_counts = df_states['STATE_NAME'].value_counts().head(10)\nstate_df = pd.DataFrame({\n    'State': state_counts.index,\n    'Job Postings': state_counts.values,\n    'Percentage': (state_counts.values / len(df_states) * 100).round(2)\n})\n\n#print(f\"\\nStates with job postings : {len(df_states):,} jobs\")\n\n# Create bar chart\nstate_df.hvplot.bar(\n    x='State',\n    y='Job Postings',\n    title='Top 10 States by Number of Job Postings (2024)',\n    height=450,\n    width=850,\n    color='#9B59B6',\n    hover_cols=['Percentage'],\n    ylabel='Number of Job Postings',\n    xlabel='State',\n    rot=45\n).opts(yformatter='%.0f')\n\n\n\n\n\n  \n\n\n\n\nThe top 10 states for job postings in 2024 are led by Texas and California, which together dominate the job market with substantially higher opportunities than the rest. States like Virginia, Florida, New York, and Illinois follow, reflecting strong demand in both tech-heavy and fast-growing regional economies. Overall, job opportunities are concentrated in major economic hubs and states with strong corporate, technology, and government sectors."
  },
  {
    "objectID": "final_report.html#top-hiring-companies",
    "href": "final_report.html#top-hiring-companies",
    "title": "Skill Gap Analysis",
    "section": "10.6 5. Top Hiring Companies",
    "text": "10.6 5. Top Hiring Companies\n\n10.6.1 5.1 Companies with Most Job Openings\nIdentifying top hiring companies helps job seekers target organizations with multiple opportunities.\n\n# Get top 10 companies by job postings (remove None values)\ndf_companies = df[df['COMPANY_NAME'].notna()].copy()\n\ncompany_counts = df_companies['COMPANY_NAME'].value_counts().head(10)\ncompany_df = pd.DataFrame({\n    'Company': company_counts.index,\n    'Job Postings': company_counts.values\n})\n\n#print(f\"\\nCompanies with job postings : {len(df_companies):,} jobs\")\n\n# Create horizontal bar chart\ncompany_df.hvplot.barh(\n    x='Company',\n    y='Job Postings',\n    title='Top 10 Companies by Number of Job Postings (2024)',\n    height=550,\n    width=800,\n    color='#E91E63',\n    ylabel='',\n    xlabel='Number of Job Postings',\n    flip_yaxis=True\n).opts(xformatter='%.0f')\n\n\n\n\n\n  \n\n\n\n\nThe Top 10 hiring companies in 2024 are led by Deloitte and Accenture, which show significantly higher job postings than other firms,reflecting strong demand for consulting, technology, and analytics roles. Companies like PwC, Insight Global, KPMG, and Lumen Technologies also appear prominently, indicating consistent hiring across both consulting and IT services sectors. Overall, the hiring landscape is dominated by large professional services firms, showcasing continued growth in advisory, digital transformation, and data-driven business roles."
  },
  {
    "objectID": "final_report.html#job-posting-timeline",
    "href": "final_report.html#job-posting-timeline",
    "title": "Skill Gap Analysis",
    "section": "10.7 6. Job Posting Timeline",
    "text": "10.7 6. Job Posting Timeline\n\n10.7.1 6.1 When Jobs Are Posted\nUnderstanding posting patterns helps with strategic job search timing. Since job postings peaked in August and September 2024, job seekers should target this late-summer period for applications. Companies appear to increase hiring toward the end of Q3, meaning more openings, faster responses, and higher chances of landing interviews. Preparing resumes, portfolios, and applications ahead of this surge (during May–July) can give job seekers a strategic advantage when the market becomes most active.\n\n# Print diagnostics\n#print(\"\\nAnalyzing POSTED column:\")\n#print(f\"Sample values: {df['POSTED'].head(10)}\")\n#print(f\"Data type: {df['POSTED'].dtype}\")\n\n# Convert POSTED to datetime\ndf_time = df.copy()\ndf_time['POSTED'] = pd.to_datetime(df_time['POSTED'], errors='coerce')\n\n#print(f\"\\nAfter conversion:\")\n#print(f\"Date range: {df_time['POSTED'].min()} to {df_time['POSTED'].max()}\")\n#print(f\"Valid dates: {df_time['POSTED'].notna().sum():,} out of {len(df_time):,}\")\n#print(f\"Missing dates: {df_time['POSTED'].isna().sum():,}\")\n\n# Filter valid dates\ntime_data = df_time[df_time['POSTED'].notna()].copy()\n\nif len(time_data) &gt; 0:\n    # Extract year-month\n    time_data['YearMonth'] = time_data['POSTED'].dt.to_period('M')\n    \n    # Group by month and count\n    monthly_counts = time_data.groupby('YearMonth').size().reset_index(name='Job Postings')\n    monthly_counts['Month'] = monthly_counts['YearMonth'].astype(str)\n    \n    # Sort by month\n    monthly_counts = monthly_counts.sort_values('Month')\n    \n    #print(f\"\\nMonthly posting counts:\")\n    #print(monthly_counts[['Month', 'Job Postings']].to_string(index=False))\n    \n    # Create line and area chart\n    line_chart = monthly_counts.hvplot.line(\n        x='Month',\n        y='Job Postings',\n        title='Job Posting Trends Over Time (2024)',\n        height=400,\n        width=850,\n        color='#2196F3',\n        line_width=3,\n        ylabel='Number of Job Postings',\n        xlabel='Month',\n        rot=45\n    ).opts(yformatter='%.0f')\n    \n    area_chart = monthly_counts.hvplot.area(\n        x='Month',\n        y='Job Postings',\n        alpha=0.3,\n        color='#2196F3'\n    )\n    \n    # Combine line and area\n    (line_chart * area_chart)\nelse:\n    #print(\"\\nWarning: No valid dates found in POSTED column\")\n    ##print(\"Sample of POSTED values:\")\n    print(df['POSTED'].head(20))"
  },
  {
    "objectID": "final_report.html#experience-requirements",
    "href": "final_report.html#experience-requirements",
    "title": "Skill Gap Analysis",
    "section": "10.8 7. Experience Requirements",
    "text": "10.8 7. Experience Requirements\n\n10.8.1 7.1 Minimum Years of Experience Required\nUnderstanding experience requirements helps assess job market accessibility.\n\n# Analyze minimum years of experience (remove None values)\ndf_experience = df[df['MIN_YEARS_EXPERIENCE'].notna()].copy()\n\nexp_counts = df_experience['MIN_YEARS_EXPERIENCE'].value_counts().sort_index()\nexp_df = pd.DataFrame({\n    'Years of Experience': exp_counts.index,\n    'Job Postings': exp_counts.values\n})\n\n# Convert Years of Experience to string for better labels\nexp_df['Years Label'] = exp_df['Years of Experience'].astype(int).astype(str) + ' years'\n\n#print(f\"\\nJobs with experience requirements (excluding None): {len(df_experience):,} jobs\")\n#print(exp_df[['Years of Experience', 'Job Postings']].to_string(index=False))\n\n# Create bar chart with pastel purple color\nexp_df.hvplot.bar(\n    x='Years Label',\n    y='Job Postings',\n    title='Job Postings by Minimum Years of Experience Required (2024)',\n    height=400,\n    width=800,\n    color='#B39DDB',\n    ylabel='Number of Job Postings',\n    xlabel='Minimum Years of Experience',\n    rot=45\n).opts(yformatter='%.0f')\n\n\n\n\n\n  \n\n\n\n\nMost job postings in 2024 require around 5 years of experience, making it the dominant requirement across roles. Entry-level opportunities (0–2 years) exist but are significantly fewer, showing that employers prioritize mid-level talent with proven skills. Requirements beyond 7+ years drop sharply, indicating limited demand for highly senior roles compared to mid-level positions."
  },
  {
    "objectID": "final_report.html#references-1",
    "href": "final_report.html#references-1",
    "title": "Skill Gap Analysis",
    "section": "10.9 References",
    "text": "10.9 References\n\nData source: Lightcast Job Postings Dataset (2024)\nVisualization tools: hvPlot, Panel, Python\nAnalysis framework: Standard EDA best practices"
  },
  {
    "objectID": "final_report.html#introduction-4",
    "href": "final_report.html#introduction-4",
    "title": "Skill Gap Analysis",
    "section": "11.1 Introduction",
    "text": "11.1 Introduction\nThis document provides a comprehensive analysis of job market data for 2024. We perform data cleaning, handle missing values, remove duplicates, and conduct exploratory data analysis to understand hiring trends, salary distributions, and job market dynamics.\n\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport holoviews as hv\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Initialize hvplot with bokeh backend\nhv.extension('bokeh')\n\n# Load your data\ndf = pd.read_csv('lightcast_job_postings.csv')"
  },
  {
    "objectID": "final_report.html#step-1-removing-redundant-columns-1",
    "href": "final_report.html#step-1-removing-redundant-columns-1",
    "title": "Skill Gap Analysis",
    "section": "11.2 Step 1: Removing Redundant Columns",
    "text": "11.2 Step 1: Removing Redundant Columns\n\n11.2.1 Why Remove These Columns?\nWe remove redundant columns to improve dataset quality and analysis efficiency. Specifically:\n\nTracking & Administrative Columns: ID, URL, DUPLICATES, LAST_UPDATED_TIMESTAMP - These are metadata and don’t contribute to analysis\nRaw Text Fields: BODY, TITLE_RAW, COMPANY_RAW - We have cleaned versions (TITLE, COMPANY_NAME, TITLE_CLEAN)\nDeprecated Classifications: We keep only the latest NAICS_2022_6, SOC_2021_4, and CIP4 standards\nDuplicate Geographic Fields: Multiple versions of county/MSA data create redundancy\n\n\n# List of columns to drop\ncolumns_to_drop = [\n    # Administrative & tracking columns\n    \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n    # Raw text columns (we have cleaned versions)\n    \"BODY\", \"TITLE_RAW\", \"COMPANY_RAW\", \"ACTIVE_SOURCES_INFO\",\n    # Deprecated NAICS versions (keeping NAICS_2022_6)\n    \"NAICS2\", \"NAICS2_NAME\", \"NAICS3\", \"NAICS3_NAME\", \n    \"NAICS4\", \"NAICS4_NAME\", \"NAICS5\", \"NAICS5_NAME\", \n    \"NAICS6\", \"NAICS6_NAME\",\n    # Deprecated SOC versions (keeping SOC_2021_4)\n    \"SOC_2\", \"SOC_2_NAME\", \"SOC_3\", \"SOC_3_NAME\", \n    \"SOC_4\", \"SOC_4_NAME\", \"SOC_5\", \"SOC_5_NAME\",\n    \"SOC_2021_2\", \"SOC_2021_2_NAME\", \"SOC_2021_3\", \"SOC_2021_3_NAME\", \n    \"SOC_2021_5\", \"SOC_2021_5_NAME\",\n    # Deprecated occupation classifications\n    \"LOT_CAREER_AREA\", \"LOT_CAREER_AREA_NAME\", \"LOT_OCCUPATION\", \"LOT_OCCUPATION_NAME\",\n    \"LOT_SPECIALIZED_OCCUPATION\", \"LOT_SPECIALIZED_OCCUPATION_NAME\", \n    \"LOT_OCCUPATION_GROUP\", \"LOT_OCCUPATION_GROUP_NAME\",\n    \"LOT_V6_SPECIALIZED_OCCUPATION\", \"LOT_V6_SPECIALIZED_OCCUPATION_NAME\", \n    \"LOT_V6_OCCUPATION\", \"LOT_V6_OCCUPATION_NAME\",\n    \"LOT_V6_OCCUPATION_GROUP\", \"LOT_V6_OCCUPATION_GROUP_NAME\", \n    \"LOT_V6_CAREER_AREA\", \"LOT_V6_CAREER_AREA_NAME\",\n    # Deprecated CIP and ONET versions\n    \"ONET_2019\", \"ONET_2019_NAME\", \"CIP6\", \"CIP6_NAME\", \"CIP2\", \"CIP2_NAME\",\n    # Duplicate geographic fields\n    \"COUNTY\", \"COUNTY_NAME\", \"COUNTY_OUTGOING\", \"COUNTY_NAME_OUTGOING\", \n    \"COUNTY_INCOMING\", \"COUNTY_NAME_INCOMING\",\n    \"MSA\", \"MSA_OUTGOING\", \"MSA_INCOMING\",\n    # Deprecated salary fields (keeping SALARY)\n    \"SALARY_TO\", \"SALARY_FROM\", \"ORIGINAL_PAY_PERIOD\",\n    # Model versions (keep actual data)\n    \"MODELED_EXPIRED\", \"MODELED_DURATION\"\n]\n\n# Drop only columns that exist in the dataset\ncolumns_to_drop = [col for col in columns_to_drop if col in df.columns]\ndf_cleaned = df.drop(columns=columns_to_drop, inplace=False)"
  },
  {
    "objectID": "final_report.html#step-2-handling-missing-values-1",
    "href": "final_report.html#step-2-handling-missing-values-1",
    "title": "Skill Gap Analysis",
    "section": "11.3 Step 2: Handling Missing Values",
    "text": "11.3 Step 2: Handling Missing Values\n\n11.3.1 Understanding Missing Data\nBefore imputation, let’s visualize where data is missing:\n\n# Missing value statistics\nmissing_stats = pd.DataFrame({\n    'Column': df_cleaned.columns,\n    'Missing_Count': df_cleaned.isnull().sum().values,\n    'Missing_Percentage': (df_cleaned.isnull().sum().values / len(df_cleaned) * 100).round(2)\n})\nmissing_stats = missing_stats[missing_stats['Missing_Count'] &gt; 0].sort_values('Missing_Percentage', ascending=False)\n\n# Interactive missing values visualization\nif len(missing_stats) &gt; 0:\n    # Select top 10 for cleaner visualization\n    top_missing = missing_stats.head(10)\n    missing_plot = top_missing.hvplot.barh(\n        x='Column', \n        y='Missing_Percentage',\n        title='Top 10 Columns with Missing Values',\n        xlabel='Missing Percentage (%)',\n        ylabel='Column Name',\n        height=450,\n        width=900,\n        color='#e74c3c',\n        hover_cols=['Missing_Count', 'Missing_Percentage']\n    ).opts(invert_yaxis=True)\n    missing_plot\n\n\n\n11.3.2 Missing Value Imputation Strategy\nWe applied a strategic approach to handle missing data:\n\nDropped columns with more than 50% missing values\nFilled numerical columns with median values to maintain distribution\nFilled categorical columns with “Unknown” for clarity\n\n\n# Strategy: Drop columns with &gt;50% missing values\nthreshold = len(df_cleaned) * 0.5\ncols_before = len(df_cleaned.columns)\ndf_cleaned = df_cleaned.dropna(thresh=threshold, axis=1)\ncols_after = len(df_cleaned.columns)\n\n# Fill numerical columns with median\nnumerical_cols = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\nfor col in numerical_cols:\n    if df_cleaned[col].isnull().sum() &gt; 0:\n        median_val = df_cleaned[col].median()\n        df_cleaned[col].fillna(median_val, inplace=True)\n\n# Fill categorical columns with \"Unknown\"\ncategorical_cols = df_cleaned.select_dtypes(include=['object']).columns.tolist()\nfor col in categorical_cols:\n    if df_cleaned[col].isnull().sum() &gt; 0:\n        df_cleaned[col].fillna(\"Unknown\", inplace=True)"
  },
  {
    "objectID": "final_report.html#step-3-removing-duplicates-1",
    "href": "final_report.html#step-3-removing-duplicates-1",
    "title": "Skill Gap Analysis",
    "section": "11.4 Step 3: Removing Duplicates",
    "text": "11.4 Step 3: Removing Duplicates\n\n11.4.1 Why Remove Duplicates?\nDuplicate job postings skew our analysis. We remove them based on job title, company, and location to ensure each unique position is counted only once.\n\n# Remove duplicates based on key identifiers\ndf_cleaned = df_cleaned.drop_duplicates(\n    subset=['TITLE_NAME', 'COMPANY_NAME', 'LOCATION'], \n    keep='first'\n)"
  },
  {
    "objectID": "final_report.html#step-4-exploratory-data-analysis-eda",
    "href": "final_report.html#step-4-exploratory-data-analysis-eda",
    "title": "Skill Gap Analysis",
    "section": "11.5 Step 4: Exploratory Data Analysis (EDA)",
    "text": "11.5 Step 4: Exploratory Data Analysis (EDA)\n\n11.5.1 Visualization 1: Top Job Titles\nUnderstanding which job titles are most in demand helps job seekers identify the skills and roles to target.\n\n# Get top 10 job titles\ntitle_counts = df_cleaned['TITLE_NAME'].value_counts().head(10)\ntitle_df = pd.DataFrame({'Job Title': title_counts.index, 'Count': title_counts.values})\n\n# Create interactive plot\ntitle_plot = title_df.hvplot.barh(\n    x='Job Title',\n    y='Count',\n    title='Top 10 Job Titles by Postings (2024)',\n    xlabel='Number of Job Postings',\n    ylabel='Job Title',\n    height=500,\n    width=1000,\n    color='#3498db',\n    hover_cols='all'\n).opts(invert_yaxis=True)\n\ntitle_plot\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nThe top job titles show where the job market is most active. This data helps identify roles with highest demand for new talent.\n\n\n\n\n11.5.2 Visualization 2: Job Postings by Employment Type\nEmployment type indicates the nature of positions available in the job market. Understanding the distribution helps job seekers target positions that match their preferences.\n\n# Count jobs by employment type\nemployment_counts = df_cleaned['EMPLOYMENT_TYPE_NAME'].value_counts()\nemployment_df = pd.DataFrame({\n    'Employment Type': employment_counts.index, \n    'Count': employment_counts.values,\n    'Percentage': (employment_counts.values / employment_counts.sum() * 100).round(1)\n})\n\n# Create interactive scatter plot with size\nemployment_plot = employment_df.hvplot.scatter(\n    x='Employment Type',\n    y='Percentage',\n    size='Count',\n    title='Job Market Distribution by Employment Type (2024)',\n    xlabel='Employment Type',\n    ylabel='Percentage of Total Jobs (%)',\n    height=500,\n    width=1000,\n    color='#e74c3c',\n    hover_cols=['Count', 'Percentage'],\n    alpha=0.6,\n    size_max=1000\n)\n\nemployment_plot\n\nWARNING:param.main: size_max option not found for scatter plot with bokeh; similar options include: ['size']\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nThe distribution of employment types shows whether the market is primarily offering full-time, part-time, or contract positions, helping job seekers identify opportunities that match their work preferences.\n\n\n\n\n11.5.3 Visualization 3: Remote Work Distribution\nThe prevalence of remote work reflects post-pandemic hiring trends and work flexibility.\n\n# Count jobs by remote type\nremote_counts = df_cleaned['REMOTE_TYPE_NAME'].value_counts()\nremote_df = pd.DataFrame({\n    'Remote Type': remote_counts.index,\n    'Count': remote_counts.values,\n    'Percentage': (remote_counts.values / remote_counts.sum() * 100).round(1)\n})\n\n# Create interactive bar chart\nremote_plot = remote_df.hvplot.bar(\n    x='Remote Type',\n    y='Count',\n    title='Job Market Distribution: Remote vs. On-Site (2024)',\n    xlabel='Remote Type',\n    ylabel='Number of Job Postings',\n    height=500,\n    width=900,\n    color='#2E8B57',\n    hover_cols=['Count', 'Percentage']\n)\n\nremote_plot\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nThe balance between remote and on-site jobs shows employers’ flexibility preferences. Higher remote job percentages indicate industry acceptance of distributed workforces.\n\n\n\n\n11.5.4 Visualization 4: Top Companies Hiring\nUnderstanding which companies are actively hiring helps job seekers identify potential employers with multiple open positions.\n\n# Get top 10 companies by job postings\ncompany_counts = df_cleaned['COMPANY_NAME'].value_counts().head(10)\ncompany_df = pd.DataFrame({\n    'Company': company_counts.index,\n    'Job Postings': company_counts.values\n})\n\n# Create area chart\ncompany_plot = company_df.hvplot.area(\n    x='Company',\n    y='Job Postings',\n    title='Top 10 Companies by Number of Job Postings (2024)',\n    xlabel='Company Name',\n    ylabel='Number of Job Postings',\n    height=500,\n    width=1000,\n    color='#9b59b6',\n    line_width=2,\n    alpha=0.5,\n    hover_cols='all'\n).opts(xrotation=45)\n\ncompany_plot\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nCompanies with the highest number of job postings indicate organizations that are actively expanding their workforce and may offer more opportunities for candidates."
  },
  {
    "objectID": "final_report.html#summary-of-data-cleaning-analysis",
    "href": "final_report.html#summary-of-data-cleaning-analysis",
    "title": "Skill Gap Analysis",
    "section": "11.6 Summary of Data Cleaning & Analysis",
    "text": "11.6 Summary of Data Cleaning & Analysis\n\n\n\n\n\n\nImportantSummary\n\n\n\nThe data cleaning process successfully prepared the job market dataset for analysis. We removed redundant administrative and deprecated classification columns, handled missing values through strategic imputation, and eliminated duplicate job postings.\nThe exploratory analysis revealed key insights into job market trends, including:\n\nHigh-demand roles: Identification of the most sought-after job titles in 2024\nEmployment stability: Understanding the distribution of full-time, part-time, and contract positions\nWorkplace flexibility: Analysis of remote, hybrid, and on-site work opportunities\nSalary trends: Clear understanding of compensation ranges across different job categories\n\nThese insights provide valuable guidance for job seekers, employers, and market analysts in understanding the current state of the 2024 job market."
  },
  {
    "objectID": "final_report.html#references-2",
    "href": "final_report.html#references-2",
    "title": "Skill Gap Analysis",
    "section": "11.7 References",
    "text": "11.7 References\nAll data sourced from Lightcast Job Postings Dataset (2024). Analysis performed using Python with pandas, hvplot, and holoviews libraries."
  },
  {
    "objectID": "final_report.html#introduction-5",
    "href": "final_report.html#introduction-5",
    "title": "Skill Gap Analysis",
    "section": "12.1 Introduction",
    "text": "12.1 Introduction\nThis section applies Natural Language Processing (NLP) techniques to analyze job postings and extract meaningful insights about skills, requirements, and industry trends. By processing structured skills data, we can uncover patterns about the most in-demand competencies.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set seaborn style\nsns.set_style(\"whitegrid\")\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (8, 5)\n\n# Load data\ndf = pd.read_csv('cleanedjob_postings.csv')\n\nprint(f\"Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n\n# Identify skills columns\nskills_columns = [col for col in df.columns if 'SKILLS_NAME' in col or 'SKILLS' in col]\nprint(f\"Skills columns found: {skills_columns}\")\n\nDataset loaded: 59,220 rows, 56 columns\nSkills columns found: ['SKILLS', 'SKILLS_NAME', 'SPECIALIZED_SKILLS', 'SPECIALIZED_SKILLS_NAME', 'COMMON_SKILLS', 'COMMON_SKILLS_NAME', 'SOFTWARE_SKILLS', 'SOFTWARE_SKILLS_NAME']"
  },
  {
    "objectID": "final_report.html#data-preparation",
    "href": "final_report.html#data-preparation",
    "title": "Skill Gap Analysis",
    "section": "12.2 1. Data Preparation",
    "text": "12.2 1. Data Preparation\n\n12.2.1 1.1 Skills Data Overview\n\n# Combine all skills into a single text field for analysis\ndef combine_skills(row):\n    \"\"\"Combine all skills columns into a single text\"\"\"\n    skills = []\n    for col in ['SKILLS_NAME', 'SPECIALIZED_SKILLS_NAME', 'COMMON_SKILLS_NAME', 'SOFTWARE_SKILLS_NAME']:\n        if col in df.columns and pd.notna(row.get(col)):\n            skills.append(str(row[col]))\n    return ' | '.join(skills)\n\ndf['all_skills'] = df.apply(combine_skills, axis=1)\ndf_skills = df[df['all_skills'].str.len() &gt; 0].copy()\n\nprint(f\"✓ Successfully processed: {len(df_skills):,} job postings with skills\")\nprint(f\"✓ Average skills text length: {df_skills['all_skills'].str.len().mean():.0f} characters\")\n\n✓ Successfully processed: 59,220 job postings with skills\n✓ Average skills text length: 1204 characters"
  },
  {
    "objectID": "final_report.html#keyword-extraction",
    "href": "final_report.html#keyword-extraction",
    "title": "Skill Gap Analysis",
    "section": "12.3 2. Keyword Extraction",
    "text": "12.3 2. Keyword Extraction\n\n12.3.1 2.1 Top 10 Most Common Skills\n\n# Extract most common skill keywords (excluding the extreme outlier)\nif len(df_skills) &gt; 0:\n    all_skills = []\n    \n    for skills_text in df_skills['all_skills']:\n        # Split by pipe separator and clean thoroughly\n        skills = [s.strip() for s in str(skills_text).split('|')]\n        # Remove quotes, brackets, and clean each skill\n        cleaned_skills = []\n        for skill in skills:\n            clean = skill.replace('\"', '').replace(\"'\", \"\").replace('[', '').replace(']', '')\n            clean = clean.replace('(', '').replace(')', '').strip().lower()\n            if len(clean) &gt; 2:  # Only keep skills with more than 2 characters\n                cleaned_skills.append(clean)\n        all_skills.extend(cleaned_skills)\n    \n    # Remove empty strings\n    all_skills = [s for s in all_skills if len(s) &gt; 0]\n    \n    if len(all_skills) &gt; 0:\n        skill_freq = Counter(all_skills)\n        \n        # Get top 11 skills, then exclude the top one if it's an extreme outlier\n        top_skills_raw = skill_freq.most_common(11)\n        \n        # Check if the top skill is an extreme outlier (&gt;10x the second)\n        if len(top_skills_raw) &gt; 1 and top_skills_raw[0][1] &gt; 10 * top_skills_raw[1][1]:\n            print(f\"Note: Excluded extreme outlier '{top_skills_raw[0][0]}' with {top_skills_raw[0][1]:,} occurrences\")\n            top_skills = top_skills_raw[1:11]  # Skip the first, take next 10\n        else:\n            top_skills = top_skills_raw[:10]\n        \n        skills, counts = zip(*top_skills)\n        \n        # Create dataframe for seaborn\n        skill_df = pd.DataFrame({'Skill': skills, 'Frequency': counts})\n        skill_df = skill_df.sort_values('Frequency')\n        \n        # Clean up skill names - remove quotes and brackets, capitalize properly\n        skill_df['Skill'] = skill_df['Skill'].str.replace('\"', '').str.replace(\"'\", \"\")\n        skill_df['Skill'] = skill_df['Skill'].str.replace(r'[\\[\\]\\(\\)]', '', regex=True)\n        skill_df['Skill'] = skill_df['Skill'].str.strip()\n        \n        # Split combined labels - if comma-separated, take first item only\n        skill_df['Skill'] = skill_df['Skill'].apply(lambda x: x.split(',')[0].strip() if ',' in x else x)\n        skill_df['Skill'] = skill_df['Skill'].str.title()\n        \n        fig, ax = plt.subplots(figsize=(8, 5))\n        bars = sns.barplot(data=skill_df, y='Skill', x='Frequency', palette='Blues_r', ax=ax)\n        ax.set_title('Top 10 Most Common Skills in Job Postings', fontsize=14, fontweight='bold', pad=15)\n        ax.set_xlabel('Frequency', fontsize=11, fontweight='bold')\n        ax.set_ylabel('Skill', fontsize=11, fontweight='bold')\n        \n        # Add value labels\n        for i, v in enumerate(skill_df['Frequency']):\n            ax.text(v + max(skill_df['Frequency'])*0.02, i, f'{int(v):,}', \n                   va='center', fontweight='bold', fontsize=9)\n        \n        # Improve layout\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(\"No skills found\")\n\n\n\n\n\n\n\n\nTop 10 Skills Insight:\nSAP Applications dominates with 1,114 postings, reflecting the strong demand for enterprise resource planning expertise in large organizations. Oracle Cloud (667) and Microsoft Office (637) follow, showing that cloud platforms and productivity tools remain essential. Communication (532) appears as the top soft skill, emphasizing that technical roles require strong interpersonal abilities. The data reveals a mix of enterprise software (SAP, Oracle Cloud), business analysis capabilities (Data Analysis, Dashboard, Project Management), and emerging areas like Cyber Security and UX Design. Job seekers should prioritize SAP expertise if targeting enterprise roles, while building a foundation in Microsoft Office, data analysis, and communication skills for broader market appeal."
  },
  {
    "objectID": "final_report.html#technical-skills-analysis",
    "href": "final_report.html#technical-skills-analysis",
    "title": "Skill Gap Analysis",
    "section": "12.4 3. Technical Skills Analysis",
    "text": "12.4 3. Technical Skills Analysis\n\n12.4.1 3.1 Top 10 Software & Technical Skills\n\n# Focus on software/technical skills column\nif 'SOFTWARE_SKILLS_NAME' in df.columns:\n    software_skills = []\n    \n    for skills_text in df['SOFTWARE_SKILLS_NAME'].dropna():\n        skills = [s.strip() for s in str(skills_text).split('|')]\n        # Clean each skill thoroughly\n        for skill in skills:\n            clean = skill.replace('\"', '').replace(\"'\", \"\").replace('[', '').replace(']', '')\n            clean = clean.replace('(', '').replace(')', '').strip().lower()\n            if len(clean) &gt; 2:\n                software_skills.append(clean)\n    \n    software_skills = [s for s in software_skills if len(s) &gt; 0]\n    \n    if len(software_skills) &gt; 0:\n        software_freq = Counter(software_skills)\n        \n        # Get top 11, check for outlier\n        top_software_raw = software_freq.most_common(11)\n        \n        if len(top_software_raw) &gt; 1 and top_software_raw[0][1] &gt; 10 * top_software_raw[1][1]:\n            print(f\"Note: Excluded extreme outlier '{top_software_raw[0][0]}' with {top_software_raw[0][1]:,} occurrences\")\n            top_software = top_software_raw[1:11]\n        else:\n            top_software = top_software_raw[:10]\n        \n        software, counts = zip(*top_software)\n        \n        software_df = pd.DataFrame({'Software': software, 'Count': counts})\n        software_df = software_df.sort_values('Count')\n        \n        # Clean software names - remove quotes, brackets, parentheses\n        software_df['Software'] = software_df['Software'].str.replace('\"', '').str.replace(\"'\", \"\")\n        software_df['Software'] = software_df['Software'].str.replace(r'[\\[\\]\\(\\)]', '', regex=True)\n        software_df['Software'] = software_df['Software'].str.strip()\n        \n        # Split combined labels - if comma-separated, take first item only\n        software_df['Software'] = software_df['Software'].apply(lambda x: x.split(',')[0].strip() if ',' in x else x)\n        software_df['Software'] = software_df['Software'].str.title()\n        \n        fig, ax = plt.subplots(figsize=(8, 5))\n        sns.barplot(data=software_df, y='Software', x='Count', palette='viridis_r', ax=ax)\n        ax.set_title('Top 10 Software Skills in Job Postings', fontsize=14, fontweight='bold', pad=15)\n        ax.set_xlabel('Number of Job Postings', fontsize=11, fontweight='bold')\n        ax.set_ylabel('Software Skill', fontsize=11, fontweight='bold')\n        \n        # Add value labels at the end of each bar\n        max_val = software_df['Count'].max()\n        for i, v in enumerate(software_df['Count']):\n            ax.text(v + max_val*0.01, i, f'{int(v):,}', \n                   va='center', fontweight='bold', fontsize=9)\n        \n        # Set x-axis limit to prevent cutoff\n        ax.set_xlim(0, max_val * 1.15)\n        \n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        plt.tight_layout()\n        plt.show()\n\n\n\n\n\n\n\n\nTop 10 Software Skills - Job Seeker Insights:\nSAP Applications leads dramatically with 934 postings, making it the single most valuable software skill for enterprise-focused careers. Oracle Cloud (633) and Microsoft Office (602) demonstrate the importance of cloud infrastructure and productivity suites. Dashboard skills (433) reflect the growing need for data visualization capabilities across all roles. SQL Programming Language (353) and Microsoft Excel (282) remain fundamental for data manipulation and analysis. The presence of specialized tools like Anaplan (196), Onestream CPM Software (196), and Oracle E-Business Suite (173) indicates niche opportunities in financial planning and enterprise systems. Job seekers should master SAP for enterprise roles, Excel and SQL for data work, and consider specializing in emerging tools like Anaplan for competitive advantage."
  },
  {
    "objectID": "final_report.html#specialized-skills-analysis",
    "href": "final_report.html#specialized-skills-analysis",
    "title": "Skill Gap Analysis",
    "section": "12.5 4. Specialized Skills Analysis",
    "text": "12.5 4. Specialized Skills Analysis\n\n12.5.1 4.1 Top 10 Specialized Skills\n\n# Focus on specialized skills column\nif 'SPECIALIZED_SKILLS_NAME' in df.columns:\n    specialized_skills = []\n    \n    for skills_text in df['SPECIALIZED_SKILLS_NAME'].dropna():\n        skills = [s.strip() for s in str(skills_text).split('|')]\n        # Clean each skill thoroughly\n        for skill in skills:\n            clean = skill.replace('\"', '').replace(\"'\", \"\").replace('[', '').replace(']', '')\n            clean = clean.replace('(', '').replace(')', '').strip().lower()\n            if len(clean) &gt; 2:\n                specialized_skills.append(clean)\n    \n    specialized_skills = [s for s in specialized_skills if len(s) &gt; 0]\n    \n    if len(specialized_skills) &gt; 0:\n        spec_freq = Counter(specialized_skills)\n        top_spec = spec_freq.most_common(10)\n        \n        spec, counts = zip(*top_spec)\n        \n        spec_df = pd.DataFrame({'Skill': spec, 'Count': counts})\n        spec_df = spec_df.sort_values('Count')\n        \n        # Clean skill names - remove quotes, brackets, parentheses\n        spec_df['Skill'] = spec_df['Skill'].str.replace('\"', '').str.replace(\"'\", \"\")\n        spec_df['Skill'] = spec_df['Skill'].str.replace(r'[\\[\\]\\(\\)]', '', regex=True)\n        spec_df['Skill'] = spec_df['Skill'].str.strip()\n        \n        # Split combined labels - if comma-separated, take first item only\n        spec_df['Skill'] = spec_df['Skill'].apply(lambda x: x.split(',')[0].strip() if ',' in x else x)\n        \n        # Truncate long skill names to prevent overlap\n        spec_df['Skill'] = spec_df['Skill'].apply(lambda x: x[:35] + '...' if len(x) &gt; 35 else x)\n        spec_df['Skill'] = spec_df['Skill'].str.title()\n        \n        fig, ax = plt.subplots(figsize=(8, 6))\n        sns.barplot(data=spec_df, y='Skill', x='Count', palette='rocket_r', ax=ax)\n        ax.set_title('Top 10 Specialized Skills in Job Postings', fontsize=14, fontweight='bold', pad=15)\n        ax.set_xlabel('Number of Job Postings', fontsize=11, fontweight='bold')\n        ax.set_ylabel('Specialized Skill', fontsize=11, fontweight='bold')\n        \n        # Add value labels\n        max_val = spec_df['Count'].max()\n        for i, v in enumerate(spec_df['Count']):\n            ax.text(v + max_val*0.01, i, f'{int(v):,}', \n                   va='center', fontweight='bold', fontsize=9)\n        \n        # Set x-axis limit\n        ax.set_xlim(0, max_val * 1.15)\n        \n        # Adjust y-axis labels to prevent overlap\n        ax.tick_params(axis='y', labelsize=10)\n        plt.yticks(rotation=0)\n        \n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        plt.tight_layout()\n        plt.show()\n\n\n\n\n\n\n\n\nTop 10 Specialized Skills - Job Seeker Insights:\nUser Experience (UX) Design leads with 260 postings, highlighting the critical importance of user-centered design in modern product development. Data Analysis (220) and Cloud Computing (196) show strong demand for analytical and cloud architecture expertise. Emergency Response (143) and Pivot Tables and Charts (125) represent specialized operational and analytical capabilities. SAP Applications (104) appears again, reinforcing its enterprise value. Microsoft Access (89), Databricks (85), Mulesoft (85), and Sales Process (74) round out the list, showing diverse specializations from database management to integration platforms and CRM. Job seekers should prioritize UX design for product roles, data analysis for analytical positions, and cloud computing for infrastructure careers, while considering niche specializations like Databricks or Mulesoft for premium positioning."
  },
  {
    "objectID": "final_report.html#skill-requirements-by-job-title",
    "href": "final_report.html#skill-requirements-by-job-title",
    "title": "Skill Gap Analysis",
    "section": "12.6 5. Skill Requirements by Job Title",
    "text": "12.6 5. Skill Requirements by Job Title\n\n12.6.1 5.1 Skills vs Top Job Titles - Heatmap\n\n# Cross-tabulate skills by top job titles\nif 'TITLE_NAME' in df.columns and 'SOFTWARE_SKILLS_NAME' in df.columns:\n    print(\"Creating heatmap...\")\n    \n    # Get top 8 job titles\n    top_titles = df['TITLE_NAME'].value_counts().head(8).index\n    print(f\"Top job titles: {list(top_titles)}\")\n    \n    # Get top 8 software skills (excluding extreme outlier and empty strings)\n    all_software = []\n    for skills_text in df['SOFTWARE_SKILLS_NAME'].dropna():\n        skills = [s.strip() for s in str(skills_text).split('|')]\n        for skill in skills:\n            # Clean thoroughly\n            clean = skill.replace('\"', '').replace(\"'\", \"\").replace('[', '').replace(']', '')\n            clean = clean.replace('(', '').replace(')', '').strip().lower()\n            if len(clean) &gt; 2:\n                all_software.append(clean)\n    \n    software_freq = Counter(all_software)\n    top_software_list = software_freq.most_common(15)\n    \n    # Filter out problematic entries\n    filtered_software = []\n    for skill, count in top_software_list:\n        # Skip if contains 'cpm', 'onestream', or is too short\n        if 'cpm' not in skill.lower() and 'onestream' not in skill.lower() and len(skill) &gt; 2:\n            filtered_software.append((skill, count))\n    \n    # Check for extreme outlier\n    if len(filtered_software) &gt; 1 and filtered_software[0][1] &gt; 10 * filtered_software[1][1]:\n        top_8_software = [s[0] for s in filtered_software[1:9]]\n    else:\n        top_8_software = [s[0] for s in filtered_software[:8]]\n    \n    print(f\"Top software skills: {top_8_software}\")\n    \n    # Build matrix\n    skill_title_matrix = []\n    \n    for title in top_titles:\n        title_data = df[df['TITLE_NAME'] == title].copy()\n        skill_row = {'Title': title}\n        \n        for skill in top_8_software:\n            count = 0\n            for skills_text in title_data['SOFTWARE_SKILLS_NAME'].dropna():\n                if skill in str(skills_text).lower():\n                    count += 1\n            # Clean skill name for column header - remove all special characters\n            clean_skill = skill.replace('\"', '').replace(\"'\", \"\").replace('[', '').replace(']', '')\n            clean_skill = clean_skill.replace('(', '').replace(')', '').replace(',', '').strip()\n            # Take only first part if multiple words separated by comma\n            if ',' in clean_skill or len(clean_skill) &gt; 20:\n                clean_skill = clean_skill.split(',')[0].split()[0:3]  # First 3 words max\n                clean_skill = ' '.join(clean_skill)\n            clean_skill = clean_skill.title()[:20]  # Max 20 chars\n            skill_row[clean_skill] = (count / len(title_data) * 100) if len(title_data) &gt; 0 else 0\n        \n        skill_title_matrix.append(skill_row)\n    \n    if skill_title_matrix:\n        print(f\"Matrix created with {len(skill_title_matrix)} rows\")\n        skill_matrix_df = pd.DataFrame(skill_title_matrix)\n        skill_matrix_df = skill_matrix_df.set_index('Title')\n        \n        print(f\"Dataframe shape: {skill_matrix_df.shape}\")\n        print(f\"Columns: {list(skill_matrix_df.columns)}\")\n        \n        # Shorten job titles if too long\n        skill_matrix_df.index = [title[:25] + '...' if len(title) &gt; 25 else title \n                                  for title in skill_matrix_df.index]\n        \n        fig, ax = plt.subplots(figsize=(8, 4))\n        sns.heatmap(skill_matrix_df, annot=True, fmt='.1f', cmap='YlOrRd', \n                    cbar_kws={'label': '% of Postings'}, \n                    ax=ax, linewidths=0.5, linecolor='white', annot_kws={'size': 7.5})\n        ax.set_title('Software Skill Requirements by Job Title', \n                     fontsize=12, fontweight='bold', pad=10)\n        ax.set_xlabel('Software Skill', fontsize=10, fontweight='bold')\n        ax.set_ylabel('Job Title', fontsize=10, fontweight='bold')\n        plt.xticks(rotation=45, ha='right', fontsize=8)\n        plt.yticks(rotation=0, fontsize=8)\n        \n        plt.tight_layout()\n        plt.show()\n        print(\"Heatmap displayed successfully!\")\n    else:\n        print(\"ERROR: No skill matrix data created\")\nelse:\n    print(\"ERROR: Required columns not found\")\n\nCreating heatmap...\nTop job titles: ['Data Analysts', 'Unclassified', 'Business Intelligence Analysts', 'Enterprise Architects', 'Data Modelers', 'Data Governance Analysts', 'Oracle Cloud HCM Consultants', 'Solutions Architects']\nTop software skills: ['sap applications', 'oracle cloud', 'microsoft office', 'dashboard', 'sql programming language', 'microsoft excel', 'microsoft powerpoint,\\n  microsoft excel', 'oracle e-business suite']\nMatrix created with 8 rows\nDataframe shape: (8, 8)\nColumns: ['Sap Applications', 'Oracle Cloud', 'Microsoft Office', 'Dashboard', 'Sql Programming Lang', 'Microsoft Excel', 'Microsoft Powerpoint', 'Oracle E-Business Su']\n\n\n\n\n\n\n\n\n\nHeatmap displayed successfully!\n\n\nSkills vs Job Titles - Job Seeker Insights:\nThe heatmap reveals distinct skill patterns across job roles. Unclassified positions show the highest demand for general software skills (37.2% require SAP Applications), suggesting broad technical requirements. Oracle Cloud HCM Consultants have specialized focus with 11.1% requiring Oracle Cloud expertise. Data Governance Analysts need the most diverse skillset with strong SQL Programming Language (13.8%) and moderate requirements across Dashboard, Microsoft Office, and SAP. Enterprise Architects and Solutions Architects show concentrated needs in SAP (10.5% and 8.5% respectively) with minimal other software requirements. Data Modelers uniquely emphasize SQL Programming (2.1%) over other tools. The low percentages overall indicate that most roles don’t mandate specific software, creating opportunities for candidates to differentiate through technical mastery. Job seekers should target SAP for enterprise roles, Oracle Cloud for HCM consulting, and SQL for data-focused positions."
  },
  {
    "objectID": "final_report.html#summary-2",
    "href": "final_report.html#summary-2",
    "title": "Skill Gap Analysis",
    "section": "12.7 Summary",
    "text": "12.7 Summary\n\n\n\n\n\n\nImportantNLP Analysis Key Findings\n\n\n\nSkills Analysis Results:\n\n✓ Successfully analyzed skills from structured data columns\n✓ Identified top 10 most common skills across all job postings\n✓ Extracted and ranked top 10 software/technical skills\n✓ Analyzed top 10 specialized skills for advanced roles\n✓ Cross-referenced skills with job title requirements via heatmap\n\nKey Insights:\n\nSpecific skills dominate the job market across different categories\nSoftware skills are critical technical differentiators\nSpecialized skills offer opportunities for premium positioning\nSkills requirements vary significantly by job title\n\nRecommendations for Job Seekers:\n\nPrioritize learning the top 10 skills identified in each category\nDevelop proficiency in the most demanded software tools\nBuild specialized skills for senior or expert-level opportunities\nFocus on developing complementary skills that frequently co-occur\nUse the heatmap to understand skill priorities for your target job titles\nTailor your resume to highlight relevant skill combinations"
  },
  {
    "objectID": "final_report.html#references-3",
    "href": "final_report.html#references-3",
    "title": "Skill Gap Analysis",
    "section": "12.8 References",
    "text": "12.8 References\n\nPandas Documentation: https://pandas.pydata.org/\nSeaborn Documentation: https://seaborn.pydata.org/\nCollections Counter: https://docs.python.org/3/library/collections.html#collections.Counter"
  },
  {
    "objectID": "final_report.html#introduction-6",
    "href": "final_report.html#introduction-6",
    "title": "Skill Gap Analysis",
    "section": "13.1 Introduction",
    "text": "13.1 Introduction\nThis analysis compares our team’s current skill levels against the skills demanded in the IT job market. By identifying gaps between our capabilities and market requirements, we can develop targeted learning strategies to enhance our competitiveness as job candidates.\n\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport panel as pn\nfrom collections import Counter\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Enable panel for rendering\npn.extension()\n\n# Color palette\nCOLORS = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c']\n\n# Load cleaned job data\ndf = pd.read_csv('cleanedjob_postings.csv')\nprint(f\"Analyzing {len(df):,} job postings for skill requirements\")\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nAnalyzing 59,220 job postings for skill requirements"
  },
  {
    "objectID": "final_report.html#team-skill-assessment",
    "href": "final_report.html#team-skill-assessment",
    "title": "Skill Gap Analysis",
    "section": "13.2 1. Team Skill Assessment",
    "text": "13.2 1. Team Skill Assessment\n\n13.2.1 1.1 Creating Team Skills Profile\nEach team member rates their proficiency in key IT skills on a scale of 1-5: - 1 = Beginner (aware of the skill) - 2 = Basic knowledge (can perform simple tasks) - 3 = Intermediate (comfortable with common scenarios) - 4 = Advanced (can handle complex problems) - 5 = Expert (can teach others and solve any problem)\n\n# Team member skill assessments\n# TODO: Replace with actual team member names and skill ratings\nskills_data = {\n    \"Name\": [\"Tuba Anwar\", \"Kriti Singh\", \"Soham Deshkhaire\"],\n    \"Python\": [4, 3, 4],\n    \"SQL\": [3, 4, 3],\n    \"Machine Learning\": [3, 2, 4],\n    \"Data Analysis\": [4, 4, 3],\n    \"Cloud Computing\": [2, 2, 3],\n    \"Java\": [3, 2, 2],\n    \"JavaScript\": [2, 3, 3],\n    \"R\": [3, 3, 2],\n    \"Tableau\": [3, 4, 2],\n    \"Excel\": [4, 5, 3],\n    \"AWS\": [2, 1, 2],\n    \"Azure\": [1, 2, 2],\n    \"Docker\": [2, 1, 3],\n    \"Git\": [3, 3, 4],\n    \"Power BI\": [2, 3, 2],\n    \"Spark\": [2, 2, 3],\n    \"TensorFlow\": [2, 1, 3],\n    \"NLP\": [3, 2, 3]\n}\n\ndf_team_skills = pd.DataFrame(skills_data)\ndf_team_skills.set_index(\"Name\", inplace=True)\n\n# Display team skills\n#print(\"Team Skills Profile:\")\n#print(df_team_skills)\n\n# Calculate team averages\nteam_avg = df_team_skills.mean().sort_values(ascending=False)\n#print(f\"\\nTeam Average Skills (sorted):\")\n#print(team_avg)\n\nThe team’s strongest skills are *Excel, Python, and Data Analysis, showing solid readiness for analytical and data-focused roles. Skills like **SQL, Git, Tableau, and Machine Learning* are moderately strong, indicating reliable but improvable proficiency. Meanwhile, areas such as AWS, Azure, TensorFlow, Docker, and Cloud Computing show lower scores, highlighting clear opportunities for growth—especially important given rising market demand for cloud and ML engineering skills. Overall, the team has a strong analytical foundation but should focus on boosting cloud and advanced technical competencies to stay competitive.\n\n\n13.2.2 1.2 Team Skills Heatmap\nVisualize each team member’s strengths and weaknesses across all skills.\n\n# Create heatmap using hvPlot\nheatmap = df_team_skills.T.hvplot.heatmap(\n    title='Team Skill Proficiency Heatmap',\n    cmap='RdYlGn',\n    height=700,\n    width=800,\n    xlabel='Team Member',\n    ylabel='Skill',\n    clabel='Proficiency Level',\n    rot=0\n)\nheatmap\n\n\n\n\n\n  \n\n\n\n\nThis heatmap shows how strong each team member is across key tech skills like AWS, Excel, SQL, Machine Learning, and more. The greener the box, the stronger the skill—red means the weakest. From a job seeker’s perspective, this quickly highlights which skills are common strengths and which ones need improvement to stay competitive. For example, Excel, Data Analysis, and SQL show strong proficiency across the team, meaning these are essential, in-demand skills worth mastering. Meanwhile, AWS and JavaScript show weaker proficiency, signaling great opportunities for upskilling—especially since cloud and coding skills are highly valued in today’s market.\n\n\n13.2.3 1.3 Team Average Skills Bar Chart\nCompare team average proficiency across all skills.\n\n# Prepare data for bar chart\nteam_avg_df = team_avg.reset_index()\nteam_avg_df.columns = ['Skill', 'Average Proficiency']\n\n# Create bar chart\nchart = team_avg_df.sort_values('Average Proficiency', ascending=True).hvplot.barh(\n    x='Skill',\n    y='Average Proficiency',\n    title='Team Average Skill Proficiency',\n    height=700,\n    width=900,\n    color='#3498db',\n    xlabel='Average Proficiency (1-5)',\n    ylabel='',\n    flip_yaxis=True\n)\nchart\n\n\n\n\n\n  \n\n\n\n\nThe team’s top 10 skills show strong proficiency in high-value areas like *Excel, Data Analysis, Python, Git, and SQL, which are core requirements for most analytics and tech roles. Mid-level strengths in **Machine Learning, Tableau, R, and JavaScript* show the team can handle more advanced tasks but still has room to grow. Overall, these skills place the team in a competitive position for data and tech jobs, while highlighting opportunities to strengthen cloud and AI-related tools for even better market readiness."
  },
  {
    "objectID": "final_report.html#market-skill-demand-analysis",
    "href": "final_report.html#market-skill-demand-analysis",
    "title": "Skill Gap Analysis",
    "section": "13.3 2. Market Skill Demand Analysis",
    "text": "13.3 2. Market Skill Demand Analysis\n\n13.3.1 2.1 Extracting Skills from Job Descriptions\nWe analyze job postings to identify the most in-demand skills in the IT job market.\n\n# Define comprehensive skill keywords to search for\nskill_keywords = {\n    'Python': ['python', 'python3', 'py'],\n    'SQL': ['sql', 'mysql', 'postgresql', 'sql server', 'oracle sql'],\n    'Machine Learning': ['machine learning', 'ml', 'deep learning', 'neural network'],\n    'Data Analysis': ['data analysis', 'data analytics', 'analytical'],\n    'Cloud Computing': ['cloud', 'cloud computing', 'cloud services'],\n    'Java': ['java', 'java8', 'java 8'],\n    'JavaScript': ['javascript', 'js', 'node.js', 'nodejs'],\n    'R': [' r ', 'r programming', 'rstudio'],\n    'Tableau': ['tableau'],\n    'Excel': ['excel', 'microsoft excel', 'advanced excel'],\n    'AWS': ['aws', 'amazon web services', 'ec2', 's3'],\n    'Azure': ['azure', 'microsoft azure'],\n    'Docker': ['docker', 'containerization'],\n    'Git': ['git', 'github', 'version control'],\n    'Power BI': ['power bi', 'powerbi'],\n    'Spark': ['spark', 'apache spark', 'pyspark'],\n    'TensorFlow': ['tensorflow', 'tf'],\n    'NLP': ['nlp', 'natural language processing', 'text mining']\n}\n\n# Function to extract skills from text\ndef extract_skills(text):\n    if pd.isna(text):\n        return []\n    text = str(text).lower()\n    found_skills = []\n    for skill, keywords in skill_keywords.items():\n        for keyword in keywords:\n            if keyword in text:\n                found_skills.append(skill)\n                break\n    return found_skills\n\n# Extract skills from job titles and descriptions (if available)\nif 'TITLE_NAME' in df.columns:\n    df['extracted_skills'] = df['TITLE_NAME'].apply(extract_skills)\n    \n    # If job description available, combine with title\n    if 'DESCRIPTION' in df.columns or 'JOB_DESCRIPTION' in df.columns:\n        desc_col = 'DESCRIPTION' if 'DESCRIPTION' in df.columns else 'JOB_DESCRIPTION'\n        df['extracted_skills'] = df.apply(\n            lambda row: list(set(extract_skills(row['TITLE_NAME']) + extract_skills(row[desc_col]))),\n            axis=1\n        )\n\n# Count skill occurrences\nall_skills = [skill for skills_list in df['extracted_skills'] for skill in skills_list]\nskill_counts = Counter(all_skills)\nmarket_skills_df = pd.DataFrame(skill_counts.items(), columns=['Skill', 'Job Postings'])\nmarket_skills_df = market_skills_df.sort_values('Job Postings', ascending=False)\nmarket_skills_df['Percentage'] = (market_skills_df['Job Postings'] / len(df) * 100).round(2)\n\n#print(\"Market Skill Demand (Top Skills):\")\n#print(market_skills_df.head(10))\n\nThe skill extraction shows that Data Analysis, Cloud Computing, and TensorFlow are the most frequently requested skills in job postings, making them top priorities for job seekers. Technical fundamentals like Git, SQL, and Azure also appear often, highlighting the importance of both data-related and cloud skills. Lower-frequency skills such as Spark or AWS still matter but are requested less often, suggesting they may serve as valuable “bonus” skills rather than core requirements.\n\n\n13.3.2 2.2 Top In-Demand Skills Visualization\n\n# Create bar chart of top 10 market skills\ntop_market_skills = market_skills_df.head(10).sort_values('Job Postings', ascending=True)\n\nchart = top_market_skills.hvplot.barh(\n    x='Skill',\n    y='Job Postings',\n    title='Top 10 Most In-Demand Skills in Job Market (2024)',\n    height=600,\n    width=900,\n    color='#3498db',\n    hover_cols=['Percentage'],\n    xlabel='Number of Job Postings Requiring Skill',\n    ylabel='',\n    flip_yaxis=True\n)\nchart\n\n\n\n\n\n  \n\n\n\n\nThe chart shows that Data Analysis and Cloud Computing are the most in-demand skills in the 2024 job market, with far more postings than any other skill. This means employers are strongly prioritizing candidates who can analyze data and work with cloud platforms. Skills like *TensorFlow, Git, SQL, Azure, and **Java* are also highly requested, making them great additions to your skillset. Lower-demand skills such as *Spark, AWS, Tableau, and **Machine Learning* still matter, but they won’t give as much of a competitive edge as the top skills."
  },
  {
    "objectID": "final_report.html#skill-gap-identification",
    "href": "final_report.html#skill-gap-identification",
    "title": "Skill Gap Analysis",
    "section": "13.4 3. Skill Gap Identification",
    "text": "13.4 3. Skill Gap Identification\n\n13.4.1 3.1 Comparing Team Skills to Market Demand\nNow we identify gaps between our team’s capabilities and what the market requires.\n\n# Normalize market demand to 1-5 scale for comparison\n# We'll scale based on percentage of jobs requiring each skill\nmax_percentage = market_skills_df['Percentage'].max()\n\n# Create comparison dataframe\ncomparison_data = []\n\nfor skill in df_team_skills.columns:\n    team_avg_skill = team_avg[skill]\n    \n    # Get market demand (normalized to 1-5 scale)\n    if skill in market_skills_df['Skill'].values:\n        market_pct = market_skills_df[market_skills_df['Skill'] == skill]['Percentage'].values[0]\n        market_demand = (market_pct / max_percentage) * 5  # Scale to 1-5\n    else:\n        market_demand = 0\n    \n    gap = market_demand - team_avg_skill\n    \n    comparison_data.append({\n        'Skill': skill,\n        'Team Average': team_avg_skill,\n        'Market Demand': market_demand,\n        'Gap': gap,\n        'Gap_Category': 'Strength' if gap &lt;= 0 else ('Moderate Gap' if gap &lt;= 2 else 'Critical Gap')\n    })\n\ncomparison_df = pd.DataFrame(comparison_data)\ncomparison_df = comparison_df.sort_values('Gap', ascending=False)\n\n#print(\"Skill Gap Analysis:\")\n#print(comparison_df)\n\nThe analysis shows that our team is *strong in most core skills, especially Python, SQL, Tableau, Machine Learning, and Git — all areas where market demand is relatively low to moderate. However, there are **two skills with clear gaps* compared to what employers want:\n\nCloud Computing (e.g., AWS, Azure)\nData Analysis\n\nThese areas are in very high demand, but our team’s proficiency is lower than what the job market expects. Strengthening Cloud tools and advanced Data Analysis techniques would significantly boost job readiness and competitiveness in today’s tech hiring landscape.\n\n\n13.4.2 3.2 Skill Gap Visualization\n\n# Create comparison dataframe for visualization\ncomparison_melted = comparison_df.melt(\n    id_vars='Skill',\n    value_vars=['Team Average', 'Market Demand'],\n    var_name='Metric',\n    value_name='Level'\n)\n\n# Sort by Market Demand\nskill_order = comparison_df.sort_values('Market Demand', ascending=False)['Skill'].tolist()\n\n# Create grouped bar chart\nchart = comparison_melted.hvplot.bar(\n    x='Skill',\n    y='Level',\n    by='Metric',\n    title='Team Skills vs Market Demand Comparison',\n    height=600,\n    width=1000,\n    ylabel='Proficiency / Demand Level (1-5)',\n    xlabel='Skill',\n    legend='top_right',\n    rot=45\n)\nchart\n\n\n\n\n\n  \n\n\n\n\nThe chart clearly shows that market demand is much higher than the team’s current skill levels across nearly all skills. The biggest gaps appear in Cloud Computing, Data Analysis, TensorFlow, Azure, and AWS, meaning these are the most urgent areas for development. Skills like Git, SQL, Python, and Machine Learning are team strengths, but still trail behind what employers expect.\nFor job seekers like us, this means focusing on cloud technologies, advanced analytics, and AI frameworks will significantly boost competitiveness and align better with real market needs.\n\n\n13.4.3 3.3 Skill Gap Priority Matrix\nIdentify which skills need immediate attention based on gap size.\n\n# Create scatter plot: Team proficiency vs Market demand\nchart = comparison_df.hvplot.scatter(\n    x='Team Average',\n    y='Market Demand',\n    by='Gap_Category',\n    size=abs(comparison_df['Gap']) * 50,\n    hover_cols=['Skill', 'Gap'],\n    title='Skill Gap Priority Matrix',\n    height=700,\n    width=900,\n    xlabel='Team Average Proficiency (1-5)',\n    ylabel='Market Demand Level (1-5 normalized)',\n    legend='top_left',\n    color=['#2ecc71', '#f39c12', '#e74c3c']\n)\nchart\n\n\n\n\n\n  \n\n\n\n\nThe matrix helps identify which skills your team should focus on first.\n\nGreen (Moderate Gap): These skills—like Cloud Computing, Data Analysis, and TensorFlow—are high in market demand but lower in team proficiency, meaning they should be top priority for upskilling.\nOrange (Strength): Most other skills fall in this category. These are areas where the team is already strong compared to market demand—great to maintain but not urgent for improvement."
  },
  {
    "objectID": "final_report.html#individual-skill-gap-analysis",
    "href": "final_report.html#individual-skill-gap-analysis",
    "title": "Skill Gap Analysis",
    "section": "13.5 4. Individual Skill Gap Analysis",
    "text": "13.5 4. Individual Skill Gap Analysis\n\n13.5.1 4.1 Skill Gaps by Team Member\nIdentify personalized skill development needs for each team member.\n\n# Calculate individual gaps\nindividual_gaps = []\n\nfor member in df_team_skills.index:\n    for skill in df_team_skills.columns:\n        member_skill = df_team_skills.loc[member, skill]\n        \n        # Get market demand\n        if skill in market_skills_df['Skill'].values:\n            market_pct = market_skills_df[market_skills_df['Skill'] == skill]['Percentage'].values[0]\n            market_demand = (market_pct / max_percentage) * 5\n        else:\n            market_demand = 0\n        \n        gap = market_demand - member_skill\n        \n        if gap &gt; 0:  # Only include gaps (areas for improvement)\n            individual_gaps.append({\n                'Member': member,\n                'Skill': skill,\n                'Current Level': member_skill,\n                'Market Demand': market_demand,\n                'Gap': gap\n            })\n\nindividual_gaps_df = pd.DataFrame(individual_gaps)\nindividual_gaps_df = individual_gaps_df.sort_values(['Member', 'Gap'], ascending=[True, False])\n\n# Show top 5 gaps per member\n#print(\"Top 5 Skill Gaps per Team Member:\")\nfor member in df_team_skills.index:\n    #print(f\"\\n{member}:\")\n    member_gaps = individual_gaps_df[individual_gaps_df['Member'] == member].head(5)\n    #print(member_gaps[['Skill', 'Current Level', 'Gap']].to_string(index=False))\n\nach team member has two main skill gaps: Cloud Computing and Data Analysis.\nTuba Anwar needs improvement mainly in Cloud Computing, with a smaller gap in Data Analysis.\nKriti Singh also shows the same pattern—Cloud Computing is the biggest gap, followed by Data Analysis.\nSoham Deshkhaires has the highest gap in Data Analysis, and a moderate gap in Cloud Computing.\nOverall Insight: All three team members share the same critical areas for improvement. Strengthening Cloud Computing and Data Analysis should be the top priority for the team.\n\n\n13.5.2 4.2 Individual Gap Visualization\n\n# Get top 5 gaps per member\ntop_individual_gaps = individual_gaps_df.groupby('Member').head(5)\n\n# Create bar chart for each member\nfor member in df_team_skills.index:\n    member_data = top_individual_gaps[top_individual_gaps['Member'] == member].sort_values('Gap', ascending=True)\n    \n    plot = member_data.hvplot.barh(\n        x='Skill',\n        y='Gap',\n        title=f'Top 5 Skill Gaps - {member}',\n        height=400,\n        width=700,\n        color='#e74c3c',\n        xlabel='Skill Gap (Market - Current)',\n        ylabel='',\n        flip_yaxis=True\n    )\n    \n    display(plot)\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n  \n\n\n\n\nAcross all three members—Tuba, Kriti, and Soham—the largest skill gap is in Cloud Computing, meaning the market demands this skill at a much higher level than the team currently possesses. This makes Cloud Computing the top priority area for improvement for everyone.\nAdditionally, Data Analysis appears as a major gap for both Tuba and Kriti, while Soham shows a moderate gap in Cloud Computing only. This indicates that although the team already has some analytical skills, the job market expects a stronger command in this area.\nThe charts highlight two urgent development needs for the team:\nCloud Computing → biggest gap for all members\nData Analysis → second-highest gap for Tuba & Kriti\nFocusing training efforts on these skills will significantly improve alignment with market expectations.individual"
  },
  {
    "objectID": "final_report.html#skill-development-plan",
    "href": "final_report.html#skill-development-plan",
    "title": "Skill Gap Analysis",
    "section": "13.6 5. Skill Development Plan",
    "text": "13.6 5. Skill Development Plan\n\n13.6.1 5.1 Priority Skills for Team Development\nBased on our analysis, here are the priority skills the team should focus on:\n\n# Identify critical gaps (gap &gt; 2)\ncritical_gaps = comparison_df[comparison_df['Gap'] &gt; 2].sort_values('Gap', ascending=False)\n\n#print(\"CRITICAL SKILLS TO DEVELOP (Gap &gt; 2):\")\n#print(critical_gaps[['Skill', 'Team Average', 'Market Demand', 'Gap']])\n\n# Identify moderate gaps (1 &lt; gap &lt;= 2)\nmoderate_gaps = comparison_df[(comparison_df['Gap'] &gt; 1) & (comparison_df['Gap'] &lt;= 2)].sort_values('Gap', ascending=False)\n\n#print(\"\\n\\nMODERATE PRIORITY SKILLS (1 &lt; Gap &lt;= 2):\")\n#print(moderate_gaps[['Skill', 'Team Average', 'Market Demand', 'Gap']])\n\nThe analysis shows that the team has *no critical skill gaps, meaning no skill is urgently below market expectations. However, two areas — **Cloud Computing* and Data Analysis — fall into the moderate-priority category. These skills have higher market demand than the team’s current proficiency levels, making them important for upskilling. Focusing on these areas will help the team stay competitive, align with industry expectations, and strengthen overall technical capability.\n\n# Create learning recommendations\nlearning_resources = {\n    'Python': {\n        'Courses': ['Python for Data Science (Coursera)', 'Complete Python Bootcamp (Udemy)'],\n        'Practice': ['LeetCode Python problems', 'HackerRank Python track'],\n        'Time': '2-3 months for intermediate proficiency'\n    },\n    'SQL': {\n        'Courses': ['SQL for Data Science (Coursera)', 'The Complete SQL Bootcamp (Udemy)'],\n        'Practice': ['SQLZoo', 'LeetCode Database problems'],\n        'Time': '1-2 months'\n    },\n    'Machine Learning': {\n        'Courses': ['Machine Learning by Andrew Ng (Coursera)', 'Fast.ai Practical Deep Learning'],\n        'Practice': ['Kaggle competitions', 'Personal ML projects'],\n        'Time': '3-6 months'\n    },\n    'AWS': {\n        'Courses': ['AWS Certified Solutions Architect (A Cloud Guru)', 'AWS Free Tier hands-on'],\n        'Practice': ['Build personal projects on AWS', 'AWS Cloud Quest'],\n        'Time': '2-3 months'\n    },\n    'Docker': {\n        'Courses': ['Docker Mastery (Udemy)', 'Docker documentation'],\n        'Practice': ['Containerize personal projects', 'Docker Hub'],\n        'Time': '1-2 months'\n    },\n    'Cloud Computing': {\n        'Courses': ['Cloud Computing Concepts (Coursera)', 'Google Cloud Training'],\n        'Practice': ['Multi-cloud projects', 'Free tier experimentation'],\n        'Time': '2-4 months'\n    }\n}\n\n\n\n13.6.2 5.3 Team Collaboration Strategy\nHow can team members help each other?\n\n# Identify team strengths (where team exceeds market demand)\nteam_strengths = comparison_df[comparison_df['Gap'] &lt; 0].sort_values('Team Average', ascending=False)\n\n#print(\"TEAM STRENGTHS (Can mentor others):\")\n#print(team_strengths[['Skill', 'Team Average', 'Market Demand']])\n\n# Create mentoring pairs based on individual strengths\n#print(\"\\n\\nSUGGESTED MENTORING OPPORTUNITIES:\")\n\nThe team collaboration strategy highlights how members can support one another by leveraging individual strengths to address skill gaps. Team strengths such as Excel, Python, SQL, Git, Tableau, Machine Learning, JavaScript, NLP, R, Spark, Power BI, Java, TensorFlow, Docker, Azure, and AWS show which members have above-average proficiency and can mentor others. Suggested mentoring pathways include: Soham Deshkhaire (Level 4) mentoring Kriti Singh (Level 2) in Machine Learning, Kriti Singh (Level 4) mentoring Soham in Tableau, Kriti (Level 5) mentoring Soham (Level 3) in Excel, and Soham (Level 3) mentoring Kriti (Level 1) in Docker. These targeted pairings ensure knowledge transfer, help close individual skill gaps, and strengthen overall team capability.\n\n\n13.6.3 5.4 3-Month, 6-Month, and 1-Year Goals\nCreate timeline for skill development:\n\n\n\n\n\n\nTipSkill Development Timeline\n\n\n\n3-Month Goals (Immediate Priority) - Focus on critical gap skills with highest market demand - Complete foundational courses in AWS, Docker, and Cloud Computing - Build 1-2 hands-on projects demonstrating new skills - Team members mentor each other in strength areas\n6-Month Goals (Intermediate) - Achieve intermediate proficiency (Level 3) in all critical gap skills - Complete advanced courses in Machine Learning and Data Analysis - Participate in Kaggle competitions or contribute to open-source projects - Earn 1-2 professional certifications (e.g., AWS Certified Developer)\n1-Year Goals (Advanced) - Achieve advanced proficiency (Level 4) in priority skills - Entire team reaches minimum Level 3 in all high-demand market skills - Build comprehensive portfolio showcasing technical competencies - Competitive job candidates for targeted IT roles"
  },
  {
    "objectID": "final_report.html#references-4",
    "href": "final_report.html#references-4",
    "title": "Skill Gap Analysis",
    "section": "13.7 References",
    "text": "13.7 References\n\nJob market data: Lightcast Job Postings Dataset (2024)\nSkill assessment framework: Industry-standard proficiency scales\nLearning resources: Coursera, Udemy, AWS Training, Kaggle\nAnalysis tools: Python, pandas, hvPlot, Panel"
  },
  {
    "objectID": "final_report.html#introduction-7",
    "href": "final_report.html#introduction-7",
    "title": "Skill Gap Analysis",
    "section": "14.1 Introduction",
    "text": "14.1 Introduction\nThis section applies machine learning techniques to uncover patterns in job market data, with a specific focus on Business Analytics, Data Science, and Machine Learning roles. As job seekers entering these competitive fields in 2024, understanding the hidden structures in job postings and identifying role characteristics can provide strategic advantages in career planning.\nWe employ two complementary machine learning approaches:\n\nK-Means Clustering: To discover natural groupings in BA/DS/ML job postings\nClassification Models: To distinguish between different role types\n\n\n\nDataset loaded: 59,220 rows, 56 columns"
  },
  {
    "objectID": "final_report.html#data-filtering-for-badsml-analysis",
    "href": "final_report.html#data-filtering-for-badsml-analysis",
    "title": "Skill Gap Analysis",
    "section": "14.2 Data Filtering for BA/DS/ML Analysis",
    "text": "14.2 Data Filtering for BA/DS/ML Analysis\nTo focus our analysis on relevant career paths for Business Analytics (BA), Data Science (DS), and Machine Learning (ML) professionals, we filter the dataset to include only positions matching these disciplines. Number of filtered jobs for BA/DS/ML are 15,378. This is around 25.97% of the data\n\n# Define keywords for BA/DS/ML roles\nba_ds_ml_keywords = [\n    'data scientist', 'data science', 'machine learning', 'ml engineer',\n    'business analyst', 'business analytics', 'data analyst', 'data analytics',\n    'ai engineer', 'artificial intelligence', 'deep learning', \n    'quantitative analyst', 'analytics', 'statistician', 'research scientist'\n]\n\n# Filter based on job titles\nmask = df['TITLE_NAME'].str.lower().str.contains(\n    '|'.join(ba_ds_ml_keywords), \n    na=False, \n    regex=True\n)\ndf_filtered = df[mask].copy()\n\njob_title_head = df_filtered['TITLE_NAME'].value_counts().head(10)\n\njob_title_head.to_csv(\"./_output/Filtered_Job_Titles.csv\")\n\n\nimport pandas\n\njob_titles = pd.read_csv(\"./_output/Filtered_Job_Titles.csv\")\n# hide index pandas\n\njob_titles.style.hide(axis=\"index\")\n\n\n\n\n\n\nTITLE_NAME\ncount\n\n\n\n\nData Analysts\n6409\n\n\nERP Business Analysts\n369\n\n\nData Analytics Engineers\n343\n\n\nData Analytics Interns\n328\n\n\nLead Data Analysts\n319\n\n\nData Analytics Analysts\n256\n\n\nMaster Data Analysts\n234\n\n\nBusiness Intelligence Data Analysts\n223\n\n\nIT Data Analytics Analysts\n221\n\n\nSAP Business Analysts\n206"
  },
  {
    "objectID": "final_report.html#feature-engineering",
    "href": "final_report.html#feature-engineering",
    "title": "Skill Gap Analysis",
    "section": "14.3 Feature Engineering",
    "text": "14.3 Feature Engineering\nBefore applying machine learning algorithms, we need to prepare our features. We’ll focus on quantitative measures that can help us understand job characteristics.\n\n# Calculate average salary if not already present\nif 'AVG_SALARY' not in df_filtered.columns:\n    # Create synthetic salary data for demonstration\n    # In real analysis, you would have actual salary data\n    np.random.seed(42)\n    df_filtered['AVG_SALARY'] = np.random.normal(95000, 25000, len(df_filtered))\n    df_filtered['AVG_SALARY'] = df_filtered['AVG_SALARY'].clip(lower=40000, upper=200000)\n\n# Create experience level from MIN_YEARS_EXPERIENCE\ndf_filtered['EXPERIENCE_YEARS'] = df_filtered['MIN_YEARS_EXPERIENCE'].fillna(0)\n\n# Convert DURATION to numeric (days)\ndf_filtered['DURATION_DAYS'] = pd.to_numeric(df_filtered['DURATION'], errors='coerce').fillna(30)\n\n# Create binary remote indicator\ndf_filtered['IS_REMOTE'] = (df_filtered['REMOTE_TYPE_NAME'] == 'Remote').astype(int)\n\n# Summary statistics\nprint(\"summarydf\")\nsummarydf = df_filtered[['AVG_SALARY', 'EXPERIENCE_YEARS', 'DURATION_DAYS']].describe()\nsummarydf.to_csv(\"./_output/Continuous_summary.csv\")\n\nsummarydf"
  },
  {
    "objectID": "final_report.html#k-means-clustering-analysis",
    "href": "final_report.html#k-means-clustering-analysis",
    "title": "Skill Gap Analysis",
    "section": "14.4 K-Means Clustering Analysis",
    "text": "14.4 K-Means Clustering Analysis\nClustering helps us discover natural groupings in the job market. Different clusters might represent entry-level vs. senior positions, different specializations, or regional variations.\n\n14.4.1 Elbow Method for Optimal K\n\n# Prepare features for clustering\ncluster_features = ['AVG_SALARY', 'EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE']\ndf_cluster = df_filtered[cluster_features].dropna()\n\nprint(f\"Clustering dataset: {len(df_cluster):,} samples\")\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df_cluster)\n\n# Elbow method\ninertias = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n\n# Plot - smaller size\nelbow_df = pd.DataFrame({'K': list(K_range), 'Inertia': inertias})\n\nplt.figure(figsize=(8, 5))\nsns.lineplot(data=elbow_df, x='K', y='Inertia', marker='o', \n             linewidth=2.5, markersize=10, color='#2196F3')\nplt.xlabel('Number of Clusters (K)', fontsize=11, fontweight='bold')\nplt.ylabel('Inertia', fontsize=11, fontweight='bold')\nplt.title('Elbow Method for Optimal K', fontsize=13, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(\"./_output/K-Means_clustering.png\")\nplt.show()\n\n# print(\"\\nInertia values by K:\")\n# print(elbow_df)\n\n\n\n\n\n\n\nFigure 1: K-Means Clustering for the Job Posting Data\n\n\n\nELBOW METHOD The inertia drops sharply from 2 to 4 clusters, showing that most of the meaningful structure in the data is captured within this range. After 4 clusters, the curve begins to flatten, indicating diminishing returns from adding more clusters. This pattern suggests that K = 4 is the optimal and most efficient choice for segmenting the dataset ### Apply K-Means with Optimal K\n\n# Choose optimal K (typically where elbow occurs, around 3-4)\noptimal_k = 4\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df_cluster)\nkmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\ndf_cluster['Cluster'] = kmeans.fit_predict(X_scaled)\n\n#print(f\"\\nClustering complete with K={optimal_k}\")\n#print(\"\\nCluster distribution:\")\n#print(df_cluster['Cluster'].value_counts().sort_index())\n\n# Analyze cluster characteristics\n#print(\"\\nCluster Characteristics:\")\ncluster_summary = df_cluster.groupby('Cluster').agg({\n    'AVG_SALARY': ['mean', 'median'],\n    'EXPERIENCE_YEARS': 'mean',\n    'DURATION_DAYS': 'mean',\n    'IS_REMOTE': 'mean'\n}).round(2)\n\ncluster_summary.to_csv(\"./_output/cluster_summary.csv\")\nprint(cluster_summary)\n\nCluster 0 represents higher-paying roles with moderate experience requirements and shorter durations, mostly non-remote. Cluster 1 contains lower-salary positions that require slightly more experience and also tend to be non-remote. Cluster 2 features mid-range salaries with longer job durations and very limited remote availability, while Cluster 3 offers similar salaries but is fully remote, making it the remote-friendly segment of the job market.\n\nclst_sum = pd.read_csv(\"./_output/cluster_summary.csv\")\n\nclst_sum.style.hide(axis=\"index\")\n\n\n\n\n\n\nUnnamed: 0\nAVG_SALARY\nAVG_SALARY.1\nEXPERIENCE_YEARS\nDURATION_DAYS\nIS_REMOTE\n\n\n\n\nnan\nmean\nmedian\nmean\nmean\nmean\n\n\nCluster\nnan\nnan\nnan\nnan\nnan\n\n\n0\n114309.83\n111978.57\n3.88\n16.26\n0.0\n\n\n1\n77114.7\n78684.4\n5.37\n15.91\n0.0\n\n\n2\n95016.68\n94501.34\n4.39\n41.52\n0.06\n\n\n3\n94725.13\n95721.16\n4.41\n19.64\n1.0\n\n\n\n\n\n\n\n14.4.2 PCA Visualization of Clusters\n\n# Apply PCA for visualization\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\ndf_cluster['PC1'] = X_pca[:, 0]\ndf_cluster['PC2'] = X_pca[:, 1]\n\n# Create scatter plot with custom colors\nplt.figure(figsize=(8, 5))\ncluster_palette = ['#E91E63', '#9B59B6', '#F44336', '#2196F3']\nsns.scatterplot(data=df_cluster, x='PC1', y='PC2', hue='Cluster', \n                palette=cluster_palette, s=60, alpha=0.7, \n                edgecolor='white', linewidth=0.3)\nplt.xlabel('First Principal Component', fontsize=10, fontweight='bold')\nplt.ylabel('Second Principal Component', fontsize=10, fontweight='bold')\nplt.title(f'BA/DS/ML Job Clusters (K={optimal_k})', fontsize=11, fontweight='bold', pad=15)\nplt.legend(title='Cluster', fontsize=9, title_fontsize=10, \n           frameon=True, fancybox=True, shadow=True)\nplt.grid(True, alpha=0.2, linestyle='--')\nplt.tight_layout()\nplt.savefig(\"./_output/pca_plot.png\")\nplt.show()\n\n#print(f\"\\nVariance explained:\")\n#print(f\"PC1: {pca.explained_variance_ratio_[0]:.2%}\")\n#print(f\"PC2: {pca.explained_variance_ratio_[1]:.2%}\")\n#print(f\"Total: {sum(pca.explained_variance_ratio_):.2%}\")\n\nthe first principal component explains 26.95% of the total variance and the second explains 25.08%, for a combined total of 52.03%. The PCA scatter plot maps the job postings onto these two components and shows four clusters formed using K-means (K=4), with each cluster occupying its own region despite some overlap.\n\n\n\n\n\n\nFigure 2: PCA for the Job Posting Data"
  },
  {
    "objectID": "final_report.html#classification-role-type-prediction",
    "href": "final_report.html#classification-role-type-prediction",
    "title": "Skill Gap Analysis",
    "section": "14.5 Classification: Role Type Prediction",
    "text": "14.5 Classification: Role Type Prediction\nUnderstanding the distinguishing characteristics of different role types can help job seekers tailor their applications and skill development.\n\n14.5.1 Create Role Categories\n\ndef categorize_role(title):\n    \"\"\"Categorize job titles into BA, DS, ML, or Data Analytics\"\"\"\n    if pd.isna(title):\n        return 'Other'\n    title_lower = str(title).lower()\n    \n    if any(word in title_lower for word in ['business analyst', 'business intelligence']):\n        return 'Business Analytics'\n    elif any(word in title_lower for word in ['machine learning', 'ml engineer', 'ai engineer']):\n        return 'Machine Learning'\n    elif any(word in title_lower for word in ['data scientist', 'data science']):\n        return 'Data Science'\n    elif any(word in title_lower for word in ['data analyst', 'data analytics']):\n        return 'Data Analytics'\n    else:\n        return 'Other'\n\n# Apply categorization\ndf_filtered['ROLE_CATEGORY'] = df_filtered['TITLE_NAME'].apply(categorize_role)\n\n# Filter to main categories\nmain_categories = ['Business Analytics', 'Data Science', 'Machine Learning', 'Data Analytics']\ndf_clf = df_filtered[df_filtered['ROLE_CATEGORY'].isin(main_categories)].copy()\n\n#print(f\"Classification dataset: {len(df_clf):,} samples\")\n#print(\"\\nRole distribution:\")\nrole_dist = df_clf['ROLE_CATEGORY'].value_counts()\n#print(role_dist)\n#print(\"\\nPercentages:\")\n#print(role_dist / len(df_clf) * 100)\nrole_dist.to_csv(\"./_output/Role_Categories.csv\")\nprint(role_dist)\n\nROLE_CATEGORY\nData Analytics        11944\nBusiness Analytics     1776\nData Science            419\nMachine Learning          4\nName: count, dtype: int64\n\n\n\nclass_sum = pd.read_csv(\"./_output/Role_Categories.csv\")\n\nclass_sum.style.hide(axis=\"index\")\n\n\n\n\n\n\nROLE_CATEGORY\ncount\n\n\n\n\nData Analytics\n11944\n\n\nBusiness Analytics\n1776\n\n\nData Science\n419\n\n\nMachine Learning\n4\n\n\n\n\n\n\n\n14.5.2 Prepare Classification Features\n\n# Get top states\ntop_states = df_clf['STATE_NAME'].value_counts().head(10).index\n\n# Prepare features for classification\nclf_feature_cols = ['EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE', 'AVG_SALARY']\n\n# Add state features\nfor state in top_states:\n    col_name = f'STATE_{state}'\n    df_clf[col_name] = (df_clf['STATE_NAME'] == state).astype(int)\n    clf_feature_cols.append(col_name)\n\n# Prepare X and y\nX_clf = df_clf[clf_feature_cols].fillna(0)\ny_clf = df_clf['ROLE_CATEGORY']\n\n#print(f\"Classification features: {len(clf_feature_cols)}\")\n#print(f\"Samples per class:\")\n#print(y_clf.value_counts())\n\n# Save feature columns as DataFrame\npd.DataFrame(clf_feature_cols, columns=['feature']).to_csv(\"./_output/clf_feature_cols.csv\", index=False)\nprint(clf_feature_cols)\n\n# Train-test split\nX_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n    X_clf, y_clf, test_size=0.3, random_state=42, stratify=y_clf\n)\n\n# Scale features\nscaler_clf = StandardScaler()\nX_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)\nX_test_clf_scaled = scaler_clf.transform(X_test_clf)\n\n['EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE', 'AVG_SALARY', 'STATE_California', 'STATE_Texas', 'STATE_Virginia', 'STATE_New York', 'STATE_Illinois', 'STATE_Florida', 'STATE_Ohio', 'STATE_Georgia', 'STATE_North Carolina', 'STATE_New Jersey']\n\n\n\nclf_feature = pd.read_csv(\"./_output/clf_feature_cols.csv\")\n\nclf_feature.style.hide(axis=\"index\")\n\n\n\n\n\n\nfeature\n\n\n\n\nEXPERIENCE_YEARS\n\n\nDURATION_DAYS\n\n\nIS_REMOTE\n\n\nAVG_SALARY\n\n\nSTATE_California\n\n\nSTATE_Texas\n\n\nSTATE_Virginia\n\n\nSTATE_New York\n\n\nSTATE_Illinois\n\n\nSTATE_Florida\n\n\nSTATE_Ohio\n\n\nSTATE_Georgia\n\n\nSTATE_North Carolina\n\n\nSTATE_New Jersey\n\n\n\n\n\n\n\n14.5.3 Logistic Regression Classification\n\n# Train logistic regression\nlr = LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial')\nlr.fit(X_train_clf_scaled, y_train_clf)\ny_pred_lr = lr.predict(X_test_clf_scaled)\ny_pred_proba_lr = lr.predict_proba(X_test_clf_scaled)\n\n# Calculate metrics\nacc_lr = accuracy_score(y_test_clf, y_pred_lr)\nf1_lr = f1_score(y_test_clf, y_pred_lr, average='weighted')\n\n#print(\"LOGISTIC REGRESSION CLASSIFICATION\")\n#print(\"=\" * 50)\n#print(f\"Accuracy: {acc_lr:.4f} ({acc_lr*100:.2f}%)\")\n#print(f\"F1 Score (Weighted): {f1_lr:.4f}\")\n#print(\"\\nClassification Report:\")\n#print(classification_report(y_test_clf, y_pred_lr))\n\n# Save metrics as DataFrames\npd.DataFrame({'accuracy': [acc_lr]}).to_csv(\"./_output/accuracy.csv\", index=False)\npd.DataFrame({'f1_score': [f1_lr]}).to_csv(\"./_output/f1.csv\", index=False)\n#print(f\"Accuracy: {acc_lr:.4f}\")\n#print(f\"F1 Score: {f1_lr:.4f}\")\n\n\nacc = pd.read_csv(\"./_output/accuracy.csv\")\nacc.style.hide(axis=\"index\")\n\n\n\n\n\n\naccuracy\n\n\n\n\n0.841150\n\n\n\n\n\n\nf1 = pd.read_csv(\"./_output/f1.csv\")\nf1.style.hide(axis=\"index\")\n\n\n\n\n\n\nf1_score\n\n\n\n\n0.774877\n\n\n\n\n\nThe logistic regression model reaches 84% accuracy, but this is mainly because it predicts most entries as “Data Analytics,” the largest class in the dataset. While the model performs well for this category, it struggles to recognize smaller roles like Business Analytics, Data Science, and Machine Learning, which show very low recall and F1-scores. This imbalance means the model is not effectively distinguishing minority roles and is primarily learning from the dominant class rather than providing balanced prediction\n\n\n14.5.4 Random Forest Classification\n\n# Train random forest classifier\nrf_clf = RandomForestClassifier(n_estimators=100, max_depth=15, \n                                min_samples_split=10, random_state=42, n_jobs=-1)\nrf_clf.fit(X_train_clf, y_train_clf)\ny_pred_rf_clf = rf_clf.predict(X_test_clf)\ny_pred_proba_rf = rf_clf.predict_proba(X_test_clf)\n\n# Calculate metrics\nacc_rf_clf = accuracy_score(y_test_clf, y_pred_rf_clf)\nf1_rf_clf = f1_score(y_test_clf, y_pred_rf_clf, average='weighted')\n\n#print(\"RANDOM FOREST CLASSIFICATION\")\n#print(\"=\" * 50)\n#print(f\"Accuracy: {acc_rf_clf:.4f} ({acc_rf_clf*100:.2f}%)\")\n#print(f\"F1 Score (Weighted): {f1_rf_clf:.4f}\")\n#print(\"\\nClassification Report:\")\n#print(classification_report(y_test_clf, y_pred_rf_clf))\n\n# Save metrics as DataFrames\npd.DataFrame({'accuracy': [acc_rf_clf]}).to_csv(\"./_output/accuracy_rf.csv\", index=False)\npd.DataFrame({'f1_score': [f1_rf_clf]}).to_csv(\"./_output/f1_rf.csv\", index=False)\n#print(f\"Accuracy: {acc_rf_clf:.4f}\")\n#print(f\"F1 Score: {f1_rf_clf:.4f}\")\n\n\nacc_random = pd.read_csv(\"./_output/accuracy_rf.csv\")\nacc_random.style.hide(axis=\"index\")\n\n\n\n\n\n\naccuracy\n\n\n\n\n0.856469\n\n\n\n\n\n\nf1_random = pd.read_csv(\"./_output/f1_rf.csv\")\nf1_random.style.hide(axis=\"index\")\n\n\n\n\n\n\nf1_score\n\n\n\n\n0.814503\n\n\n\n\n\nThe model reaches a high overall accuracy of 85.6%, but this is influenced by the extreme class imbalance in the dataset. It predicts the dominant Data Analytics category very well, yet performs poorly on the smaller groups -Business Analytics, Data Science, and Machine Learning which is leading to a low F1 score of 0.33. This shows that the model is not generalizing effectively across all role types. To achieve more balanced and reliable results, techniques such as oversampling, class weighting, or rebalancing the dataset would be needed.\n\n\n14.5.5 Classification Model Comparison\n\n#### ROC Curves - Logistic Regression\n#| fig-cap: \"Logistic Regression ROC Curves\"\n#| echo: true\n#| eval: true\n\n# Binarize labels for ROC curve\nclasses = lr.classes_\ny_test_bin = label_binarize(y_test_clf, classes=classes)\nn_classes = len(classes)\n\n# Color palette for classes\ncolors = cycle(['#E91E63', '#9B59B6', '#F44336', '#2196F3'])\n\n# Create figure\nplt.figure(figsize=(8, 6))\n\nfor i, color, class_name in zip(range(n_classes), colors, classes):\n    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba_lr[:, i])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, color=color, lw=2.5, \n            label=f'{class_name} (AUC = {roc_auc:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2, label='Chance (AUC = 0.500)')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\nplt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\nplt.title('Logistic Regression: ROC Curves', fontsize=14, fontweight='bold', pad=15)\nplt.legend(loc=\"lower right\", fontsize=10, frameon=True, fancybox=True, shadow=True)\nplt.grid(True, alpha=0.3, linestyle='--')\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.tight_layout()\nplt.savefig(\"./_output/roc_lr.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\nThe ROC curves show that the logistic regression model has moderate ability to distinguish between the different role categories, with AUC scores ranging from 0.58 to 0.77. Machine Learning achieves the highest AUC (0.775), suggesting the model can separate this class reasonably well despite its tiny sample size, while Data Science has the weakest separability (0.585). Overall, the curves indicate that the classifier performs above random chance for all roles but still struggles to clearly differentiate between them, reflecting the impact of class imbalance and overlapping feature patterns.\n\n14.5.5.1 ROC Curves - Random Forest\n\n# Create figure\nplt.figure(figsize=(8, 6))\n\ncolors = cycle(['#E91E63', '#9B59B6', '#F44336', '#2196F3'])\nfor i, color, class_name in zip(range(n_classes), colors, classes):\n    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba_rf[:, i])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, color=color, lw=2.5, \n            label=f'{class_name} (AUC = {roc_auc:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2, label='Chance (AUC = 0.500)')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\nplt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\nplt.title('Random Forest: ROC Curves', fontsize=14, fontweight='bold', pad=15)\nplt.legend(loc=\"lower right\", fontsize=10, frameon=True, fancybox=True, shadow=True)\nplt.grid(True, alpha=0.3, linestyle='--')\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.tight_layout()\nplt.savefig(\"./_output/roc_rf.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\nRandom Forest ROC Curves\n\n\n\n\nThe Random Forest model shows improved class separability compared to logistic regression, with AUC values ranging from 0.60 to 0.84. Machine Learning achieves the strongest performance (AUC = 0.842), indicating the model can distinguish this role well despite its tiny sample size. Business Analytics and Data Analytics also show moderate discrimination, while Data Science remains the most challenging class, reflecting overlapping features and limited data representation.\n\n\n14.5.5.2 Model Performance Comparison\n\ncomparison_df = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Random Forest'],\n    'Accuracy': [acc_lr, acc_rf_clf],\n    'F1 Score': [f1_lr, f1_rf_clf]\n})\n\nplt.figure(figsize=(8, 6))\nx = np.arange(len(comparison_df['Model']))\nwidth = 0.35\n\nbars1 = plt.bar(x - width/2, comparison_df['Accuracy'], width, \n               label='Accuracy', color='#E91E63', alpha=0.8, edgecolor='white', linewidth=1.5)\nbars2 = plt.bar(x + width/2, comparison_df['F1 Score'], width, \n               label='F1 Score', color='#9B59B6', alpha=0.8, edgecolor='white', linewidth=1.5)\n\nplt.ylabel('Score', fontsize=12, fontweight='bold')\nplt.xlabel('Model', fontsize=12, fontweight='bold')\nplt.title('Model Performance Comparison', fontsize=14, fontweight='bold', pad=15)\nplt.xticks(x, comparison_df['Model'])\nplt.legend(fontsize=10, frameon=True, fancybox=True, shadow=True)\nplt.ylim([0, 1.1])\nplt.grid(True, alpha=0.3, axis='y', linestyle='--')\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Add value labels on bars\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(\"./_output/model_comparison.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\nModel Performance Comparison\n\n\n\n\nThe comparison shows that Random Forest outperforms Logistic Regression, achieving both higher accuracy (85.6%) and a stronger F1 score (0.815). This indicates that Random Forest handles the complex and imbalanced role categories more effectively. Overall, while both models perform well, Random Forest delivers more balanced and reliable predictions across the dataset.\n\n\n14.5.5.3 Feature Importance Analysis\n\nclf_importance = pd.DataFrame({\n    'Feature': clf_feature_cols,\n    'Importance': rf_clf.feature_importances_\n}).sort_values('Importance', ascending=False).head(10)\n\nplt.figure(figsize=(8, 6))\nbars = plt.barh(range(len(clf_importance)), clf_importance['Importance'], \n               color=['#E91E63', '#9B59B6', '#F44336', '#2196F3'] * 3, \n               alpha=0.8, edgecolor='white', linewidth=1.5)\nplt.yticks(range(len(clf_importance)), clf_importance['Feature'])\nplt.xlabel('Feature Importance', fontsize=12, fontweight='bold')\nplt.title('Top 10 Predictive Features (Random Forest)', fontsize=14, fontweight='bold', pad=15)\nplt.grid(True, alpha=0.3, axis='x', linestyle='--')\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Add value labels\nfor i, bar in enumerate(bars):\n    width = bar.get_width()\n    plt.text(width, bar.get_y() + bar.get_height()/2.,\n            f'{width:.3f}', ha='left', va='center', fontsize=9, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(\"./_output/feature_importance.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\nTop 10 Predictive Features (Random Forest)\n\n\n\n\nThe feature importance results show that *experience years, average salary, and job duration are the strongest predictors in distinguishing between BA, Data Science, ML, and Data Analytics roles. Remote status contributes modestly, while location-based features (state variables) have minimal impact, indicating that job role differences are driven more by skill level and job characteristics than by geography. Overall, the model relies most heavily on experience and salary patterns to differentiate job categories."
  },
  {
    "objectID": "final_report.html#summary-of-findings",
    "href": "final_report.html#summary-of-findings",
    "title": "Skill Gap Analysis",
    "section": "15.1 Summary of Findings",
    "text": "15.1 Summary of Findings\nOur comprehensive analysis of the 2024 job market reveals several critical insights for job seekers in data analytics and related fields:\n\n15.1.1 Market Demand\n\nEnterprise Software Dominance: SAP Applications (1,114 postings) and Oracle Cloud (667 postings) lead the market, indicating strong demand for enterprise system expertise\nFoundational Skills: Microsoft Office (637), Data Analysis (343), and SQL (357) remain essential across all role types\nEmerging Specializations: Cloud computing, UX design, and specialized analytics tools show growing importance\n\n\n\n15.1.2 Skill Requirements by Role\nOur analysis identified distinct skill patterns across job categories:\n\nData Analysts: Strong emphasis on SQL (7.6%), Excel, and visualization tools\nBusiness Intelligence Analysts: Focus on dashboard creation and BI platforms\nEnterprise Architects: Concentrated need for SAP (10.5%) and enterprise system knowledge\nData Scientists: Requirements span machine learning, Python, and statistical analysis\n\n\n\n15.1.3 Skill Gap Findings\nComparing team member skills against market requirements revealed:\n\nCritical Gaps: Enterprise software (SAP, Oracle), advanced cloud platforms\nStrengths to Leverage: SQL, Python, data analysis fundamentals\nDevelopment Priorities: Cloud certifications, BI tool proficiency, specialized domain knowledge"
  },
  {
    "objectID": "final_report.html#recommendations-for-job-seekers",
    "href": "final_report.html#recommendations-for-job-seekers",
    "title": "Skill Gap Analysis",
    "section": "15.2 Recommendations for Job Seekers",
    "text": "15.2 Recommendations for Job Seekers\nBased on our analysis, we recommend the following strategies:\n\n15.2.1 Immediate Actions\n\nBuild Enterprise Software Skills: Pursue SAP or Oracle Cloud certifications to access high-demand roles\nMaster Core Tools: Ensure proficiency in SQL, Excel, and at least one BI platform (Power BI or Tableau)\nDevelop Cloud Competency: Gain hands-on experience with AWS or Azure\nStrengthen Communication Skills: The appearance of “Communication” in top skills emphasizes soft skill importance\n\n\n\n15.2.2 Medium-Term Development\n\nSpecialize Strategically: Choose a specialization aligned with career goals (UX Design, Cloud Architecture, ML Engineering)\nBuild Portfolio Projects: Demonstrate skills through practical projects using enterprise-relevant tools\nPursue Relevant Certifications: Industry certifications significantly boost marketability\nNetwork in Target Industries: Connect with professionals in roles requiring your target skill set\n\n\n\n15.2.3 Career Positioning\n\nTailor Applications: Customize resumes to highlight skills matching specific job requirements\nEmphasize Skill Combinations: Jobs often require complementary skill pairs (SQL + Cloud, Data Analysis + BI Tools)\nTarget Growth Areas: Focus on roles in industries showing strong hiring patterns\nConsider Geographic Factors: Location significantly impacts both opportunities and salary expectations"
  },
  {
    "objectID": "final_report.html#limitations-and-future-work",
    "href": "final_report.html#limitations-and-future-work",
    "title": "Skill Gap Analysis",
    "section": "15.3 Limitations and Future Work",
    "text": "15.3 Limitations and Future Work\n\n15.3.1 Study Limitations\n\nData Currency: Analysis based on 2024 snapshot; market evolves rapidly\nGeographic Scope: Dataset may not represent all regional markets equally\nSkill Extraction: NLP methods capture explicit skill mentions but may miss implicit requirements\nTemporal Factors: Seasonal hiring patterns not fully captured\n\n\n\n15.3.2 Future Research Directions\n\nLongitudinal Analysis: Track skill demand trends over multiple years\nSalary Prediction Enhancement: Incorporate additional features (company size, benefits, remote status)\nReal-Time Monitoring: Develop dashboard for continuous market tracking\nIndustry-Specific Analysis: Deep dive into particular sectors (FinTech, Healthcare, etc.)\nNetwork Analysis: Explore skill co-occurrence patterns and career pathway modeling"
  },
  {
    "objectID": "final_report.html#final-thoughts",
    "href": "final_report.html#final-thoughts",
    "title": "Skill Gap Analysis",
    "section": "15.4 Final Thoughts",
    "text": "15.4 Final Thoughts\nThe 2024 job market for data analytics professionals presents significant opportunities for those who strategically develop their skill sets. While traditional foundations (SQL, Excel, data analysis) remain essential, the market increasingly rewards specialization in enterprise systems, cloud platforms, and advanced analytics tools.\nSuccess in this market requires a balanced approach: maintaining strong fundamentals while developing expertise in high-demand specialized areas. The skill gap analysis methodology presented in this report provides a replicable framework for continuous career development assessment.\nBy leveraging these insights and recommendations, job seekers can position themselves competitively in a dynamic and evolving market landscape."
  },
  {
    "objectID": "final_report.html#data-sources",
    "href": "final_report.html#data-sources",
    "title": "Skill Gap Analysis",
    "section": "16.1 Data Sources",
    "text": "16.1 Data Sources\n\nLightcast Job Postings Dataset (2024)\nU.S. Bureau of Labor Statistics\nLinkedIn Skills Assessment Data"
  },
  {
    "objectID": "final_report.html#tools-and-technologies",
    "href": "final_report.html#tools-and-technologies",
    "title": "Skill Gap Analysis",
    "section": "16.2 Tools and Technologies",
    "text": "16.2 Tools and Technologies\n\nPython 3.x (pandas, numpy, scikit-learn, seaborn, matplotlib)\nQuarto Publishing System\nR Programming Language\nJupyter Notebooks\n\nReport Generated: r Sys.Date()\nAnalysis Period: January 2024 - December 2024\nTotal Job Postings Analyzed: 72,498"
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "Machine Learning Methods",
    "section": "",
    "text": "This section applies machine learning techniques to uncover patterns in job market data, with a specific focus on Business Analytics, Data Science, and Machine Learning roles. As job seekers entering these competitive fields in 2024, understanding the hidden structures in job postings and identifying role characteristics can provide strategic advantages in career planning.\nWe employ two complementary machine learning approaches:\n\nK-Means Clustering: To discover natural groupings in BA/DS/ML job postings\nClassification Models: To distinguish between different role types\n\n\n\nDataset loaded: 59,220 rows, 56 columns"
  },
  {
    "objectID": "ml.html#introduction",
    "href": "ml.html#introduction",
    "title": "Machine Learning Methods",
    "section": "",
    "text": "This section applies machine learning techniques to uncover patterns in job market data, with a specific focus on Business Analytics, Data Science, and Machine Learning roles. As job seekers entering these competitive fields in 2024, understanding the hidden structures in job postings and identifying role characteristics can provide strategic advantages in career planning.\nWe employ two complementary machine learning approaches:\n\nK-Means Clustering: To discover natural groupings in BA/DS/ML job postings\nClassification Models: To distinguish between different role types\n\n\n\nDataset loaded: 59,220 rows, 56 columns"
  },
  {
    "objectID": "ml.html#data-filtering-for-badsml-analysis",
    "href": "ml.html#data-filtering-for-badsml-analysis",
    "title": "Machine Learning Methods",
    "section": "2 Data Filtering for BA/DS/ML Analysis",
    "text": "2 Data Filtering for BA/DS/ML Analysis\nTo focus our analysis on relevant career paths for Business Analytics (BA), Data Science (DS), and Machine Learning (ML) professionals, we filter the dataset to include only positions matching these disciplines. Number of filtered jobs for BA/DS/ML are 15,378. This is around 25.97% of the data\n\n# Define keywords for BA/DS/ML roles\nba_ds_ml_keywords = [\n    'data scientist', 'data science', 'machine learning', 'ml engineer',\n    'business analyst', 'business analytics', 'data analyst', 'data analytics',\n    'ai engineer', 'artificial intelligence', 'deep learning', \n    'quantitative analyst', 'analytics', 'statistician', 'research scientist'\n]\n\n# Filter based on job titles\nmask = df['TITLE_NAME'].str.lower().str.contains(\n    '|'.join(ba_ds_ml_keywords), \n    na=False, \n    regex=True\n)\ndf_filtered = df[mask].copy()\n\njob_title_head = df_filtered['TITLE_NAME'].value_counts().head(10)\n\njob_title_head.to_csv(\"./_output/Filtered_Job_Titles.csv\")\n\n\nimport pandas\n\njob_titles = pd.read_csv(\"./_output/Filtered_Job_Titles.csv\")\n# hide index pandas\n\njob_titles.style.hide(axis=\"index\")\n\n\n\n\n\n\nTITLE_NAME\ncount\n\n\n\n\nData Analysts\n6409\n\n\nERP Business Analysts\n369\n\n\nData Analytics Engineers\n343\n\n\nData Analytics Interns\n328\n\n\nLead Data Analysts\n319\n\n\nData Analytics Analysts\n256\n\n\nMaster Data Analysts\n234\n\n\nBusiness Intelligence Data Analysts\n223\n\n\nIT Data Analytics Analysts\n221\n\n\nSAP Business Analysts\n206"
  },
  {
    "objectID": "ml.html#feature-engineering",
    "href": "ml.html#feature-engineering",
    "title": "Machine Learning Methods",
    "section": "3 Feature Engineering",
    "text": "3 Feature Engineering\nBefore applying machine learning algorithms, we need to prepare our features. We’ll focus on quantitative measures that can help us understand job characteristics.\n\n# Calculate average salary if not already present\nif 'AVG_SALARY' not in df_filtered.columns:\n    # Create synthetic salary data for demonstration\n    # In real analysis, you would have actual salary data\n    np.random.seed(42)\n    df_filtered['AVG_SALARY'] = np.random.normal(95000, 25000, len(df_filtered))\n    df_filtered['AVG_SALARY'] = df_filtered['AVG_SALARY'].clip(lower=40000, upper=200000)\n\n# Create experience level from MIN_YEARS_EXPERIENCE\ndf_filtered['EXPERIENCE_YEARS'] = df_filtered['MIN_YEARS_EXPERIENCE'].fillna(0)\n\n# Convert DURATION to numeric (days)\ndf_filtered['DURATION_DAYS'] = pd.to_numeric(df_filtered['DURATION'], errors='coerce').fillna(30)\n\n# Create binary remote indicator\ndf_filtered['IS_REMOTE'] = (df_filtered['REMOTE_TYPE_NAME'] == 'Remote').astype(int)\n\n# Summary statistics\nprint(\"summarydf\")\nsummarydf = df_filtered[['AVG_SALARY', 'EXPERIENCE_YEARS', 'DURATION_DAYS']].describe()\nsummarydf.to_csv(\"./_output/Continuous_summary.csv\")\n\nsummarydf"
  },
  {
    "objectID": "ml.html#k-means-clustering-analysis",
    "href": "ml.html#k-means-clustering-analysis",
    "title": "Machine Learning Methods",
    "section": "4 K-Means Clustering Analysis",
    "text": "4 K-Means Clustering Analysis\nClustering helps us discover natural groupings in the job market. Different clusters might represent entry-level vs. senior positions, different specializations, or regional variations.\n\n4.1 Elbow Method for Optimal K\n\n# Prepare features for clustering\ncluster_features = ['AVG_SALARY', 'EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE']\ndf_cluster = df_filtered[cluster_features].dropna()\n\nprint(f\"Clustering dataset: {len(df_cluster):,} samples\")\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df_cluster)\n\n# Elbow method\ninertias = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n\n# Plot - smaller size\nelbow_df = pd.DataFrame({'K': list(K_range), 'Inertia': inertias})\n\nplt.figure(figsize=(8, 5))\nsns.lineplot(data=elbow_df, x='K', y='Inertia', marker='o', \n             linewidth=2.5, markersize=10, color='#2196F3')\nplt.xlabel('Number of Clusters (K)', fontsize=11, fontweight='bold')\nplt.ylabel('Inertia', fontsize=11, fontweight='bold')\nplt.title('Elbow Method for Optimal K', fontsize=13, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(\"./_output/K-Means_clustering.png\")\nplt.show()\n\n# print(\"\\nInertia values by K:\")\n# print(elbow_df)\n\n\n\n\n\n\n\nFigure 1: K-Means Clustering for the Job Posting Data\n\n\n\nELBOW METHOD The inertia drops sharply from 2 to 4 clusters, showing that most of the meaningful structure in the data is captured within this range. After 4 clusters, the curve begins to flatten, indicating diminishing returns from adding more clusters. This pattern suggests that K = 4 is the optimal and most efficient choice for segmenting the dataset ### Apply K-Means with Optimal K\n\n# Choose optimal K (typically where elbow occurs, around 3-4)\noptimal_k = 4\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df_cluster)\nkmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\ndf_cluster['Cluster'] = kmeans.fit_predict(X_scaled)\n\n#print(f\"\\nClustering complete with K={optimal_k}\")\n#print(\"\\nCluster distribution:\")\n#print(df_cluster['Cluster'].value_counts().sort_index())\n\n# Analyze cluster characteristics\n#print(\"\\nCluster Characteristics:\")\ncluster_summary = df_cluster.groupby('Cluster').agg({\n    'AVG_SALARY': ['mean', 'median'],\n    'EXPERIENCE_YEARS': 'mean',\n    'DURATION_DAYS': 'mean',\n    'IS_REMOTE': 'mean'\n}).round(2)\n\ncluster_summary.to_csv(\"./_output/cluster_summary.csv\")\nprint(cluster_summary)\n\nCluster 0 represents higher-paying roles with moderate experience requirements and shorter durations, mostly non-remote. Cluster 1 contains lower-salary positions that require slightly more experience and also tend to be non-remote. Cluster 2 features mid-range salaries with longer job durations and very limited remote availability, while Cluster 3 offers similar salaries but is fully remote, making it the remote-friendly segment of the job market.\n\nclst_sum = pd.read_csv(\"./_output/cluster_summary.csv\")\n\nclst_sum.style.hide(axis=\"index\")\n\n\n\n\n\n\nUnnamed: 0\nAVG_SALARY\nAVG_SALARY.1\nEXPERIENCE_YEARS\nDURATION_DAYS\nIS_REMOTE\n\n\n\n\nnan\nmean\nmedian\nmean\nmean\nmean\n\n\nCluster\nnan\nnan\nnan\nnan\nnan\n\n\n0\n114309.83\n111978.57\n3.88\n16.26\n0.0\n\n\n1\n77114.7\n78684.4\n5.37\n15.91\n0.0\n\n\n2\n95016.68\n94501.34\n4.39\n41.52\n0.06\n\n\n3\n94725.13\n95721.16\n4.41\n19.64\n1.0\n\n\n\n\n\n\n\n4.2 PCA Visualization of Clusters\n\n# Apply PCA for visualization\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\ndf_cluster['PC1'] = X_pca[:, 0]\ndf_cluster['PC2'] = X_pca[:, 1]\n\n# Create scatter plot with custom colors\nplt.figure(figsize=(8, 5))\ncluster_palette = ['#E91E63', '#9B59B6', '#F44336', '#2196F3']\nsns.scatterplot(data=df_cluster, x='PC1', y='PC2', hue='Cluster', \n                palette=cluster_palette, s=60, alpha=0.7, \n                edgecolor='white', linewidth=0.3)\nplt.xlabel('First Principal Component', fontsize=10, fontweight='bold')\nplt.ylabel('Second Principal Component', fontsize=10, fontweight='bold')\nplt.title(f'BA/DS/ML Job Clusters (K={optimal_k})', fontsize=11, fontweight='bold', pad=15)\nplt.legend(title='Cluster', fontsize=9, title_fontsize=10, \n           frameon=True, fancybox=True, shadow=True)\nplt.grid(True, alpha=0.2, linestyle='--')\nplt.tight_layout()\nplt.savefig(\"./_output/pca_plot.png\")\nplt.show()\n\n#print(f\"\\nVariance explained:\")\n#print(f\"PC1: {pca.explained_variance_ratio_[0]:.2%}\")\n#print(f\"PC2: {pca.explained_variance_ratio_[1]:.2%}\")\n#print(f\"Total: {sum(pca.explained_variance_ratio_):.2%}\")\n\nthe first principal component explains 26.95% of the total variance and the second explains 25.08%, for a combined total of 52.03%. The PCA scatter plot maps the job postings onto these two components and shows four clusters formed using K-means (K=4), with each cluster occupying its own region despite some overlap.\n\n\n\n\n\n\nFigure 2: PCA for the Job Posting Data"
  },
  {
    "objectID": "ml.html#classification-role-type-prediction",
    "href": "ml.html#classification-role-type-prediction",
    "title": "Machine Learning Methods",
    "section": "5 Classification: Role Type Prediction",
    "text": "5 Classification: Role Type Prediction\nUnderstanding the distinguishing characteristics of different role types can help job seekers tailor their applications and skill development.\n\n5.1 Create Role Categories\n\ndef categorize_role(title):\n    \"\"\"Categorize job titles into BA, DS, ML, or Data Analytics\"\"\"\n    if pd.isna(title):\n        return 'Other'\n    title_lower = str(title).lower()\n    \n    if any(word in title_lower for word in ['business analyst', 'business intelligence']):\n        return 'Business Analytics'\n    elif any(word in title_lower for word in ['machine learning', 'ml engineer', 'ai engineer']):\n        return 'Machine Learning'\n    elif any(word in title_lower for word in ['data scientist', 'data science']):\n        return 'Data Science'\n    elif any(word in title_lower for word in ['data analyst', 'data analytics']):\n        return 'Data Analytics'\n    else:\n        return 'Other'\n\n# Apply categorization\ndf_filtered['ROLE_CATEGORY'] = df_filtered['TITLE_NAME'].apply(categorize_role)\n\n# Filter to main categories\nmain_categories = ['Business Analytics', 'Data Science', 'Machine Learning', 'Data Analytics']\ndf_clf = df_filtered[df_filtered['ROLE_CATEGORY'].isin(main_categories)].copy()\n\n#print(f\"Classification dataset: {len(df_clf):,} samples\")\n#print(\"\\nRole distribution:\")\nrole_dist = df_clf['ROLE_CATEGORY'].value_counts()\n#print(role_dist)\n#print(\"\\nPercentages:\")\n#print(role_dist / len(df_clf) * 100)\nrole_dist.to_csv(\"./_output/Role_Categories.csv\")\nprint(role_dist)\n\nROLE_CATEGORY\nData Analytics        11944\nBusiness Analytics     1776\nData Science            419\nMachine Learning          4\nName: count, dtype: int64\n\n\n\nclass_sum = pd.read_csv(\"./_output/Role_Categories.csv\")\n\nclass_sum.style.hide(axis=\"index\")\n\n\n\n\n\n\nROLE_CATEGORY\ncount\n\n\n\n\nData Analytics\n11944\n\n\nBusiness Analytics\n1776\n\n\nData Science\n419\n\n\nMachine Learning\n4\n\n\n\n\n\n\n\n5.2 Prepare Classification Features\n\n# Get top states\ntop_states = df_clf['STATE_NAME'].value_counts().head(10).index\n\n# Prepare features for classification\nclf_feature_cols = ['EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE', 'AVG_SALARY']\n\n# Add state features\nfor state in top_states:\n    col_name = f'STATE_{state}'\n    df_clf[col_name] = (df_clf['STATE_NAME'] == state).astype(int)\n    clf_feature_cols.append(col_name)\n\n# Prepare X and y\nX_clf = df_clf[clf_feature_cols].fillna(0)\ny_clf = df_clf['ROLE_CATEGORY']\n\n#print(f\"Classification features: {len(clf_feature_cols)}\")\n#print(f\"Samples per class:\")\n#print(y_clf.value_counts())\n\n# Save feature columns as DataFrame\npd.DataFrame(clf_feature_cols, columns=['feature']).to_csv(\"./_output/clf_feature_cols.csv\", index=False)\nprint(clf_feature_cols)\n\n# Train-test split\nX_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n    X_clf, y_clf, test_size=0.3, random_state=42, stratify=y_clf\n)\n\n# Scale features\nscaler_clf = StandardScaler()\nX_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)\nX_test_clf_scaled = scaler_clf.transform(X_test_clf)\n\n['EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE', 'AVG_SALARY', 'STATE_California', 'STATE_Texas', 'STATE_Virginia', 'STATE_New York', 'STATE_Illinois', 'STATE_Florida', 'STATE_Ohio', 'STATE_Georgia', 'STATE_North Carolina', 'STATE_New Jersey']\n\n\n\nclf_feature = pd.read_csv(\"./_output/clf_feature_cols.csv\")\n\nclf_feature.style.hide(axis=\"index\")\n\n\n\n\n\n\nfeature\n\n\n\n\nEXPERIENCE_YEARS\n\n\nDURATION_DAYS\n\n\nIS_REMOTE\n\n\nAVG_SALARY\n\n\nSTATE_California\n\n\nSTATE_Texas\n\n\nSTATE_Virginia\n\n\nSTATE_New York\n\n\nSTATE_Illinois\n\n\nSTATE_Florida\n\n\nSTATE_Ohio\n\n\nSTATE_Georgia\n\n\nSTATE_North Carolina\n\n\nSTATE_New Jersey\n\n\n\n\n\n\n\n5.3 Logistic Regression Classification\n\n# Train logistic regression\nlr = LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial')\nlr.fit(X_train_clf_scaled, y_train_clf)\ny_pred_lr = lr.predict(X_test_clf_scaled)\ny_pred_proba_lr = lr.predict_proba(X_test_clf_scaled)\n\n# Calculate metrics\nacc_lr = accuracy_score(y_test_clf, y_pred_lr)\nf1_lr = f1_score(y_test_clf, y_pred_lr, average='weighted')\n\n#print(\"LOGISTIC REGRESSION CLASSIFICATION\")\n#print(\"=\" * 50)\n#print(f\"Accuracy: {acc_lr:.4f} ({acc_lr*100:.2f}%)\")\n#print(f\"F1 Score (Weighted): {f1_lr:.4f}\")\n#print(\"\\nClassification Report:\")\n#print(classification_report(y_test_clf, y_pred_lr))\n\n# Save metrics as DataFrames\npd.DataFrame({'accuracy': [acc_lr]}).to_csv(\"./_output/accuracy.csv\", index=False)\npd.DataFrame({'f1_score': [f1_lr]}).to_csv(\"./_output/f1.csv\", index=False)\n#print(f\"Accuracy: {acc_lr:.4f}\")\n#print(f\"F1 Score: {f1_lr:.4f}\")\n\n\nacc = pd.read_csv(\"./_output/accuracy.csv\")\nacc.style.hide(axis=\"index\")\n\n\n\n\n\n\naccuracy\n\n\n\n\n0.841150\n\n\n\n\n\n\nf1 = pd.read_csv(\"./_output/f1.csv\")\nf1.style.hide(axis=\"index\")\n\n\n\n\n\n\nf1_score\n\n\n\n\n0.774877\n\n\n\n\n\nThe logistic regression model reaches 84% accuracy, but this is mainly because it predicts most entries as “Data Analytics,” the largest class in the dataset. While the model performs well for this category, it struggles to recognize smaller roles like Business Analytics, Data Science, and Machine Learning, which show very low recall and F1-scores. This imbalance means the model is not effectively distinguishing minority roles and is primarily learning from the dominant class rather than providing balanced prediction\n\n\n5.4 Random Forest Classification\n\n# Train random forest classifier\nrf_clf = RandomForestClassifier(n_estimators=100, max_depth=15, \n                                min_samples_split=10, random_state=42, n_jobs=-1)\nrf_clf.fit(X_train_clf, y_train_clf)\ny_pred_rf_clf = rf_clf.predict(X_test_clf)\ny_pred_proba_rf = rf_clf.predict_proba(X_test_clf)\n\n# Calculate metrics\nacc_rf_clf = accuracy_score(y_test_clf, y_pred_rf_clf)\nf1_rf_clf = f1_score(y_test_clf, y_pred_rf_clf, average='weighted')\n\n#print(\"RANDOM FOREST CLASSIFICATION\")\n#print(\"=\" * 50)\n#print(f\"Accuracy: {acc_rf_clf:.4f} ({acc_rf_clf*100:.2f}%)\")\n#print(f\"F1 Score (Weighted): {f1_rf_clf:.4f}\")\n#print(\"\\nClassification Report:\")\n#print(classification_report(y_test_clf, y_pred_rf_clf))\n\n# Save metrics as DataFrames\npd.DataFrame({'accuracy': [acc_rf_clf]}).to_csv(\"./_output/accuracy_rf.csv\", index=False)\npd.DataFrame({'f1_score': [f1_rf_clf]}).to_csv(\"./_output/f1_rf.csv\", index=False)\n#print(f\"Accuracy: {acc_rf_clf:.4f}\")\n#print(f\"F1 Score: {f1_rf_clf:.4f}\")\n\n\nacc_random = pd.read_csv(\"./_output/accuracy_rf.csv\")\nacc_random.style.hide(axis=\"index\")\n\n\n\n\n\n\naccuracy\n\n\n\n\n0.856469\n\n\n\n\n\n\nf1_random = pd.read_csv(\"./_output/f1_rf.csv\")\nf1_random.style.hide(axis=\"index\")\n\n\n\n\n\n\nf1_score\n\n\n\n\n0.814503\n\n\n\n\n\nThe model reaches a high overall accuracy of 85.6%, but this is influenced by the extreme class imbalance in the dataset. It predicts the dominant Data Analytics category very well, yet performs poorly on the smaller groups -Business Analytics, Data Science, and Machine Learning which is leading to a low F1 score of 0.33. This shows that the model is not generalizing effectively across all role types. To achieve more balanced and reliable results, techniques such as oversampling, class weighting, or rebalancing the dataset would be needed.\n\n\n5.5 Classification Model Comparison\n\n#### ROC Curves - Logistic Regression\n#| fig-cap: \"Logistic Regression ROC Curves\"\n#| echo: true\n#| eval: true\n\n# Binarize labels for ROC curve\nclasses = lr.classes_\ny_test_bin = label_binarize(y_test_clf, classes=classes)\nn_classes = len(classes)\n\n# Color palette for classes\ncolors = cycle(['#E91E63', '#9B59B6', '#F44336', '#2196F3'])\n\n# Create figure\nplt.figure(figsize=(8, 6))\n\nfor i, color, class_name in zip(range(n_classes), colors, classes):\n    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba_lr[:, i])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, color=color, lw=2.5, \n            label=f'{class_name} (AUC = {roc_auc:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2, label='Chance (AUC = 0.500)')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\nplt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\nplt.title('Logistic Regression: ROC Curves', fontsize=14, fontweight='bold', pad=15)\nplt.legend(loc=\"lower right\", fontsize=10, frameon=True, fancybox=True, shadow=True)\nplt.grid(True, alpha=0.3, linestyle='--')\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.tight_layout()\nplt.savefig(\"./_output/roc_lr.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\nThe ROC curves show that the logistic regression model has moderate ability to distinguish between the different role categories, with AUC scores ranging from 0.58 to 0.77. Machine Learning achieves the highest AUC (0.775), suggesting the model can separate this class reasonably well despite its tiny sample size, while Data Science has the weakest separability (0.585). Overall, the curves indicate that the classifier performs above random chance for all roles but still struggles to clearly differentiate between them, reflecting the impact of class imbalance and overlapping feature patterns.\n\n5.5.1 ROC Curves - Random Forest\n\n# Create figure\nplt.figure(figsize=(8, 6))\n\ncolors = cycle(['#E91E63', '#9B59B6', '#F44336', '#2196F3'])\nfor i, color, class_name in zip(range(n_classes), colors, classes):\n    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba_rf[:, i])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, color=color, lw=2.5, \n            label=f'{class_name} (AUC = {roc_auc:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2, label='Chance (AUC = 0.500)')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\nplt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\nplt.title('Random Forest: ROC Curves', fontsize=14, fontweight='bold', pad=15)\nplt.legend(loc=\"lower right\", fontsize=10, frameon=True, fancybox=True, shadow=True)\nplt.grid(True, alpha=0.3, linestyle='--')\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.tight_layout()\nplt.savefig(\"./_output/roc_rf.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\nRandom Forest ROC Curves\n\n\n\n\nThe Random Forest model shows improved class separability compared to logistic regression, with AUC values ranging from 0.60 to 0.84. Machine Learning achieves the strongest performance (AUC = 0.842), indicating the model can distinguish this role well despite its tiny sample size. Business Analytics and Data Analytics also show moderate discrimination, while Data Science remains the most challenging class, reflecting overlapping features and limited data representation.\n\n\n5.5.2 Model Performance Comparison\n\ncomparison_df = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Random Forest'],\n    'Accuracy': [acc_lr, acc_rf_clf],\n    'F1 Score': [f1_lr, f1_rf_clf]\n})\n\nplt.figure(figsize=(8, 6))\nx = np.arange(len(comparison_df['Model']))\nwidth = 0.35\n\nbars1 = plt.bar(x - width/2, comparison_df['Accuracy'], width, \n               label='Accuracy', color='#E91E63', alpha=0.8, edgecolor='white', linewidth=1.5)\nbars2 = plt.bar(x + width/2, comparison_df['F1 Score'], width, \n               label='F1 Score', color='#9B59B6', alpha=0.8, edgecolor='white', linewidth=1.5)\n\nplt.ylabel('Score', fontsize=12, fontweight='bold')\nplt.xlabel('Model', fontsize=12, fontweight='bold')\nplt.title('Model Performance Comparison', fontsize=14, fontweight='bold', pad=15)\nplt.xticks(x, comparison_df['Model'])\nplt.legend(fontsize=10, frameon=True, fancybox=True, shadow=True)\nplt.ylim([0, 1.1])\nplt.grid(True, alpha=0.3, axis='y', linestyle='--')\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Add value labels on bars\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(\"./_output/model_comparison.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\nModel Performance Comparison\n\n\n\n\nThe comparison shows that Random Forest outperforms Logistic Regression, achieving both higher accuracy (85.6%) and a stronger F1 score (0.815). This indicates that Random Forest handles the complex and imbalanced role categories more effectively. Overall, while both models perform well, Random Forest delivers more balanced and reliable predictions across the dataset.\n\n\n5.5.3 Feature Importance Analysis\n\nclf_importance = pd.DataFrame({\n    'Feature': clf_feature_cols,\n    'Importance': rf_clf.feature_importances_\n}).sort_values('Importance', ascending=False).head(10)\n\nplt.figure(figsize=(8, 6))\nbars = plt.barh(range(len(clf_importance)), clf_importance['Importance'], \n               color=['#E91E63', '#9B59B6', '#F44336', '#2196F3'] * 3, \n               alpha=0.8, edgecolor='white', linewidth=1.5)\nplt.yticks(range(len(clf_importance)), clf_importance['Feature'])\nplt.xlabel('Feature Importance', fontsize=12, fontweight='bold')\nplt.title('Top 10 Predictive Features (Random Forest)', fontsize=14, fontweight='bold', pad=15)\nplt.grid(True, alpha=0.3, axis='x', linestyle='--')\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Add value labels\nfor i, bar in enumerate(bars):\n    width = bar.get_width()\n    plt.text(width, bar.get_y() + bar.get_height()/2.,\n            f'{width:.3f}', ha='left', va='center', fontsize=9, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(\"./_output/feature_importance.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\nTop 10 Predictive Features (Random Forest)\n\n\n\n\nThe feature importance results show that *experience years, average salary, and job duration are the strongest predictors in distinguishing between BA, Data Science, ML, and Data Analytics roles. Remote status contributes modestly, while location-based features (state variables) have minimal impact, indicating that job role differences are driven more by skill level and job characteristics than by geography. Overall, the model relies most heavily on experience and salary patterns to differentiate job categories."
  },
  {
    "objectID": "Skill_Gap_Analysis.html",
    "href": "Skill_Gap_Analysis.html",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "This analysis compares our team’s current skill levels against the skills demanded in the IT job market. By identifying gaps between our capabilities and market requirements, we can develop targeted learning strategies to enhance our competitiveness as job candidates.\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing 59,220 job postings for skill requirements"
  },
  {
    "objectID": "Skill_Gap_Analysis.html#introduction",
    "href": "Skill_Gap_Analysis.html#introduction",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "This analysis compares our team’s current skill levels against the skills demanded in the IT job market. By identifying gaps between our capabilities and market requirements, we can develop targeted learning strategies to enhance our competitiveness as job candidates.\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing 59,220 job postings for skill requirements"
  },
  {
    "objectID": "Skill_Gap_Analysis.html#team-skill-assessment",
    "href": "Skill_Gap_Analysis.html#team-skill-assessment",
    "title": "Skill Gap Analysis",
    "section": "2 1. Team Skill Assessment",
    "text": "2 1. Team Skill Assessment\n\n2.1 1.1 Creating Team Skills Profile\nEach team member rates their proficiency in key IT skills on a scale of 1-5: - 1 = Beginner (aware of the skill) - 2 = Basic knowledge (can perform simple tasks) - 3 = Intermediate (comfortable with common scenarios) - 4 = Advanced (can handle complex problems) - 5 = Expert (can teach others and solve any problem)\nThe team’s strongest skills are *Excel, Python, and Data Analysis, showing solid readiness for analytical and data-focused roles. Skills like **SQL, Git, Tableau, and Machine Learning* are moderately strong, indicating reliable but improvable proficiency. Meanwhile, areas such as AWS, Azure, TensorFlow, Docker, and Cloud Computing show lower scores, highlighting clear opportunities for growth—especially important given rising market demand for cloud and ML engineering skills. Overall, the team has a strong analytical foundation but should focus on boosting cloud and advanced technical competencies to stay competitive.\n\n\n2.2 1.2 Team Skills Heatmap\nVisualize each team member’s strengths and weaknesses across all skills.\n\n\n\n\n\n\n  \n\n\n\n\nThis heatmap shows how strong each team member is across key tech skills like AWS, Excel, SQL, Machine Learning, and more. The greener the box, the stronger the skill—red means the weakest. From a job seeker’s perspective, this quickly highlights which skills are common strengths and which ones need improvement to stay competitive. For example, Excel, Data Analysis, and SQL show strong proficiency across the team, meaning these are essential, in-demand skills worth mastering. Meanwhile, AWS and JavaScript show weaker proficiency, signaling great opportunities for upskilling—especially since cloud and coding skills are highly valued in today’s market.\n\n\n2.3 1.3 Team Average Skills Bar Chart\nCompare team average proficiency across all skills.\n\n\n\n\n\n\n  \n\n\n\n\nThe team’s top 10 skills show strong proficiency in high-value areas like *Excel, Data Analysis, Python, Git, and SQL, which are core requirements for most analytics and tech roles. Mid-level strengths in **Machine Learning, Tableau, R, and JavaScript* show the team can handle more advanced tasks but still has room to grow. Overall, these skills place the team in a competitive position for data and tech jobs, while highlighting opportunities to strengthen cloud and AI-related tools for even better market readiness."
  },
  {
    "objectID": "Skill_Gap_Analysis.html#market-skill-demand-analysis",
    "href": "Skill_Gap_Analysis.html#market-skill-demand-analysis",
    "title": "Skill Gap Analysis",
    "section": "3 2. Market Skill Demand Analysis",
    "text": "3 2. Market Skill Demand Analysis\n\n3.1 2.1 Extracting Skills from Job Descriptions\nWe analyze job postings to identify the most in-demand skills in the IT job market.\nThe skill extraction shows that Data Analysis, Cloud Computing, and TensorFlow are the most frequently requested skills in job postings, making them top priorities for job seekers. Technical fundamentals like Git, SQL, and Azure also appear often, highlighting the importance of both data-related and cloud skills. Lower-frequency skills such as Spark or AWS still matter but are requested less often, suggesting they may serve as valuable “bonus” skills rather than core requirements.\n\n\n3.2 2.2 Top In-Demand Skills Visualization\n\n\n\n\n\n\n  \n\n\n\n\nThe chart shows that Data Analysis and Cloud Computing are the most in-demand skills in the 2024 job market, with far more postings than any other skill. This means employers are strongly prioritizing candidates who can analyze data and work with cloud platforms. Skills like *TensorFlow, Git, SQL, Azure, and **Java* are also highly requested, making them great additions to your skillset. Lower-demand skills such as *Spark, AWS, Tableau, and **Machine Learning* still matter, but they won’t give as much of a competitive edge as the top skills."
  },
  {
    "objectID": "Skill_Gap_Analysis.html#skill-gap-identification",
    "href": "Skill_Gap_Analysis.html#skill-gap-identification",
    "title": "Skill Gap Analysis",
    "section": "4 3. Skill Gap Identification",
    "text": "4 3. Skill Gap Identification\n\n4.1 3.1 Comparing Team Skills to Market Demand\nNow we identify gaps between our team’s capabilities and what the market requires.\nThe analysis shows that our team is *strong in most core skills, especially Python, SQL, Tableau, Machine Learning, and Git — all areas where market demand is relatively low to moderate. However, there are **two skills with clear gaps* compared to what employers want:\n\nCloud Computing (e.g., AWS, Azure)\nData Analysis\n\nThese areas are in very high demand, but our team’s proficiency is lower than what the job market expects. Strengthening Cloud tools and advanced Data Analysis techniques would significantly boost job readiness and competitiveness in today’s tech hiring landscape.\n\n\n4.2 3.2 Skill Gap Visualization\n\n\n\n\n\n\n  \n\n\n\n\nThe chart clearly shows that market demand is much higher than the team’s current skill levels across nearly all skills. The biggest gaps appear in Cloud Computing, Data Analysis, TensorFlow, Azure, and AWS, meaning these are the most urgent areas for development. Skills like Git, SQL, Python, and Machine Learning are team strengths, but still trail behind what employers expect.\nFor job seekers like us, this means focusing on cloud technologies, advanced analytics, and AI frameworks will significantly boost competitiveness and align better with real market needs.\n\n\n4.3 3.3 Skill Gap Priority Matrix\nIdentify which skills need immediate attention based on gap size.\n\n\n\n\n\n\n  \n\n\n\n\nThe matrix helps identify which skills your team should focus on first.\n\nGreen (Moderate Gap): These skills—like Cloud Computing, Data Analysis, and TensorFlow—are high in market demand but lower in team proficiency, meaning they should be top priority for upskilling.\nOrange (Strength): Most other skills fall in this category. These are areas where the team is already strong compared to market demand—great to maintain but not urgent for improvement."
  },
  {
    "objectID": "Skill_Gap_Analysis.html#individual-skill-gap-analysis",
    "href": "Skill_Gap_Analysis.html#individual-skill-gap-analysis",
    "title": "Skill Gap Analysis",
    "section": "5 4. Individual Skill Gap Analysis",
    "text": "5 4. Individual Skill Gap Analysis\n\n5.1 4.1 Skill Gaps by Team Member\nIdentify personalized skill development needs for each team member.\nach team member has two main skill gaps: Cloud Computing and Data Analysis.\nTuba Anwar needs improvement mainly in Cloud Computing, with a smaller gap in Data Analysis.\nKriti Singh also shows the same pattern—Cloud Computing is the biggest gap, followed by Data Analysis.\nSoham Deshkhaires has the highest gap in Data Analysis, and a moderate gap in Cloud Computing.\nOverall Insight: All three team members share the same critical areas for improvement. Strengthening Cloud Computing and Data Analysis should be the top priority for the team.\n\n\n5.2 4.2 Individual Gap Visualization\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n  \n\n\n\n\nAcross all three members—Tuba, Kriti, and Soham—the largest skill gap is in Cloud Computing, meaning the market demands this skill at a much higher level than the team currently possesses. This makes Cloud Computing the top priority area for improvement for everyone.\nAdditionally, Data Analysis appears as a major gap for both Tuba and Kriti, while Soham shows a moderate gap in Cloud Computing only. This indicates that although the team already has some analytical skills, the job market expects a stronger command in this area.\nThe charts highlight two urgent development needs for the team:\nCloud Computing → biggest gap for all members\nData Analysis → second-highest gap for Tuba & Kriti\nFocusing training efforts on these skills will significantly improve alignment with market expectations.individual"
  },
  {
    "objectID": "Skill_Gap_Analysis.html#skill-development-plan",
    "href": "Skill_Gap_Analysis.html#skill-development-plan",
    "title": "Skill Gap Analysis",
    "section": "6 5. Skill Development Plan",
    "text": "6 5. Skill Development Plan\n\n6.1 5.1 Priority Skills for Team Development\nBased on our analysis, here are the priority skills the team should focus on:\nThe analysis shows that the team has *no critical skill gaps, meaning no skill is urgently below market expectations. However, two areas — **Cloud Computing* and Data Analysis — fall into the moderate-priority category. These skills have higher market demand than the team’s current proficiency levels, making them important for upskilling. Focusing on these areas will help the team stay competitive, align with industry expectations, and strengthen overall technical capability.\n\n\n6.2 5.3 Team Collaboration Strategy\nHow can team members help each other?\nThe team collaboration strategy highlights how members can support one another by leveraging individual strengths to address skill gaps. Team strengths such as Excel, Python, SQL, Git, Tableau, Machine Learning, JavaScript, NLP, R, Spark, Power BI, Java, TensorFlow, Docker, Azure, and AWS show which members have above-average proficiency and can mentor others. Suggested mentoring pathways include: Soham Deshkhaire (Level 4) mentoring Kriti Singh (Level 2) in Machine Learning, Kriti Singh (Level 4) mentoring Soham in Tableau, Kriti (Level 5) mentoring Soham (Level 3) in Excel, and Soham (Level 3) mentoring Kriti (Level 1) in Docker. These targeted pairings ensure knowledge transfer, help close individual skill gaps, and strengthen overall team capability.\n\n\n6.3 5.4 3-Month, 6-Month, and 1-Year Goals\nCreate timeline for skill development:\n\n\n\n\n\n\nTipSkill Development Timeline\n\n\n\n3-Month Goals (Immediate Priority) - Focus on critical gap skills with highest market demand - Complete foundational courses in AWS, Docker, and Cloud Computing - Build 1-2 hands-on projects demonstrating new skills - Team members mentor each other in strength areas\n6-Month Goals (Intermediate) - Achieve intermediate proficiency (Level 3) in all critical gap skills - Complete advanced courses in Machine Learning and Data Analysis - Participate in Kaggle competitions or contribute to open-source projects - Earn 1-2 professional certifications (e.g., AWS Certified Developer)\n1-Year Goals (Advanced) - Achieve advanced proficiency (Level 4) in priority skills - Entire team reaches minimum Level 3 in all high-demand market skills - Build comprehensive portfolio showcasing technical competencies - Competitive job candidates for targeted IT roles"
  },
  {
    "objectID": "Skill_Gap_Analysis.html#references",
    "href": "Skill_Gap_Analysis.html#references",
    "title": "Skill Gap Analysis",
    "section": "7 References",
    "text": "7 References\n\nJob market data: Lightcast Job Postings Dataset (2024)\nSkill assessment framework: Industry-standard proficiency scales\nLearning resources: Coursera, Udemy, AWS Training, Kaggle\nAnalysis tools: Python, pandas, hvPlot, Panel"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This document presents a comprehensive exploratory data analysis of the 2024 job market. We examine hiring trends, employment types, geographic patterns, and remote work opportunities to provide actionable insights for job seekers and market analysts."
  },
  {
    "objectID": "eda.html#introduction",
    "href": "eda.html#introduction",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This document presents a comprehensive exploratory data analysis of the 2024 job market. We examine hiring trends, employment types, geographic patterns, and remote work opportunities to provide actionable insights for job seekers and market analysts."
  },
  {
    "objectID": "eda.html#job-title-analysis",
    "href": "eda.html#job-title-analysis",
    "title": "Exploratory Data Analysis",
    "section": "2 1. Job Title Analysis",
    "text": "2 1. Job Title Analysis\n\n2.1 1.1 Top In-Demand Job Titles\nUnderstanding which job titles dominate the market helps identify high-demand roles and emerging career opportunities.\n\n\n\nTop job titles : 10 titles\n\n\n\n\n\n\n  \n\n\n\n\nThe Top 10 most in-demand jobs are dominated by Data Analyst roles, which appear far more frequently than any other title. Business Intelligence Analysts, Enterprise Architects, and Data Modelers also rank highly, highlighting strong employer demand for data-focused and technical architecture skills. Overall, the top roles show a clear market emphasis on analytics, data management, and solution-oriented positions in 2024."
  },
  {
    "objectID": "eda.html#employment-type-distribution",
    "href": "eda.html#employment-type-distribution",
    "title": "Exploratory Data Analysis",
    "section": "3 2. Employment Type Distribution",
    "text": "3 2. Employment Type Distribution\n\n3.1 2.1 Full-Time vs Part-Time vs Contract\nUnderstanding employment type distribution helps job seekers target positions matching their career preferences.\n\n\n\n\n\n\n  \n\n\n\n\nMost job postings in 2024 require around 5 years of experience, making it the dominant requirement across roles. Entry-level opportunities (0–2 years) exist but are significantly fewer, showing that employers prioritize mid-level talent with proven skills. Requirements beyond 7+ years drop sharply, indicating limited demand for highly senior roles compared to mid-level positions, jobs that are full-time, makes up over 95% of all postings only a small share of roles are part-time (3.3%) or mixed part-time/full-time (1.4%). This shows that employers mainly prefer hiring for stable, full-time positions."
  },
  {
    "objectID": "eda.html#remote-work-analysis",
    "href": "eda.html#remote-work-analysis",
    "title": "Exploratory Data Analysis",
    "section": "4 3. Remote Work Analysis",
    "text": "4 3. Remote Work Analysis\n\n4.1 3.1 Remote vs Hybrid vs On-Site\nThe prevalence of remote work reflects post-pandemic hiring trends and employer flexibility.\n\n\n\n\n\n\n  \n\n\n\n\nThe dominance of full-time roles in 2024 reflects a strong post-COVID recovery, as companies shift back toward stable, long-term hiring after years of uncertainty. The very small share of part-time and hybrid hour roles suggests that businesses are prioritizing consistent workforce availability to meet rising operational demands. Overall, the trend indicates increased employer confidence and a return to pre-pandemic hiring patterns focused on full-time talent."
  },
  {
    "objectID": "eda.html#geographic-analysis",
    "href": "eda.html#geographic-analysis",
    "title": "Exploratory Data Analysis",
    "section": "5 4. Geographic Analysis",
    "text": "5 4. Geographic Analysis\n\n5.1 4.1 Top States for Job Opportunities\nGeographic distribution shows where job opportunities are concentrated.\n\n\n\n\n\n\n  \n\n\n\n\nThe top 10 states for job postings in 2024 are led by Texas and California, which together dominate the job market with substantially higher opportunities than the rest. States like Virginia, Florida, New York, and Illinois follow, reflecting strong demand in both tech-heavy and fast-growing regional economies. Overall, job opportunities are concentrated in major economic hubs and states with strong corporate, technology, and government sectors."
  },
  {
    "objectID": "eda.html#top-hiring-companies",
    "href": "eda.html#top-hiring-companies",
    "title": "Exploratory Data Analysis",
    "section": "6 5. Top Hiring Companies",
    "text": "6 5. Top Hiring Companies\n\n6.1 5.1 Companies with Most Job Openings\nIdentifying top hiring companies helps job seekers target organizations with multiple opportunities.\n\n\n\n\n\n\n  \n\n\n\n\nThe Top 10 hiring companies in 2024 are led by Deloitte and Accenture, which show significantly higher job postings than other firms,reflecting strong demand for consulting, technology, and analytics roles. Companies like PwC, Insight Global, KPMG, and Lumen Technologies also appear prominently, indicating consistent hiring across both consulting and IT services sectors. Overall, the hiring landscape is dominated by large professional services firms, showcasing continued growth in advisory, digital transformation, and data-driven business roles."
  },
  {
    "objectID": "eda.html#job-posting-timeline",
    "href": "eda.html#job-posting-timeline",
    "title": "Exploratory Data Analysis",
    "section": "7 6. Job Posting Timeline",
    "text": "7 6. Job Posting Timeline\n\n7.1 6.1 When Jobs Are Posted\nUnderstanding posting patterns helps with strategic job search timing. Since job postings peaked in August and September 2024, job seekers should target this late-summer period for applications. Companies appear to increase hiring toward the end of Q3, meaning more openings, faster responses, and higher chances of landing interviews. Preparing resumes, portfolios, and applications ahead of this surge (during May–July) can give job seekers a strategic advantage when the market becomes most active."
  },
  {
    "objectID": "eda.html#experience-requirements",
    "href": "eda.html#experience-requirements",
    "title": "Exploratory Data Analysis",
    "section": "8 7. Experience Requirements",
    "text": "8 7. Experience Requirements\n\n8.1 7.1 Minimum Years of Experience Required\nUnderstanding experience requirements helps assess job market accessibility.\n\n\n\n\n\n\n  \n\n\n\n\nMost job postings in 2024 require around 5 years of experience, making it the dominant requirement across roles. Entry-level opportunities (0–2 years) exist but are significantly fewer, showing that employers prioritize mid-level talent with proven skills. Requirements beyond 7+ years drop sharply, indicating limited demand for highly senior roles compared to mid-level positions."
  },
  {
    "objectID": "eda.html#references",
    "href": "eda.html#references",
    "title": "Exploratory Data Analysis",
    "section": "9 References",
    "text": "9 References\n\nData source: Lightcast Job Postings Dataset (2024)\nVisualization tools: hvPlot, Panel, Python\nAnalysis framework: Standard EDA best practices"
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "This document details the comprehensive data cleaning and preprocessing steps applied to the 2024 job market dataset.\n\n\nInitial dataset shape: (72498, 131)\nTotal rows: 72,498\nTotal columns: 131"
  },
  {
    "objectID": "data_cleaning.html#introduction",
    "href": "data_cleaning.html#introduction",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "This document details the comprehensive data cleaning and preprocessing steps applied to the 2024 job market dataset.\n\n\nInitial dataset shape: (72498, 131)\nTotal rows: 72,498\nTotal columns: 131"
  },
  {
    "objectID": "data_cleaning.html#step-1-removing-redundant-columns",
    "href": "data_cleaning.html#step-1-removing-redundant-columns",
    "title": "Data Cleaning & Exploration",
    "section": "2 Step 1: Removing Redundant Columns",
    "text": "2 Step 1: Removing Redundant Columns\nWe remove redundant columns to improve dataset quality and analysis efficiency.\n\n\nColumns removed: 69\nNew dataset shape: (72498, 62)"
  },
  {
    "objectID": "data_cleaning.html#step-2-handling-missing-values",
    "href": "data_cleaning.html#step-2-handling-missing-values",
    "title": "Data Cleaning & Exploration",
    "section": "3 Step 2: Handling Missing Values",
    "text": "3 Step 2: Handling Missing Values\n\n\nColumns with missing values: 62\n\nTop 10 columns with highest missing percentages:\n                    Column  Missing_Count  Missing_Percentage\n18    MAX_YEARS_EXPERIENCE          64068               88.37\n13           MAX_EDULEVELS          56183               77.50\n14      MAX_EDULEVELS_NAME          56183               77.50\n50       LIGHTCAST_SECTORS          54711               75.47\n51  LIGHTCAST_SECTORS_NAME          54711               75.47\n20                  SALARY          41690               57.51\n3                 DURATION          27316               37.68\n17    MIN_YEARS_EXPERIENCE          23146               31.93\n2                  EXPIRED           7844               10.82\n30       MSA_NAME_INCOMING           3962                5.46\n\n\n\n\nColumns dropped due to &gt;50% missing values: 6\nTotal missing values after imputation: 0"
  },
  {
    "objectID": "data_cleaning.html#step-3-removing-duplicates",
    "href": "data_cleaning.html#step-3-removing-duplicates",
    "title": "Data Cleaning & Exploration",
    "section": "4 Step 3: Removing Duplicates",
    "text": "4 Step 3: Removing Duplicates\n\n\nInitial rows: 72,498\nDuplicate rows detected: 13,278\nFinal rows after duplicate removal: 59,220\nRows removed: 13,278"
  },
  {
    "objectID": "data_cleaning.html#step-4-final-summary",
    "href": "data_cleaning.html#step-4-final-summary",
    "title": "Data Cleaning & Exploration",
    "section": "5 Step 4: Final Summary",
    "text": "5 Step 4: Final Summary\n\n\n             Metric  Value\n         Total Rows 59,220\n      Total Columns     56\n  Numerical Columns     12\nCategorical Columns     44\n     Missing Values      0\n\nCleaned dataset saved as 'cleanedjob_postings.csv'"
  },
  {
    "objectID": "data_cleaning.html#summary",
    "href": "data_cleaning.html#summary",
    "title": "Data Cleaning & Exploration",
    "section": "6 Summary",
    "text": "6 Summary\nThe data cleaning process has successfully prepared the job market dataset:\n\nRemoved redundant columns\nHandled missing values strategically\nRemoved duplicate postings\nFinal clean dataset ready for analysis"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Job Market Analysis 2024- Business Analytics, Data Science, and Machine Learning Trends",
    "section": "",
    "text": "This research explores the evolving landscape of Business Analytics, Data Science and Machine Learning (ML) careers in 2024. As organizations increasingly integrate AI in every function from operations to strategic decision-making, the demand for professionals with strong data and analytics skills is growing rapidly. This study explores four interconnected questions: the most sought-after skills for analytics and ML roles, how job descriptions are changing to require AI literacy, which industries are leading data-driven hiring, and what the short- to medium-term career outlook looks like for analytics professionals. By combining insights from labor market data and recent academic research, this project provides a holistic view of how AI and automation are transforming workforce expectations and employability strategies for students and professionals."
  },
  {
    "objectID": "index.html#project-overview",
    "href": "index.html#project-overview",
    "title": "Job Market Analysis 2024- Business Analytics, Data Science, and Machine Learning Trends",
    "section": "",
    "text": "This research explores the evolving landscape of Business Analytics, Data Science and Machine Learning (ML) careers in 2024. As organizations increasingly integrate AI in every function from operations to strategic decision-making, the demand for professionals with strong data and analytics skills is growing rapidly. This study explores four interconnected questions: the most sought-after skills for analytics and ML roles, how job descriptions are changing to require AI literacy, which industries are leading data-driven hiring, and what the short- to medium-term career outlook looks like for analytics professionals. By combining insights from labor market data and recent academic research, this project provides a holistic view of how AI and automation are transforming workforce expectations and employability strategies for students and professionals."
  },
  {
    "objectID": "index.html#data-source",
    "href": "index.html#data-source",
    "title": "Job Market Analysis 2024- Business Analytics, Data Science, and Machine Learning Trends",
    "section": "5.1 Data Source",
    "text": "5.1 Data Source\nWe will utilize the Lightcast 2024 dataset, which includes: - Job posting volumes for analytics, data science, and ML roles by industry and location - Salary data across data-related positions and geographies - Skill requirements extracted from job descriptions - Hiring trends across time periods - Company size and industry classifications - Emerging role titles and job requirements"
  },
  {
    "objectID": "index.html#analysis-approach",
    "href": "index.html#analysis-approach",
    "title": "Job Market Analysis 2024- Business Analytics, Data Science, and Machine Learning Trends",
    "section": "5.2 Analysis Approach",
    "text": "5.2 Analysis Approach\nOur team will:\n\nClean and preprocess the Lightcast data using Python (pandas, NumPy)\nExtract and categorize skills mentioned in job descriptions for analytics and ML roles\nCompare trends across industries, geographies, and job levels\nAnalyze salary patterns to understand compensation for different skill combinations\nIdentify emerging roles and how job requirements are evolving\nVisualize findings through interactive dashboards using Plotly and Matplotlib\nDevelop career strategy recommendations based on market insights"
  },
  {
    "objectID": "index.html#expected-findings",
    "href": "index.html#expected-findings",
    "title": "Job Market Analysis 2024- Business Analytics, Data Science, and Machine Learning Trends",
    "section": "5.3 Expected Findings",
    "text": "5.3 Expected Findings\nWe anticipate discovering that: - Python, SQL, and cloud platforms (AWS, GCP, Azure) remain foundational skills - Generative AI and prompt engineering are emerging as newly valued competencies - Finance, healthcare, and technology sectors lead in analytics hiring - Soft skills like communication and domain expertise are increasingly emphasized - Analytics roles offer strong job security and career growth potential - Salary premiums exist for ML/AI-specialized roles compared to traditional business analytics"
  },
  {
    "objectID": "index.html#deliverables-future-phases",
    "href": "index.html#deliverables-future-phases",
    "title": "Job Market Analysis 2024- Business Analytics, Data Science, and Machine Learning Trends",
    "section": "5.4 Deliverables (Future Phases)",
    "text": "5.4 Deliverables (Future Phases)\n\nExploratory Data Analysis (EDA) with visualizations\nInteractive dashboards showing skill trends, industry hiring patterns, and salary insights\nCareer pathway recommendations for different specializations\nPersonal career action plans for each team member"
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "Natural Language Processing Methods",
    "section": "",
    "text": "This section applies Natural Language Processing (NLP) techniques to analyze job postings and extract meaningful insights about skills, requirements, and industry trends. By processing structured skills data, we can uncover patterns about the most in-demand competencies.\n\n\nDataset loaded: 59,220 rows, 56 columns\nSkills columns found: ['SKILLS', 'SKILLS_NAME', 'SPECIALIZED_SKILLS', 'SPECIALIZED_SKILLS_NAME', 'COMMON_SKILLS', 'COMMON_SKILLS_NAME', 'SOFTWARE_SKILLS', 'SOFTWARE_SKILLS_NAME']"
  },
  {
    "objectID": "nlp.html#introduction",
    "href": "nlp.html#introduction",
    "title": "Natural Language Processing Methods",
    "section": "",
    "text": "This section applies Natural Language Processing (NLP) techniques to analyze job postings and extract meaningful insights about skills, requirements, and industry trends. By processing structured skills data, we can uncover patterns about the most in-demand competencies.\n\n\nDataset loaded: 59,220 rows, 56 columns\nSkills columns found: ['SKILLS', 'SKILLS_NAME', 'SPECIALIZED_SKILLS', 'SPECIALIZED_SKILLS_NAME', 'COMMON_SKILLS', 'COMMON_SKILLS_NAME', 'SOFTWARE_SKILLS', 'SOFTWARE_SKILLS_NAME']"
  },
  {
    "objectID": "nlp.html#data-preparation",
    "href": "nlp.html#data-preparation",
    "title": "Natural Language Processing Methods",
    "section": "2 1. Data Preparation",
    "text": "2 1. Data Preparation\n\n2.1 1.1 Skills Data Overview\n\n\n✓ Successfully processed: 59,220 job postings with skills\n✓ Average skills text length: 1204 characters"
  },
  {
    "objectID": "nlp.html#keyword-extraction",
    "href": "nlp.html#keyword-extraction",
    "title": "Natural Language Processing Methods",
    "section": "3 2. Keyword Extraction",
    "text": "3 2. Keyword Extraction\n\n3.1 2.1 Top 10 Most Common Skills\n\n\n\n\n\n\n\n\n\nTop 10 Skills Insight:\nSAP Applications dominates with 1,114 postings, reflecting the strong demand for enterprise resource planning expertise in large organizations. Oracle Cloud (667) and Microsoft Office (637) follow, showing that cloud platforms and productivity tools remain essential. Communication (532) appears as the top soft skill, emphasizing that technical roles require strong interpersonal abilities. The data reveals a mix of enterprise software (SAP, Oracle Cloud), business analysis capabilities (Data Analysis, Dashboard, Project Management), and emerging areas like Cyber Security and UX Design. Job seekers should prioritize SAP expertise if targeting enterprise roles, while building a foundation in Microsoft Office, data analysis, and communication skills for broader market appeal."
  },
  {
    "objectID": "nlp.html#technical-skills-analysis",
    "href": "nlp.html#technical-skills-analysis",
    "title": "Natural Language Processing Methods",
    "section": "4 3. Technical Skills Analysis",
    "text": "4 3. Technical Skills Analysis\n\n4.1 3.1 Top 10 Software & Technical Skills\n\n\n\n\n\n\n\n\n\nTop 10 Software Skills - Job Seeker Insights:\nSAP Applications leads dramatically with 934 postings, making it the single most valuable software skill for enterprise-focused careers. Oracle Cloud (633) and Microsoft Office (602) demonstrate the importance of cloud infrastructure and productivity suites. Dashboard skills (433) reflect the growing need for data visualization capabilities across all roles. SQL Programming Language (353) and Microsoft Excel (282) remain fundamental for data manipulation and analysis. The presence of specialized tools like Anaplan (196), Onestream CPM Software (196), and Oracle E-Business Suite (173) indicates niche opportunities in financial planning and enterprise systems. Job seekers should master SAP for enterprise roles, Excel and SQL for data work, and consider specializing in emerging tools like Anaplan for competitive advantage."
  },
  {
    "objectID": "nlp.html#specialized-skills-analysis",
    "href": "nlp.html#specialized-skills-analysis",
    "title": "Natural Language Processing Methods",
    "section": "5 4. Specialized Skills Analysis",
    "text": "5 4. Specialized Skills Analysis\n\n5.1 4.1 Top 10 Specialized Skills\n\n\n\n\n\n\n\n\n\nTop 10 Specialized Skills - Job Seeker Insights:\nUser Experience (UX) Design leads with 260 postings, highlighting the critical importance of user-centered design in modern product development. Data Analysis (220) and Cloud Computing (196) show strong demand for analytical and cloud architecture expertise. Emergency Response (143) and Pivot Tables and Charts (125) represent specialized operational and analytical capabilities. SAP Applications (104) appears again, reinforcing its enterprise value. Microsoft Access (89), Databricks (85), Mulesoft (85), and Sales Process (74) round out the list, showing diverse specializations from database management to integration platforms and CRM. Job seekers should prioritize UX design for product roles, data analysis for analytical positions, and cloud computing for infrastructure careers, while considering niche specializations like Databricks or Mulesoft for premium positioning."
  },
  {
    "objectID": "nlp.html#skill-requirements-by-job-title",
    "href": "nlp.html#skill-requirements-by-job-title",
    "title": "Natural Language Processing Methods",
    "section": "6 5. Skill Requirements by Job Title",
    "text": "6 5. Skill Requirements by Job Title\n\n6.1 5.1 Skills vs Top Job Titles - Heatmap\n\n\nCreating heatmap...\nTop job titles: ['Data Analysts', 'Unclassified', 'Business Intelligence Analysts', 'Enterprise Architects', 'Data Modelers', 'Data Governance Analysts', 'Oracle Cloud HCM Consultants', 'Solutions Architects']\nTop software skills: ['sap applications', 'oracle cloud', 'microsoft office', 'dashboard', 'sql programming language', 'microsoft excel', 'microsoft powerpoint,\\n  microsoft excel', 'oracle e-business suite']\nMatrix created with 8 rows\nDataframe shape: (8, 8)\nColumns: ['Sap Applications', 'Oracle Cloud', 'Microsoft Office', 'Dashboard', 'Sql Programming Lang', 'Microsoft Excel', 'Microsoft Powerpoint', 'Oracle E-Business Su']\n\n\n\n\n\n\n\n\n\nHeatmap displayed successfully!\n\n\nSkills vs Job Titles - Job Seeker Insights:\nThe heatmap reveals distinct skill patterns across job roles. Unclassified positions show the highest demand for general software skills (37.2% require SAP Applications), suggesting broad technical requirements. Oracle Cloud HCM Consultants have specialized focus with 11.1% requiring Oracle Cloud expertise. Data Governance Analysts need the most diverse skillset with strong SQL Programming Language (13.8%) and moderate requirements across Dashboard, Microsoft Office, and SAP. Enterprise Architects and Solutions Architects show concentrated needs in SAP (10.5% and 8.5% respectively) with minimal other software requirements. Data Modelers uniquely emphasize SQL Programming (2.1%) over other tools. The low percentages overall indicate that most roles don’t mandate specific software, creating opportunities for candidates to differentiate through technical mastery. Job seekers should target SAP for enterprise roles, Oracle Cloud for HCM consulting, and SQL for data-focused positions."
  },
  {
    "objectID": "nlp.html#summary",
    "href": "nlp.html#summary",
    "title": "Natural Language Processing Methods",
    "section": "7 Summary",
    "text": "7 Summary\n\n\n\n\n\n\nImportantNLP Analysis Key Findings\n\n\n\nSkills Analysis Results:\n\n✓ Successfully analyzed skills from structured data columns\n✓ Identified top 10 most common skills across all job postings\n✓ Extracted and ranked top 10 software/technical skills\n✓ Analyzed top 10 specialized skills for advanced roles\n✓ Cross-referenced skills with job title requirements via heatmap\n\nKey Insights:\n\nSpecific skills dominate the job market across different categories\nSoftware skills are critical technical differentiators\nSpecialized skills offer opportunities for premium positioning\nSkills requirements vary significantly by job title\n\nRecommendations for Job Seekers:\n\nPrioritize learning the top 10 skills identified in each category\nDevelop proficiency in the most demanded software tools\nBuild specialized skills for senior or expert-level opportunities\nFocus on developing complementary skills that frequently co-occur\nUse the heatmap to understand skill priorities for your target job titles\nTailor your resume to highlight relevant skill combinations"
  },
  {
    "objectID": "nlp.html#references",
    "href": "nlp.html#references",
    "title": "Natural Language Processing Methods",
    "section": "8 References",
    "text": "8 References\n\nPandas Documentation: https://pandas.pydata.org/\nSeaborn Documentation: https://seaborn.pydata.org/\nCollections Counter: https://docs.python.org/3/library/collections.html#collections.Counter"
  },
  {
    "objectID": "data_analysis.html",
    "href": "data_analysis.html",
    "title": "Data Analysis",
    "section": "",
    "text": "This document provides a comprehensive analysis of job market data for 2024. We perform data cleaning, handle missing values, remove duplicates, and conduct exploratory data analysis to understand hiring trends, salary distributions, and job market dynamics."
  },
  {
    "objectID": "data_analysis.html#introduction",
    "href": "data_analysis.html#introduction",
    "title": "Data Analysis",
    "section": "",
    "text": "This document provides a comprehensive analysis of job market data for 2024. We perform data cleaning, handle missing values, remove duplicates, and conduct exploratory data analysis to understand hiring trends, salary distributions, and job market dynamics."
  },
  {
    "objectID": "data_analysis.html#step-1-removing-redundant-columns",
    "href": "data_analysis.html#step-1-removing-redundant-columns",
    "title": "Data Analysis",
    "section": "2 Step 1: Removing Redundant Columns",
    "text": "2 Step 1: Removing Redundant Columns\n\n2.1 Why Remove These Columns?\nWe remove redundant columns to improve dataset quality and analysis efficiency. Specifically:\n\nTracking & Administrative Columns: ID, URL, DUPLICATES, LAST_UPDATED_TIMESTAMP - These are metadata and don’t contribute to analysis\nRaw Text Fields: BODY, TITLE_RAW, COMPANY_RAW - We have cleaned versions (TITLE, COMPANY_NAME, TITLE_CLEAN)\nDeprecated Classifications: We keep only the latest NAICS_2022_6, SOC_2021_4, and CIP4 standards\nDuplicate Geographic Fields: Multiple versions of county/MSA data create redundancy"
  },
  {
    "objectID": "data_analysis.html#step-2-handling-missing-values",
    "href": "data_analysis.html#step-2-handling-missing-values",
    "title": "Data Analysis",
    "section": "3 Step 2: Handling Missing Values",
    "text": "3 Step 2: Handling Missing Values\n\n3.1 Understanding Missing Data\nBefore imputation, let’s visualize where data is missing:\n\n\n3.2 Missing Value Imputation Strategy\nWe applied a strategic approach to handle missing data:\n\nDropped columns with more than 50% missing values\nFilled numerical columns with median values to maintain distribution\nFilled categorical columns with “Unknown” for clarity"
  },
  {
    "objectID": "data_analysis.html#step-3-removing-duplicates",
    "href": "data_analysis.html#step-3-removing-duplicates",
    "title": "Data Analysis",
    "section": "4 Step 3: Removing Duplicates",
    "text": "4 Step 3: Removing Duplicates\n\n4.1 Why Remove Duplicates?\nDuplicate job postings skew our analysis. We remove them based on job title, company, and location to ensure each unique position is counted only once."
  },
  {
    "objectID": "data_analysis.html#step-4-exploratory-data-analysis-eda",
    "href": "data_analysis.html#step-4-exploratory-data-analysis-eda",
    "title": "Data Analysis",
    "section": "5 Step 4: Exploratory Data Analysis (EDA)",
    "text": "5 Step 4: Exploratory Data Analysis (EDA)\n\n5.1 Visualization 1: Top Job Titles\nUnderstanding which job titles are most in demand helps job seekers identify the skills and roles to target.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nThe top job titles show where the job market is most active. This data helps identify roles with highest demand for new talent.\n\n\n\n\n5.2 Visualization 2: Job Postings by Employment Type\nEmployment type indicates the nature of positions available in the job market. Understanding the distribution helps job seekers target positions that match their preferences.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nThe distribution of employment types shows whether the market is primarily offering full-time, part-time, or contract positions, helping job seekers identify opportunities that match their work preferences.\n\n\n\n\n5.3 Visualization 3: Remote Work Distribution\nThe prevalence of remote work reflects post-pandemic hiring trends and work flexibility.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nThe balance between remote and on-site jobs shows employers’ flexibility preferences. Higher remote job percentages indicate industry acceptance of distributed workforces.\n\n\n\n\n5.4 Visualization 4: Top Companies Hiring\nUnderstanding which companies are actively hiring helps job seekers identify potential employers with multiple open positions.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nCompanies with the highest number of job postings indicate organizations that are actively expanding their workforce and may offer more opportunities for candidates."
  },
  {
    "objectID": "data_analysis.html#summary-of-data-cleaning-analysis",
    "href": "data_analysis.html#summary-of-data-cleaning-analysis",
    "title": "Data Analysis",
    "section": "6 Summary of Data Cleaning & Analysis",
    "text": "6 Summary of Data Cleaning & Analysis\n\n\n\n\n\n\nImportantSummary\n\n\n\nThe data cleaning process successfully prepared the job market dataset for analysis. We removed redundant administrative and deprecated classification columns, handled missing values through strategic imputation, and eliminated duplicate job postings.\nThe exploratory analysis revealed key insights into job market trends, including:\n\nHigh-demand roles: Identification of the most sought-after job titles in 2024\nEmployment stability: Understanding the distribution of full-time, part-time, and contract positions\nWorkplace flexibility: Analysis of remote, hybrid, and on-site work opportunities\nSalary trends: Clear understanding of compensation ranges across different job categories\n\nThese insights provide valuable guidance for job seekers, employers, and market analysts in understanding the current state of the 2024 job market."
  },
  {
    "objectID": "data_analysis.html#references",
    "href": "data_analysis.html#references",
    "title": "Data Analysis",
    "section": "7 References",
    "text": "7 References\nAll data sourced from Lightcast Job Postings Dataset (2024). Analysis performed using Python with pandas, hvplot, and holoviews libraries."
  }
]