---
title: "Data Analysis"
subtitle: "Comprehensive Data Cleaning & Exploratory Analysis of Job Market Trends"
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
    code-overflow: wrap
    embed-resources: true
    css: styles.css
jupyter: python3
execute:
  echo: false  
  warning: false
  message: false
---

## Introduction

This document provides a comprehensive analysis of job market data for 2024. We perform data cleaning, handle missing values, remove duplicates, and conduct exploratory data analysis to understand hiring trends, salary distributions, and job market dynamics.

```{python}
import pandas as pd
import numpy as np
import hvplot.pandas
import holoviews as hv
import warnings
warnings.filterwarnings('ignore')

# Initialize hvplot with bokeh backend
hv.extension('bokeh')

# Load your data
df = pd.read_csv('lightcast_job_postings.csv')
```

## Step 1: Removing Redundant Columns

### Why Remove These Columns?

We remove redundant columns to improve dataset quality and analysis efficiency. Specifically:

- **Tracking & Administrative Columns**: ID, URL, DUPLICATES, LAST_UPDATED_TIMESTAMP - These are metadata and don't contribute to analysis
- **Raw Text Fields**: BODY, TITLE_RAW, COMPANY_RAW - We have cleaned versions (TITLE, COMPANY_NAME, TITLE_CLEAN)
- **Deprecated Classifications**: We keep only the latest NAICS_2022_6, SOC_2021_4, and CIP4 standards
- **Duplicate Geographic Fields**: Multiple versions of county/MSA data create redundancy

```{python}
# List of columns to drop
columns_to_drop = [
    # Administrative & tracking columns
    "ID", "URL", "ACTIVE_URLS", "DUPLICATES", "LAST_UPDATED_TIMESTAMP",
    # Raw text columns (we have cleaned versions)
    "BODY", "TITLE_RAW", "COMPANY_RAW", "ACTIVE_SOURCES_INFO",
    # Deprecated NAICS versions (keeping NAICS_2022_6)
    "NAICS2", "NAICS2_NAME", "NAICS3", "NAICS3_NAME", 
    "NAICS4", "NAICS4_NAME", "NAICS5", "NAICS5_NAME", 
    "NAICS6", "NAICS6_NAME",
    # Deprecated SOC versions (keeping SOC_2021_4)
    "SOC_2", "SOC_2_NAME", "SOC_3", "SOC_3_NAME", 
    "SOC_4", "SOC_4_NAME", "SOC_5", "SOC_5_NAME",
    "SOC_2021_2", "SOC_2021_2_NAME", "SOC_2021_3", "SOC_2021_3_NAME", 
    "SOC_2021_5", "SOC_2021_5_NAME",
    # Deprecated occupation classifications
    "LOT_CAREER_AREA", "LOT_CAREER_AREA_NAME", "LOT_OCCUPATION", "LOT_OCCUPATION_NAME",
    "LOT_SPECIALIZED_OCCUPATION", "LOT_SPECIALIZED_OCCUPATION_NAME", 
    "LOT_OCCUPATION_GROUP", "LOT_OCCUPATION_GROUP_NAME",
    "LOT_V6_SPECIALIZED_OCCUPATION", "LOT_V6_SPECIALIZED_OCCUPATION_NAME", 
    "LOT_V6_OCCUPATION", "LOT_V6_OCCUPATION_NAME",
    "LOT_V6_OCCUPATION_GROUP", "LOT_V6_OCCUPATION_GROUP_NAME", 
    "LOT_V6_CAREER_AREA", "LOT_V6_CAREER_AREA_NAME",
    # Deprecated CIP and ONET versions
    "ONET_2019", "ONET_2019_NAME", "CIP6", "CIP6_NAME", "CIP2", "CIP2_NAME",
    # Duplicate geographic fields
    "COUNTY", "COUNTY_NAME", "COUNTY_OUTGOING", "COUNTY_NAME_OUTGOING", 
    "COUNTY_INCOMING", "COUNTY_NAME_INCOMING",
    "MSA", "MSA_OUTGOING", "MSA_INCOMING",
    # Deprecated salary fields (keeping SALARY)
    "SALARY_TO", "SALARY_FROM", "ORIGINAL_PAY_PERIOD",
    # Model versions (keep actual data)
    "MODELED_EXPIRED", "MODELED_DURATION"
]

# Drop only columns that exist in the dataset
columns_to_drop = [col for col in columns_to_drop if col in df.columns]
df_cleaned = df.drop(columns=columns_to_drop, inplace=False)
```

## Step 2: Handling Missing Values

### Understanding Missing Data

Before imputation, let's visualize where data is missing:

```{python}
# Missing value statistics
missing_stats = pd.DataFrame({
    'Column': df_cleaned.columns,
    'Missing_Count': df_cleaned.isnull().sum().values,
    'Missing_Percentage': (df_cleaned.isnull().sum().values / len(df_cleaned) * 100).round(2)
})
missing_stats = missing_stats[missing_stats['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)

# Interactive missing values visualization
if len(missing_stats) > 0:
    # Select top 10 for cleaner visualization
    top_missing = missing_stats.head(10)
    missing_plot = top_missing.hvplot.barh(
        x='Column', 
        y='Missing_Percentage',
        title='Top 10 Columns with Missing Values',
        xlabel='Missing Percentage (%)',
        ylabel='Column Name',
        height=450,
        width=900,
        color='#e74c3c',
        hover_cols=['Missing_Count', 'Missing_Percentage']
    ).opts(invert_yaxis=True)
    missing_plot
```

### Missing Value Imputation Strategy

We applied a strategic approach to handle missing data:

- **Dropped columns** with more than 50% missing values
- **Filled numerical columns** with median values to maintain distribution
- **Filled categorical columns** with "Unknown" for clarity

```{python}
# Strategy: Drop columns with >50% missing values
threshold = len(df_cleaned) * 0.5
cols_before = len(df_cleaned.columns)
df_cleaned = df_cleaned.dropna(thresh=threshold, axis=1)
cols_after = len(df_cleaned.columns)

# Fill numerical columns with median
numerical_cols = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()
for col in numerical_cols:
    if df_cleaned[col].isnull().sum() > 0:
        median_val = df_cleaned[col].median()
        df_cleaned[col].fillna(median_val, inplace=True)

# Fill categorical columns with "Unknown"
categorical_cols = df_cleaned.select_dtypes(include=['object']).columns.tolist()
for col in categorical_cols:
    if df_cleaned[col].isnull().sum() > 0:
        df_cleaned[col].fillna("Unknown", inplace=True)
```

## Step 3: Removing Duplicates

### Why Remove Duplicates?

Duplicate job postings skew our analysis. We remove them based on job title, company, and location to ensure each unique position is counted only once.

```{python}
# Remove duplicates based on key identifiers
df_cleaned = df_cleaned.drop_duplicates(
    subset=['TITLE_NAME', 'COMPANY_NAME', 'LOCATION'], 
    keep='first'
)
```

## Step 4: Exploratory Data Analysis (EDA)

### Visualization 1: Top Job Titles

Understanding which job titles are most in demand helps job seekers identify the skills and roles to target.

```{python}
# Get top 10 job titles
title_counts = df_cleaned['TITLE_NAME'].value_counts().head(10)
title_df = pd.DataFrame({'Job Title': title_counts.index, 'Count': title_counts.values})

# Create interactive plot
title_plot = title_df.hvplot.barh(
    x='Job Title',
    y='Count',
    title='Top 10 Job Titles by Postings (2024)',
    xlabel='Number of Job Postings',
    ylabel='Job Title',
    height=500,
    width=1000,
    color='#3498db',
    hover_cols='all'
).opts(invert_yaxis=True)

title_plot
```

::: {.callout-note icon=false}
## Key Insight
The top job titles show where the job market is most active. This data helps identify roles with highest demand for new talent.
:::

### Visualization 2: Job Postings by Employment Type

Employment type indicates the nature of positions available in the job market. Understanding the distribution helps job seekers target positions that match their preferences.

```{python}
# Count jobs by employment type
employment_counts = df_cleaned['EMPLOYMENT_TYPE_NAME'].value_counts()
employment_df = pd.DataFrame({
    'Employment Type': employment_counts.index, 
    'Count': employment_counts.values,
    'Percentage': (employment_counts.values / employment_counts.sum() * 100).round(1)
})

# Create interactive scatter plot with size
employment_plot = employment_df.hvplot.scatter(
    x='Employment Type',
    y='Percentage',
    size='Count',
    title='Job Market Distribution by Employment Type (2024)',
    xlabel='Employment Type',
    ylabel='Percentage of Total Jobs (%)',
    height=500,
    width=1000,
    color='#e74c3c',
    hover_cols=['Count', 'Percentage'],
    alpha=0.6,
    size_max=1000
)

employment_plot
```

::: {.callout-note icon=false}
## Key Insight
The distribution of employment types shows whether the market is primarily offering full-time, part-time, or contract positions, helping job seekers identify opportunities that match their work preferences.
:::

### Visualization 3: Remote Work Distribution
The prevalence of remote work reflects post-pandemic hiring trends and work flexibility.

```{python}
# Count jobs by remote type
remote_counts = df_cleaned['REMOTE_TYPE_NAME'].value_counts()
remote_df = pd.DataFrame({
    'Remote Type': remote_counts.index,
    'Count': remote_counts.values,
    'Percentage': (remote_counts.values / remote_counts.sum() * 100).round(1)
})

# Create interactive bar chart
remote_plot = remote_df.hvplot.bar(
    x='Remote Type',
    y='Count',
    title='Job Market Distribution: Remote vs. On-Site (2024)',
    xlabel='Remote Type',
    ylabel='Number of Job Postings',
    height=500,
    width=900,
    color='#2E8B57',
    hover_cols=['Count', 'Percentage']
)

remote_plot
```

::: {.callout-note icon=false}
## Key Insight
The balance between remote and on-site jobs shows employers' flexibility preferences. Higher remote job percentages indicate industry acceptance of distributed workforces.
:::

### Visualization 4: Top Companies Hiring

Understanding which companies are actively hiring helps job seekers identify potential employers with multiple open positions.

```{python}
# Get top 10 companies by job postings
company_counts = df_cleaned['COMPANY_NAME'].value_counts().head(10)
company_df = pd.DataFrame({
    'Company': company_counts.index,
    'Job Postings': company_counts.values
})

# Create area chart
company_plot = company_df.hvplot.area(
    x='Company',
    y='Job Postings',
    title='Top 10 Companies by Number of Job Postings (2024)',
    xlabel='Company Name',
    ylabel='Number of Job Postings',
    height=500,
    width=1000,
    color='#9b59b6',
    line_width=2,
    alpha=0.5,
    hover_cols='all'
).opts(xrotation=45)

company_plot
```

::: {.callout-note icon=false}
## Key Insight
Companies with the highest number of job postings indicate organizations that are actively expanding their workforce and may offer more opportunities for candidates.
:::

## Summary of Data Cleaning & Analysis

::: {.callout-important icon=false}
## Summary
The data cleaning process successfully prepared the job market dataset for analysis. We removed redundant administrative and deprecated classification columns, handled missing values through strategic imputation, and eliminated duplicate job postings. 

The exploratory analysis revealed key insights into job market trends, including:

- **High-demand roles**: Identification of the most sought-after job titles in 2024
- **Employment stability**: Understanding the distribution of full-time, part-time, and contract positions
- **Workplace flexibility**: Analysis of remote, hybrid, and on-site work opportunities
- **Salary trends**: Clear understanding of compensation ranges across different job categories

These insights provide valuable guidance for job seekers, employers, and market analysts in understanding the current state of the 2024 job market.
:::

## References

All data sourced from Lightcast Job Postings Dataset (2024). Analysis performed using Python with pandas, hvplot, and holoviews libraries.