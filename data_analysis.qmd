"""
Complete Data Analysis & Exploratory Analysis of Job Market Trends
Team: Tuba Anwar, Kriti Singh, Soham Deshkhaire
Boston University
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno
import warnings
warnings.filterwarnings('ignore')

# Set style for better visualizations
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

print("="*80)
print("JOB MARKET ANALYSIS 2024 - DATA CLEANING & EXPLORATORY ANALYSIS")
print("="*80)

# ========================================
# LOAD DATA
# ========================================
df = pd.read_csv('lightcast_job_postings.csv')
print(f"\nðŸ“Š Original dataset shape: {df.shape}")
print(f"ðŸ“Š Total records: {len(df):,}")
print(f"ðŸ“Š Total columns: {len(df.columns)}")

# ========================================
# STEP 1: REMOVING REDUNDANT COLUMNS
# ========================================
print("\n" + "="*80)
print("STEP 1: REMOVING REDUNDANT COLUMNS")
print("="*80)

print("""
WHY REMOVE THESE COLUMNS?
- Tracking & Administrative: ID, URL, DUPLICATES - metadata not useful for analysis
- Raw Text Fields: We have cleaned versions (TITLE, COMPANY_NAME)
- Deprecated Classifications: Keep only latest standards (NAICS_2022_6, SOC_2021_4)
- Duplicate Geographic Fields: Multiple versions of county/MSA create redundancy
""")

columns_to_drop = [
    # Administrative & tracking columns
    "ID", "URL", "ACTIVE_URLS", "DUPLICATES", "LAST_UPDATED_TIMESTAMP",
    # Raw text columns (we have cleaned versions)
    "BODY", "TITLE_RAW", "COMPANY_RAW", "ACTIVE_SOURCES_INFO",
    # Deprecated NAICS versions (keeping NAICS_2022_6)
    "NAICS2", "NAICS2_NAME", "NAICS3", "NAICS3_NAME", 
    "NAICS4", "NAICS4_NAME", "NAICS5", "NAICS5_NAME", 
    "NAICS6", "NAICS6_NAME",
    # Deprecated SOC versions (keeping SOC_2021_4)
    "SOC_2", "SOC_2_NAME", "SOC_3", "SOC_3_NAME", 
    "SOC_4", "SOC_4_NAME", "SOC_5", "SOC_5_NAME",
    "SOC_2021_2", "SOC_2021_2_NAME", "SOC_2021_3", "SOC_2021_3_NAME", 
    "SOC_2021_5", "SOC_2021_5_NAME",
    # Deprecated occupation classifications
    "LOT_CAREER_AREA", "LOT_CAREER_AREA_NAME", "LOT_OCCUPATION", "LOT_OCCUPATION_NAME",
    "LOT_SPECIALIZED_OCCUPATION", "LOT_SPECIALIZED_OCCUPATION_NAME", 
    "LOT_OCCUPATION_GROUP", "LOT_OCCUPATION_GROUP_NAME",
    "LOT_V6_SPECIALIZED_OCCUPATION", "LOT_V6_SPECIALIZED_OCCUPATION_NAME", 
    "LOT_V6_OCCUPATION", "LOT_V6_OCCUPATION_NAME",
    "LOT_V6_OCCUPATION_GROUP", "LOT_V6_OCCUPATION_GROUP_NAME", 
    "LOT_V6_CAREER_AREA", "LOT_V6_CAREER_AREA_NAME",
    # Deprecated CIP and ONET versions
    "ONET_2019", "ONET_2019_NAME", "CIP6", "CIP6_NAME", "CIP2", "CIP2_NAME",
    # Duplicate geographic fields
    "COUNTY", "COUNTY_NAME", "COUNTY_OUTGOING", "COUNTY_NAME_OUTGOING", 
    "COUNTY_INCOMING", "COUNTY_NAME_INCOMING",
    "MSA", "MSA_OUTGOING", "MSA_INCOMING",
    # Deprecated salary fields (keeping SALARY)
    "SALARY_TO", "SALARY_FROM", "ORIGINAL_PAY_PERIOD",
    # Model versions
    "MODELED_EXPIRED", "MODELED_DURATION"
]

columns_to_drop = [col for col in columns_to_drop if col in df.columns]
df_cleaned = df.drop(columns=columns_to_drop, inplace=False)

print(f"âœ… Original dataset: {df.shape}")
print(f"âœ… Cleaned dataset: {df_cleaned.shape}")
print(f"âœ… Columns removed: {len(columns_to_drop)}")
print(f"âœ… Columns retained: {len(df_cleaned.columns)}")

# ========================================
# STEP 2: HANDLING MISSING VALUES
# ========================================
print("\n" + "="*80)
print("STEP 2: UNDERSTANDING & HANDLING MISSING DATA")
print("="*80)

print("\nðŸ“Š Visualizing Missing Values Pattern...\n")

# Create missing values heatmap
fig, ax = plt.subplots(figsize=(14, 6))
msno.heatmap(df_cleaned, ax=ax, fontsize=8)
plt.title("Missing Values Heatmap - Lightcast Job Market Data", 
          fontsize=14, fontweight='bold', pad=20)
plt.tight_layout()
plt.show()

# Show top missing value statistics
missing_stats = pd.DataFrame({
    'Column': df_cleaned.columns,
    'Missing_Count': df_cleaned.isnull().sum().values,
    'Missing_Percentage': (df_cleaned.isnull().sum().values / len(df_cleaned) * 100).round(2)
})
missing_stats = missing_stats[missing_stats['Missing_Count'] > 0].sort_values(
    'Missing_Percentage', ascending=False
)

print("\nðŸ“‹ Top 10 Columns with Missing Values:")
print(missing_stats.head(10).to_string(index=False))

print("""
MISSING VALUE STRATEGY:
1. Drop columns with >50% missing values (insufficient data)
2. Fill numerical columns with median (robust to outliers)
3. Fill categorical columns with 'Unknown' (preserve data)
""")

# Drop columns with >50% missing
threshold = len(df_cleaned) * 0.5
cols_before = len(df_cleaned.columns)
df_cleaned = df_cleaned.dropna(thresh=threshold, axis=1)
cols_after = len(df_cleaned.columns)
print(f"\nâœ… Dropped {cols_before - cols_after} columns with >50% missing values")

# Fill numerical columns with median
numerical_cols = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()
filled_numerical = 0
for col in numerical_cols:
    if df_cleaned[col].isnull().sum() > 0:
        median_val = df_cleaned[col].median()
        df_cleaned[col].fillna(median_val, inplace=True)
        filled_numerical += 1

print(f"âœ… Filled {filled_numerical} numerical columns with median values")

# Fill categorical columns with "Unknown"
categorical_cols = df_cleaned.select_dtypes(include=['object']).columns.tolist()
filled_categorical = 0
for col in categorical_cols:
    if df_cleaned[col].isnull().sum() > 0:
        df_cleaned[col].fillna("Unknown", inplace=True)
        filled_categorical += 1

print(f"âœ… Filled {filled_categorical} categorical columns with 'Unknown'")
print(f"âœ… Total missing values after imputation: {df_cleaned.isnull().sum().sum()}")

# ========================================
# STEP 3: REMOVING DUPLICATES
# ========================================
print("\n" + "="*80)
print("STEP 3: REMOVING DUPLICATE JOB POSTINGS")
print("="*80)

print("""
WHY REMOVE DUPLICATES?
Duplicate job postings skew our analysis. We remove them based on:
- Job Title
- Company Name
- Location
This ensures each unique position is counted only once.
""")

duplicates_before = df_cleaned.duplicated().sum()
print(f"ðŸ“Š Duplicate records found: {duplicates_before:,}")

df_cleaned = df_cleaned.drop_duplicates(
    subset=['TITLE_NAME', 'COMPANY_NAME', 'LOCATION'], 
    keep='first'
)

duplicates_after = df_cleaned.duplicated().sum()
print(f"âœ… Duplicates after removal: {duplicates_after:,}")
print(f"âœ… Records removed: {duplicates_before - duplicates_after:,}")
print(f"âœ… Final cleaned dataset: {df_cleaned.shape}")

# ========================================
# STEP 4: EXPLORATORY DATA ANALYSIS (EDA)
# ========================================
print("\n" + "="*80)
print("STEP 4: EXPLORATORY DATA ANALYSIS")
print("="*80)

# VISUALIZATION 1: Top Job Titles
print("\nðŸ“Š VISUALIZATION 1: TOP JOB TITLES BY DEMAND")
print("-" * 80)
print("""
KEY INSIGHT: Understanding which job titles are most in demand helps job seekers
identify the skills and roles to target in their career development.
""")

title_counts = df_cleaned['TITLE_NAME'].value_counts().head(10)

fig, ax = plt.subplots(figsize=(12, 7))
bars = title_counts.plot(kind='barh', ax=ax, color='#6C9BCF')
ax.set_xlabel('Number of Job Postings', fontsize=13, fontweight='bold')
ax.set_ylabel('Job Title', fontsize=13, fontweight='bold')
ax.set_title('Top 10 Most In-Demand Job Titles (2024)', 
             fontsize=15, fontweight='bold', pad=20)
ax.invert_yaxis()
ax.tick_params(left=False, bottom=False)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

for i, v in enumerate(title_counts):
    ax.text(v + 50, i, f'{v:,}', va='center', fontweight='bold', fontsize=10)

plt.tight_layout()
plt.show()

print(f"\nðŸ“ˆ Top 5 Job Titles:")
for i, (title, count) in enumerate(title_counts.head(5).items(), 1):
    pct = (count / title_counts.sum() * 100)
    print(f"{i}. {title}: {count:,} postings ({pct:.1f}% of top 10)")

# VISUALIZATION 2: Employment Type Distribution
print("\n\n VISUALIZATION 2: JOB POSTINGS BY EMPLOYMENT TYPE")
print("-" * 80)
print("""
KEY INSIGHT: Employment type distribution shows whether the market is primarily
offering full-time, part-time, or contract positions, helping job seekers
identify opportunities that match their work preferences.
""")

employment_counts = df_cleaned['EMPLOYMENT_TYPE_NAME'].value_counts()

fig, ax = plt.subplots(figsize=(12, 7))
employment_counts.plot(kind='barh', ax=ax, color='#FF9B85')
ax.set_xlabel('Number of Job Postings', fontsize=13, fontweight='bold')
ax.set_ylabel('Employment Type', fontsize=13, fontweight='bold')
ax.set_title('Job Market by Employment Type (2024)', 
             fontsize=15, fontweight='bold', pad=20)
ax.invert_yaxis()
ax.tick_params(left=False, bottom=False)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)

for i, v in enumerate(employment_counts):
    ax.text(v + 50, i, f'{v:,}', va='center', fontweight='bold', fontsize=10)

plt.tight_layout()
plt.show()

print(f"\n Employment Type Breakdown:")
for emp_type, count in employment_counts.items():
    pct = (count / employment_counts.sum() * 100)
    print(f"â€¢ {emp_type}: {count:,} postings ({pct:.1f}%)")

# VISUALIZATION 3: Remote Work Distribution (IMPROVED WITH PASTEL COLORS)
print("\n\n VISUALIZATION 3: REMOTE WORK DISTRIBUTION")
print("-" * 80)
print("""
KEY INSIGHT: The balance between remote and on-site jobs reflects post-pandemic
hiring trends and shows employers' flexibility preferences. Higher remote job
percentages indicate industry acceptance of distributed workforces.
""")

remote_counts = df_cleaned['REMOTE_TYPE_NAME'].value_counts()

# Beautiful pastel color palette
pastel_colors = ['#B4E7CE', '#FFB5A7', '#A7C7E7', '#F8E5F1', '#FFDAB9']

fig, ax = plt.subplots(figsize=(12, 8))

wedges, texts, autotexts = ax.pie(
    remote_counts.values, 
    labels=remote_counts.index,
    autopct='%1.1f%%',
    colors=pastel_colors[:len(remote_counts)],
    startangle=90,
    pctdistance=0.85,
    explode=[0.05] * len(remote_counts),
    textprops={'fontsize': 12, 'fontweight': 'bold'},
    shadow=True
)

# Style percentage text
for autotext in autotexts:
    autotext.set_color('#2C3E50')
    autotext.set_fontweight('bold')
    autotext.set_fontsize(14)

# Style labels
for text in texts:
    text.set_fontsize(13)
    text.set_fontweight('bold')
    text.set_color('#34495E')

# Title
ax.set_title(
    "Job Market Distribution: Remote vs. On-Site Work (2024)", 
    fontsize=16, 
    fontweight='bold',
    pad=20,
    color='#2C3E50'
)

# Donut effect
centre_circle = plt.Circle((0, 0), 0.70, fc='white', linewidth=0)
fig.gca().add_artist(centre_circle)

ax.axis('equal')
plt.tight_layout()
plt.show()

print(f"\n Remote Work Statistics:")
for remote_type, count in remote_counts.items():
    pct = (count / remote_counts.sum() * 100)
    print(f"â€¢ {remote_type}: {count:,} jobs ({pct:.1f}%)")

# ========================================
# FINAL SUMMARY
# ========================================
print("\n" + "="*80)
print("DATA QUALITY & ANALYSIS SUMMARY")
print("="*80)
print(f"""
 CLEANING RESULTS:
   â€¢ Removed {len(columns_to_drop)} redundant columns
   â€¢ Handled missing values (median/Unknown imputation)
   â€¢ Removed {df.shape[0] - df_cleaned.shape[0]:,} duplicate records
   
 FINAL DATASET:
   â€¢ Records: {df_cleaned.shape[0]:,}
   â€¢ Features: {df_cleaned.shape[1]}
   â€¢ Data Quality: High (no missing values, no duplicates)

 KEY FINDINGS:
   â€¢ Top job demand: {title_counts.index[0]}
   â€¢ Primary employment type: {employment_counts.index[0]}
   â€¢ Remote work adoption: {(remote_counts.get('Remote', 0) / remote_counts.sum() * 100):.1f}%
   
 NEXT STEPS:
   â€¢ Salary analysis by industry
   â€¢ AI vs. non-AI job trends
   â€¢ Geographic hiring patterns
   â€¢ Career strategy development
""")
print("="*80) 