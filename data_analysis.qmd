---
title: "Data Analysis"
subtitle: "Comprehensive Data Cleaning & Exploratory Analysis of Job Market Trends"
author:
  - name: Tuba Anwar
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
  - name: Kriti Singh
    affiliations:
      - ref: bu
  - name: Soham Deshkhaire
    affiliations:
      - ref: bu
# bibliography: references.bib
# csl: csl/econometrica.csl
format: 
  html:
    toc: true
    number-sections: true
    df-print: paged
    code-overflow: wrap
jupyter: python3
execute:
  echo: true
  eval: true
---

## Introduction

This document provides a comprehensive analysis of job market data for 2024. We perform data cleaning, handle missing values, remove duplicates, and conduct exploratory data analysis to understand hiring trends, salary distributions, and job market dynamics.

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno
import warnings
warnings.filterwarnings('ignore')

# Set style for better visualizations
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

# Load your data
df = pd.read_csv('lightcast_job_postings.csv')
print(f"Original dataset shape: {df.shape}")
print(f"Total records: {len(df):,}")
print(f"Total columns: {len(df.columns)}")
```

## Step 1: Removing Redundant Columns

### Why Remove These Columns?

We remove redundant columns to improve dataset quality and analysis efficiency. Specifically:

- **Tracking & Administrative Columns**: ID, URL, DUPLICATES, LAST_UPDATED_TIMESTAMP - These are metadata and don't contribute to analysis
- **Raw Text Fields**: BODY, TITLE_RAW, COMPANY_RAW - We have cleaned versions (TITLE, COMPANY_NAME, TITLE_CLEAN)
- **Deprecated Classifications**: We keep only the latest NAICS_2022_6, SOC_2021_4, and CIP4 standards
- **Duplicate Geographic Fields**: Multiple versions of county/MSA data create redundancy

```{python}
# List of columns to drop
columns_to_drop = [
    # Administrative & tracking columns
    "ID", "URL", "ACTIVE_URLS", "DUPLICATES", "LAST_UPDATED_TIMESTAMP",
    # Raw text columns (we have cleaned versions)
    "BODY", "TITLE_RAW", "COMPANY_RAW", "ACTIVE_SOURCES_INFO",
    # Deprecated NAICS versions (keeping NAICS_2022_6)
    "NAICS2", "NAICS2_NAME", "NAICS3", "NAICS3_NAME", 
    "NAICS4", "NAICS4_NAME", "NAICS5", "NAICS5_NAME", 
    "NAICS6", "NAICS6_NAME",
    # Deprecated SOC versions (keeping SOC_2021_4)
    "SOC_2", "SOC_2_NAME", "SOC_3", "SOC_3_NAME", 
    "SOC_4", "SOC_4_NAME", "SOC_5", "SOC_5_NAME",
    "SOC_2021_2", "SOC_2021_2_NAME", "SOC_2021_3", "SOC_2021_3_NAME", 
    "SOC_2021_5", "SOC_2021_5_NAME",
    # Deprecated occupation classifications
    "LOT_CAREER_AREA", "LOT_CAREER_AREA_NAME", "LOT_OCCUPATION", "LOT_OCCUPATION_NAME",
    "LOT_SPECIALIZED_OCCUPATION", "LOT_SPECIALIZED_OCCUPATION_NAME", 
    "LOT_OCCUPATION_GROUP", "LOT_OCCUPATION_GROUP_NAME",
    "LOT_V6_SPECIALIZED_OCCUPATION", "LOT_V6_SPECIALIZED_OCCUPATION_NAME", 
    "LOT_V6_OCCUPATION", "LOT_V6_OCCUPATION_NAME",
    "LOT_V6_OCCUPATION_GROUP", "LOT_V6_OCCUPATION_GROUP_NAME", 
    "LOT_V6_CAREER_AREA", "LOT_V6_CAREER_AREA_NAME",
    # Deprecated CIP and ONET versions
    "ONET_2019", "ONET_2019_NAME", "CIP6", "CIP6_NAME", "CIP2", "CIP2_NAME",
    # Duplicate geographic fields
    "COUNTY", "COUNTY_NAME", "COUNTY_OUTGOING", "COUNTY_NAME_OUTGOING", 
    "COUNTY_INCOMING", "COUNTY_NAME_INCOMING",
    "MSA", "MSA_OUTGOING", "MSA_INCOMING",
    # Deprecated salary fields (keeping SALARY)
    "SALARY_TO", "SALARY_FROM", "ORIGINAL_PAY_PERIOD",
    # Model versions (keep actual data)
    "MODELED_EXPIRED", "MODELED_DURATION"
]

# Drop only columns that exist in the dataset
columns_to_drop = [col for col in columns_to_drop if col in df.columns]
df_cleaned = df.drop(columns=columns_to_drop, inplace=False)

print(f"\n{'='*60}")
print(f"DATA CLEANING SUMMARY")
print(f"{'='*60}")
print(f"Original dataset shape: {df.shape}")
print(f"Cleaned dataset shape: {df_cleaned.shape}")
print(f"Columns removed: {len(columns_to_drop)}")
print(f"Columns retained: {len(df_cleaned.columns)}")
print(f"\nRemaining key columns:\n{', '.join(df_cleaned.columns[:15])}")
```

## Step 2: Handling Missing Values

### Understanding Missing Data

Before imputation, let's visualize where data is missing:

```{python}
# Create a heatmap of missing values
fig, ax = plt.subplots(figsize=(14, 6))
msno.heatmap(df_cleaned, ax=ax)
plt.title("Missing Values Heatmap - Lightcast Job Market Data", fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# Print detailed missing value statistics
missing_stats = pd.DataFrame({
    'Column': df_cleaned.columns,
    'Missing_Count': df_cleaned.isnull().sum().values,
    'Missing_Percentage': (df_cleaned.isnull().sum().values / len(df_cleaned) * 100).round(2)
})
missing_stats = missing_stats[missing_stats['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)

print("\nMissing Value Statistics:")
print(missing_stats.head(10).to_string(index=False))
```

### Missing Value Imputation Strategy

```{python}
# Strategy: Drop columns with >50% missing values
threshold = len(df_cleaned) * 0.5
cols_before = len(df_cleaned.columns)
df_cleaned = df_cleaned.dropna(thresh=threshold, axis=1)
cols_after = len(df_cleaned.columns)

print(f"Dropped {cols_before - cols_after} columns with >50% missing values")

# Fill numerical columns with median
numerical_cols = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()
for col in numerical_cols:
    if df_cleaned[col].isnull().sum() > 0:
        median_val = df_cleaned[col].median()
        df_cleaned[col].fillna(median_val, inplace=True)
        print(f"Filled '{col}' with median value: {median_val:.2f}")

# Fill categorical columns with "Unknown"
categorical_cols = df_cleaned.select_dtypes(include=['object']).columns.tolist()
for col in categorical_cols:
    if df_cleaned[col].isnull().sum() > 0:
        df_cleaned[col].fillna("Unknown", inplace=True)
        print(f"Filled '{col}' with 'Unknown'")

print(f"\nTotal missing values after imputation: {df_cleaned.isnull().sum().sum()}")
```

## Step 3: Removing Duplicates

### Why Remove Duplicates?

Duplicate job postings skew our analysis. We remove them based on job title, company, and location to ensure each unique position is counted only once.

```{python}
print(f"Duplicate records before removal: {df_cleaned.duplicated().sum():,}")

# Remove duplicates based on key identifiers
df_cleaned = df_cleaned.drop_duplicates(
    subset=['TITLE_NAME', 'COMPANY_NAME', 'LOCATION'], 
    keep='first'
)

print(f"Duplicate records after removal: {df_cleaned.duplicated().sum():,}")
print(f"\nFinal cleaned dataset shape: {df_cleaned.shape}")
print(f"Total records retained: {len(df_cleaned):,}")
```

## Step 4: Exploratory Data Analysis (EDA)

### Visualization 1: Top Job Titles

Understanding which job titles are most in demand helps job seekers identify the skills and roles to target.

```{python}
# Get top 10 job titles
title_counts = df_cleaned['TITLE_NAME'].value_counts().head(10)

fig, ax = plt.subplots(figsize=(12, 6))
title_counts.plot(kind='barh', ax=ax, color='steelblue')
ax.set_xlabel('Number of Job Postings', fontsize=12, fontweight='bold')
ax.set_ylabel('Job Title', fontsize=12, fontweight='bold')
ax.set_title('Top 10 Job Titles by Postings (2024)', fontsize=14, fontweight='bold')
ax.invert_yaxis()
ax.tick_params(left=False, bottom=False)
for i, v in enumerate(title_counts):
    ax.text(v + 50, i, str(v), va='center', fontweight='bold')
plt.tight_layout()
plt.show()

print(f"\nTop Job Titles by Postings:")
print(title_counts)
print(f"\nJob Title Market Share (%):")
print((title_counts / title_counts.sum() * 100).round(2))
```

**Key Insight**: The top job titles show where the job market is most active. This data helps identify roles with highest demand for new talent.

### Visualization 2: Job Postings by Employment Type

Employment type indicates the nature of positions available in the job market. Understanding the distribution helps job seekers target positions that match their preferences.

```{python}
# Count jobs by employment type
employment_counts = df_cleaned['EMPLOYMENT_TYPE_NAME'].value_counts()

fig, ax = plt.subplots(figsize=(12, 6))
employment_counts.plot(kind='barh', ax=ax, color='coral')
ax.set_xlabel('Number of Job Postings', fontsize=12, fontweight='bold')
ax.set_ylabel('Employment Type', fontsize=12, fontweight='bold')
ax.set_title('Job Postings by Employment Type (2024)', fontsize=14, fontweight='bold')
ax.invert_yaxis()
ax.tick_params(left=False, bottom=False)
for i, v in enumerate(employment_counts):
    ax.text(v + 50, i, str(v), va='center', fontweight='bold')
plt.tight_layout()
plt.show()

print(f"\nJob Postings by Employment Type:")
print(employment_counts)
print(f"\nEmployment Type Distribution (%):")
print((employment_counts / employment_counts.sum() * 100).round(2))
```

**Key Insight**: The distribution of employment types shows whether the market is primarily offering full-time, part-time, or contract positions, helping job seekers identify opportunities that match their work preferences.

### Visualization 3: Remote Work Distribution

The prevalence of remote work reflects post-pandemic hiring trends and work flexibility.

```{python}
# Count jobs by remote type
remote_counts = df_cleaned['REMOTE_TYPE_NAME'].value_counts()

fig, ax = plt.subplots(figsize=(10, 6))
colors = sns.color_palette("Set2", len(remote_counts))
wedges, texts, autotexts = ax.pie(remote_counts.values, 
                                    labels=remote_counts.index,
                                    autopct='%1.1f%%',
                                    colors=colors,
                                    startangle=90)
ax.set_title("Job Market Distribution: Remote vs. On-Site (2024)", fontsize=14, fontweight='bold')
for autotext in autotexts:
    autotext.set_color('white')
    autotext.set_fontweight('bold')
plt.tight_layout()
plt.show()

print(f"\nRemote Work Distribution:")
for remote_type, count in remote_counts.items():
    pct = (count / remote_counts.sum() * 100)
    print(f"{remote_type}: {count:,} jobs ({pct:.1f}%)")
```

**Key Insight**: The balance between remote and on-site jobs shows employers' flexibility preferences. Higher remote job percentages indicate industry acceptance of distributed workforces.

## Summary of Data Cleaning & Analysis

```{python}
print(f"\n{'='*60}")
print(f"DATA QUALITY SUMMARY")
print(f"{'='*60}")
print(f"Removed {len(columns_to_drop)} redundant columns")
print(f"Handled missing values with median/Unknown imputation")
print(f"Removed {df.shape[0] - df_cleaned.shape[0]:,} duplicate records")
print(f"Final dataset: {df_cleaned.shape[0]:,} records Ã— {df_cleaned.shape[1]} columns")
print(f"{'='*60}")
```

## References