[
  {
    "objectID": "data_analysis.html",
    "href": "data_analysis.html",
    "title": "Data Analysis",
    "section": "",
    "text": "This document provides a comprehensive analysis of job market data for 2024. We perform data cleaning, handle missing values, remove duplicates, and conduct exploratory data analysis to understand hiring trends, salary distributions, and job market dynamics."
  },
  {
    "objectID": "data_analysis.html#introduction",
    "href": "data_analysis.html#introduction",
    "title": "Data Analysis",
    "section": "",
    "text": "This document provides a comprehensive analysis of job market data for 2024. We perform data cleaning, handle missing values, remove duplicates, and conduct exploratory data analysis to understand hiring trends, salary distributions, and job market dynamics."
  },
  {
    "objectID": "data_analysis.html#step-1-removing-redundant-columns",
    "href": "data_analysis.html#step-1-removing-redundant-columns",
    "title": "Data Analysis",
    "section": "2 Step 1: Removing Redundant Columns",
    "text": "2 Step 1: Removing Redundant Columns\n\n2.1 Why Remove These Columns?\nWe remove redundant columns to improve dataset quality and analysis efficiency. Specifically:\n\nTracking & Administrative Columns: ID, URL, DUPLICATES, LAST_UPDATED_TIMESTAMP - These are metadata and don‚Äôt contribute to analysis\nRaw Text Fields: BODY, TITLE_RAW, COMPANY_RAW - We have cleaned versions (TITLE, COMPANY_NAME, TITLE_CLEAN)\nDeprecated Classifications: We keep only the latest NAICS_2022_6, SOC_2021_4, and CIP4 standards\nDuplicate Geographic Fields: Multiple versions of county/MSA data create redundancy"
  },
  {
    "objectID": "data_analysis.html#step-2-handling-missing-values",
    "href": "data_analysis.html#step-2-handling-missing-values",
    "title": "Data Analysis",
    "section": "3 Step 2: Handling Missing Values",
    "text": "3 Step 2: Handling Missing Values\n\n3.1 Understanding Missing Data\nBefore imputation, let‚Äôs visualize where data is missing:\n\n\n3.2 Missing Value Imputation Strategy\nWe applied a strategic approach to handle missing data:\n\nDropped columns with more than 50% missing values\nFilled numerical columns with median values to maintain distribution\nFilled categorical columns with ‚ÄúUnknown‚Äù for clarity"
  },
  {
    "objectID": "data_analysis.html#step-3-removing-duplicates",
    "href": "data_analysis.html#step-3-removing-duplicates",
    "title": "Data Analysis",
    "section": "4 Step 3: Removing Duplicates",
    "text": "4 Step 3: Removing Duplicates\n\n4.1 Why Remove Duplicates?\nDuplicate job postings skew our analysis. We remove them based on job title, company, and location to ensure each unique position is counted only once."
  },
  {
    "objectID": "data_analysis.html#step-4-exploratory-data-analysis-eda",
    "href": "data_analysis.html#step-4-exploratory-data-analysis-eda",
    "title": "Data Analysis",
    "section": "5 Step 4: Exploratory Data Analysis (EDA)",
    "text": "5 Step 4: Exploratory Data Analysis (EDA)\n\n5.1 Visualization 1: Top Job Titles\nUnderstanding which job titles are most in demand helps job seekers identify the skills and roles to target.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nThe top job titles show where the job market is most active. This data helps identify roles with highest demand for new talent.\n\n\n\n\n5.2 Visualization 2: Job Postings by Employment Type\nEmployment type indicates the nature of positions available in the job market. Understanding the distribution helps job seekers target positions that match their preferences.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nThe distribution of employment types shows whether the market is primarily offering full-time, part-time, or contract positions, helping job seekers identify opportunities that match their work preferences.\n\n\n\n\n5.3 Visualization 3: Remote Work Distribution\nThe prevalence of remote work reflects post-pandemic hiring trends and work flexibility.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nThe balance between remote and on-site jobs shows employers‚Äô flexibility preferences. Higher remote job percentages indicate industry acceptance of distributed workforces.\n\n\n\n\n5.4 Visualization 4: Top Companies Hiring\nUnderstanding which companies are actively hiring helps job seekers identify potential employers with multiple open positions.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nCompanies with the highest number of job postings indicate organizations that are actively expanding their workforce and may offer more opportunities for candidates."
  },
  {
    "objectID": "data_analysis.html#summary-of-data-cleaning-analysis",
    "href": "data_analysis.html#summary-of-data-cleaning-analysis",
    "title": "Data Analysis",
    "section": "6 Summary of Data Cleaning & Analysis",
    "text": "6 Summary of Data Cleaning & Analysis\n\n\n\n\n\n\nImportantSummary\n\n\n\nThe data cleaning process successfully prepared the job market dataset for analysis. We removed redundant administrative and deprecated classification columns, handled missing values through strategic imputation, and eliminated duplicate job postings.\nThe exploratory analysis revealed key insights into job market trends, including:\n\nHigh-demand roles: Identification of the most sought-after job titles in 2024\nEmployment stability: Understanding the distribution of full-time, part-time, and contract positions\nWorkplace flexibility: Analysis of remote, hybrid, and on-site work opportunities\nSalary trends: Clear understanding of compensation ranges across different job categories\n\nThese insights provide valuable guidance for job seekers, employers, and market analysts in understanding the current state of the 2024 job market."
  },
  {
    "objectID": "data_analysis.html#references",
    "href": "data_analysis.html#references",
    "title": "Data Analysis",
    "section": "7 References",
    "text": "7 References\nAll data sourced from Lightcast Job Postings Dataset (2024). Analysis performed using Python with pandas, hvplot, and holoviews libraries."
  },
  {
    "objectID": "ML.html",
    "href": "ML.html",
    "title": "Machine Learning Methods",
    "section": "",
    "text": "This section applies machine learning techniques to uncover patterns in job market data. We use clustering algorithms to group similar job postings and predictive models to forecast salary trends and job categories.\n\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport panel as pn\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Enable panel for rendering\npn.extension()\n\n# Load cleaned data\ndf = pd.read_csv('lightcast_job_postings.csv')\nprint(f\"Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset loaded: 72,498 rows, 131 columns"
  },
  {
    "objectID": "ML.html#introduction",
    "href": "ML.html#introduction",
    "title": "Machine Learning Methods",
    "section": "",
    "text": "This section applies machine learning techniques to uncover patterns in job market data. We use clustering algorithms to group similar job postings and predictive models to forecast salary trends and job categories.\n\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport panel as pn\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Enable panel for rendering\npn.extension()\n\n# Load cleaned data\ndf = pd.read_csv('lightcast_job_postings.csv')\nprint(f\"Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset loaded: 72,498 rows, 131 columns"
  },
  {
    "objectID": "ML.html#feature-engineering",
    "href": "ML.html#feature-engineering",
    "title": "Machine Learning Methods",
    "section": "2 1. Feature Engineering",
    "text": "2 1. Feature Engineering\n\n2.1 1.1 Preparing Data for Machine Learning\nBefore applying ML algorithms, we need to prepare our features.\n\n# Select relevant numerical features for clustering\n# Check which numerical features exist\nprint(\"Available columns:\")\nprint(df.columns.tolist())\n\n# Select numerical features - adjust based on your actual columns\nml_features = []\n\n# Common numerical features to look for\npotential_features = ['DURATION', 'EXPERIENCE_REQUIRED', 'MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE']\n\nfor feature in potential_features:\n    if feature in df.columns:\n        ml_features.append(feature)\n\n# If SALARY exists, use it as target variable\nif 'SALARY' in df.columns:\n    # Filter for valid salaries and create working dataset\n    df_ml = df[df['SALARY'].notna() & (df['SALARY'] &gt; 0) & (df['SALARY'] &lt; 500000)].copy()\n    \n    # Add additional engineered features\n    if 'TITLE_NAME' in df.columns:\n        # Create title length as a feature\n        df_ml['TITLE_LENGTH'] = df_ml['TITLE_NAME'].str.len()\n        ml_features.append('TITLE_LENGTH')\n    \n    if 'REMOTE_TYPE_NAME' in df.columns:\n        # Encode remote type as numerical\n        remote_mapping = {'On-Site': 0, 'Hybrid': 1, 'Remote': 2}\n        df_ml['REMOTE_ENCODED'] = df_ml['REMOTE_TYPE_NAME'].map(remote_mapping).fillna(0)\n        ml_features.append('REMOTE_ENCODED')\n    \n    # Keep only rows with complete feature data\n    feature_cols = [f for f in ml_features if f in df_ml.columns]\n    df_ml = df_ml[['SALARY'] + feature_cols].dropna()\n    \n    # Update ml_features to only include existing features\n    ml_features = [f for f in ml_features if f in df_ml.columns]\n    \n    print(f\"\\nFeatures selected for ML: {ml_features}\")\n    print(f\"ML dataset shape: {df_ml.shape}\")\n    print(f\"\\nFeature statistics:\")\n    print(df_ml[ml_features + ['SALARY']].describe())\nelse:\n    print(\"SALARY column not found. Please check your data.\")\n    df_ml = pd.DataFrame()\n\nAvailable columns:\n['ID', 'LAST_UPDATED_DATE', 'LAST_UPDATED_TIMESTAMP', 'DUPLICATES', 'POSTED', 'EXPIRED', 'DURATION', 'SOURCE_TYPES', 'SOURCES', 'URL', 'ACTIVE_URLS', 'ACTIVE_SOURCES_INFO', 'TITLE_RAW', 'BODY', 'MODELED_EXPIRED', 'MODELED_DURATION', 'COMPANY', 'COMPANY_NAME', 'COMPANY_RAW', 'COMPANY_IS_STAFFING', 'EDUCATION_LEVELS', 'EDUCATION_LEVELS_NAME', 'MIN_EDULEVELS', 'MIN_EDULEVELS_NAME', 'MAX_EDULEVELS', 'MAX_EDULEVELS_NAME', 'EMPLOYMENT_TYPE', 'EMPLOYMENT_TYPE_NAME', 'MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'IS_INTERNSHIP', 'SALARY', 'REMOTE_TYPE', 'REMOTE_TYPE_NAME', 'ORIGINAL_PAY_PERIOD', 'SALARY_TO', 'SALARY_FROM', 'LOCATION', 'CITY', 'CITY_NAME', 'COUNTY', 'COUNTY_NAME', 'MSA', 'MSA_NAME', 'STATE', 'STATE_NAME', 'COUNTY_OUTGOING', 'COUNTY_NAME_OUTGOING', 'COUNTY_INCOMING', 'COUNTY_NAME_INCOMING', 'MSA_OUTGOING', 'MSA_NAME_OUTGOING', 'MSA_INCOMING', 'MSA_NAME_INCOMING', 'NAICS2', 'NAICS2_NAME', 'NAICS3', 'NAICS3_NAME', 'NAICS4', 'NAICS4_NAME', 'NAICS5', 'NAICS5_NAME', 'NAICS6', 'NAICS6_NAME', 'TITLE', 'TITLE_NAME', 'TITLE_CLEAN', 'SKILLS', 'SKILLS_NAME', 'SPECIALIZED_SKILLS', 'SPECIALIZED_SKILLS_NAME', 'CERTIFICATIONS', 'CERTIFICATIONS_NAME', 'COMMON_SKILLS', 'COMMON_SKILLS_NAME', 'SOFTWARE_SKILLS', 'SOFTWARE_SKILLS_NAME', 'ONET', 'ONET_NAME', 'ONET_2019', 'ONET_2019_NAME', 'CIP6', 'CIP6_NAME', 'CIP4', 'CIP4_NAME', 'CIP2', 'CIP2_NAME', 'SOC_2021_2', 'SOC_2021_2_NAME', 'SOC_2021_3', 'SOC_2021_3_NAME', 'SOC_2021_4', 'SOC_2021_4_NAME', 'SOC_2021_5', 'SOC_2021_5_NAME', 'LOT_CAREER_AREA', 'LOT_CAREER_AREA_NAME', 'LOT_OCCUPATION', 'LOT_OCCUPATION_NAME', 'LOT_SPECIALIZED_OCCUPATION', 'LOT_SPECIALIZED_OCCUPATION_NAME', 'LOT_OCCUPATION_GROUP', 'LOT_OCCUPATION_GROUP_NAME', 'LOT_V6_SPECIALIZED_OCCUPATION', 'LOT_V6_SPECIALIZED_OCCUPATION_NAME', 'LOT_V6_OCCUPATION', 'LOT_V6_OCCUPATION_NAME', 'LOT_V6_OCCUPATION_GROUP', 'LOT_V6_OCCUPATION_GROUP_NAME', 'LOT_V6_CAREER_AREA', 'LOT_V6_CAREER_AREA_NAME', 'SOC_2', 'SOC_2_NAME', 'SOC_3', 'SOC_3_NAME', 'SOC_4', 'SOC_4_NAME', 'SOC_5', 'SOC_5_NAME', 'LIGHTCAST_SECTORS', 'LIGHTCAST_SECTORS_NAME', 'NAICS_2022_2', 'NAICS_2022_2_NAME', 'NAICS_2022_3', 'NAICS_2022_3_NAME', 'NAICS_2022_4', 'NAICS_2022_4_NAME', 'NAICS_2022_5', 'NAICS_2022_5_NAME', 'NAICS_2022_6', 'NAICS_2022_6_NAME']\n\nFeatures selected for ML: ['DURATION', 'MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'TITLE_LENGTH', 'REMOTE_ENCODED']\nML dataset shape: (2243, 6)\n\nFeature statistics:\n          DURATION  MIN_YEARS_EXPERIENCE  MAX_YEARS_EXPERIENCE  TITLE_LENGTH  \\\ncount  2243.000000           2243.000000           2243.000000   2243.000000   \nmean     24.336603              3.729380              3.729380     21.384753   \nstd      14.621250              2.474196              2.474196      7.748297   \nmin       0.000000              0.000000              0.000000      6.000000   \n25%      12.000000              2.000000              2.000000     13.000000   \n50%      21.000000              3.000000              3.000000     21.000000   \n75%      35.000000              5.000000              5.000000     27.000000   \nmax      59.000000             12.000000             12.000000     70.000000   \n\n       REMOTE_ENCODED         SALARY  \ncount     2243.000000    2243.000000  \nmean         0.461881  105875.649576  \nstd          0.843057   34884.746685  \nmin          0.000000   31640.000000  \n25%          0.000000   80000.000000  \n50%          0.000000  100000.000000  \n75%          0.000000  125587.500000  \nmax          2.000000  338750.000000"
  },
  {
    "objectID": "ML.html#k-means-clustering",
    "href": "ML.html#k-means-clustering",
    "title": "Machine Learning Methods",
    "section": "3 2. K-Means Clustering",
    "text": "3 2. K-Means Clustering\n\n3.1 2.1 Determining Optimal Number of Clusters\nUse the elbow method to find the optimal number of clusters.\n\nif len(ml_features) &gt; 0 and len(df_ml) &gt; 0:\n    # Standardize features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(df_ml[ml_features])\n    \n    # Calculate inertia for different k values\n    inertias = []\n    K_range = range(2, 11)\n    \n    for k in K_range:\n        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n        kmeans.fit(X_scaled)\n        inertias.append(kmeans.inertia_)\n    \n    # Store scaled data for later use\n    X_scaled_stored = X_scaled.copy()\n    \n    # Create dataframe for plotting\n    elbow_df = pd.DataFrame({\n        'K': list(K_range),\n        'Inertia': inertias\n    })\n    \n    # Plot elbow curve\n    chart = elbow_df.hvplot.line(\n        x='K',\n        y='Inertia',\n        title='Elbow Method for Optimal K',\n        height=500,\n        width=800,\n        xlabel='Number of Clusters (K)',\n        ylabel='Inertia',\n        color='#3498db',\n        line_width=3\n    ) * elbow_df.hvplot.scatter(\n        x='K',\n        y='Inertia',\n        color='#3498db',\n        size=100\n    )\n    chart\nelse:\n    print(\"Insufficient data for clustering\")\n\n\n\n\n\n\n\nNoteChoosing Optimal K\n\n\n\nLook for the ‚Äúelbow‚Äù in the curve where the rate of decrease in inertia sharply changes. This indicates the optimal number of clusters.\n\n\n\n\n3.2 2.2 Applying K-Means Clustering\n\nif len(ml_features) &gt; 0 and len(df_ml) &gt; 0:\n    # Use k=4 as example - adjust based on elbow method\n    optimal_k = 4\n    \n    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n    clusters = kmeans.fit_predict(X_scaled_stored)\n    \n    # Create clean dataframe for visualization\n    df_viz = df_ml.copy()\n    df_viz['Cluster'] = clusters\n    \n    print(f\"Clustering complete with K={optimal_k}\")\n    print(\"\\nCluster distribution:\")\n    print(df_viz['Cluster'].value_counts().sort_index())\n    \n    # Calculate cluster statistics\n    print(\"\\nCluster characteristics (mean values):\")\n    cluster_stats = df_viz.groupby('Cluster')[ml_features + ['SALARY']].mean()\n    print(cluster_stats)\n\nClustering complete with K=4\n\nCluster distribution:\nCluster\n0    824\n1    467\n2    418\n3    534\nName: count, dtype: int64\n\nCluster characteristics (mean values):\n          DURATION  MIN_YEARS_EXPERIENCE  MAX_YEARS_EXPERIENCE  TITLE_LENGTH  \\\nCluster                                                                        \n0        14.291262              2.850728              2.850728     20.324029   \n1        21.111349              2.766595              2.766595     22.306210   \n2        27.983254              7.885167              7.885167     23.712919   \n3        39.803371              2.674157              2.674157     20.393258   \n\n         REMOTE_ENCODED         SALARY  \nCluster                                 \n0              0.000000   95992.574029  \n1              2.000000  106077.211991  \n2              0.244019  137974.093301  \n3              0.000000   95823.917603  \n\n\n\n\n3.3 2.3 Visualizing Clusters\n\nif len(ml_features) &gt;= 2 and len(df_ml) &gt; 0:\n    # Apply PCA for visualization\n    pca = PCA(n_components=2)\n    X_pca = pca.fit_transform(X_scaled_stored)\n    \n    # Add PCA components to viz dataframe\n    df_viz['PC1'] = X_pca[:, 0]\n    df_viz['PC2'] = X_pca[:, 1]\n    \n    print(f\"PCA explained variance: PC1={pca.explained_variance_ratio_[0]:.2%}, PC2={pca.explained_variance_ratio_[1]:.2%}\")\n    \n    # Create scatter plot\n    chart = df_viz.hvplot.scatter(\n        x='PC1',\n        y='PC2',\n        by='Cluster',\n        title=f'Job Clusters (K-Means, K={optimal_k})',\n        height=600,\n        width=900,\n        xlabel='First Principal Component',\n        ylabel='Second Principal Component',\n        size=50,\n        alpha=0.6,\n        legend='top_right'\n    )\n    chart\n\nPCA explained variance: PC1=41.00%, PC2=23.71%\n\n\n\n\n\n\n\n\nNoteCluster Visualization\n\n\n\nThe scatter plot shows job postings grouped by similarity. Points closer together share similar characteristics in terms of salary and other features.\n\n\n\n\n3.4 2.4 Cluster Salary Distribution\n\nif 'Cluster' in df_viz.columns:\n    # Box plot of salary by cluster\n    chart = df_viz.hvplot.box(\n        y='SALARY',\n        by='Cluster',\n        title='Salary Distribution by Job Cluster',\n        height=500,\n        width=900,\n        ylabel='Salary ($)',\n        xlabel='Cluster',\n        legend=False\n    )\n    chart"
  },
  {
    "objectID": "ML.html#cluster-interpretation",
    "href": "ML.html#cluster-interpretation",
    "title": "Machine Learning Methods",
    "section": "4 3. Cluster Interpretation",
    "text": "4 3. Cluster Interpretation\n\n4.1 3.1 Analyzing Cluster Characteristics\n\nif 'Cluster' in df_viz.columns:\n    print(\"Detailed Cluster Analysis:\\n\")\n    \n    for cluster_id in sorted(df_viz['Cluster'].unique()):\n        cluster_data = df_viz[df_viz['Cluster'] == cluster_id]\n        \n        print(f\"{'='*60}\")\n        print(f\"CLUSTER {cluster_id} (n={len(cluster_data):,} jobs)\")\n        print(f\"{'='*60}\")\n        print(f\"Average Salary: ${cluster_data['SALARY'].mean():,.2f}\")\n        print(f\"Median Salary: ${cluster_data['SALARY'].median():,.2f}\")\n        print(f\"Salary Range: ${cluster_data['SALARY'].min():,.0f} - ${cluster_data['SALARY'].max():,.0f}\")\n        \n        for feature in ml_features:\n            if feature in cluster_data.columns:\n                print(f\"Average {feature}: {cluster_data[feature].mean():.2f}\")\n        print()\n\nDetailed Cluster Analysis:\n\n============================================================\nCLUSTER 0 (n=824 jobs)\n============================================================\nAverage Salary: $95,992.57\nMedian Salary: $93,879.50\nSalary Range: $31,640 - $208,000\nAverage DURATION: 14.29\nAverage MIN_YEARS_EXPERIENCE: 2.85\nAverage MAX_YEARS_EXPERIENCE: 2.85\nAverage TITLE_LENGTH: 20.32\nAverage REMOTE_ENCODED: 0.00\n\n============================================================\nCLUSTER 1 (n=467 jobs)\n============================================================\nAverage Salary: $106,077.21\nMedian Salary: $105,000.00\nSalary Range: $37,500 - $240,000\nAverage DURATION: 21.11\nAverage MIN_YEARS_EXPERIENCE: 2.77\nAverage MAX_YEARS_EXPERIENCE: 2.77\nAverage TITLE_LENGTH: 22.31\nAverage REMOTE_ENCODED: 2.00\n\n============================================================\nCLUSTER 2 (n=418 jobs)\n============================================================\nAverage Salary: $137,974.09\nMedian Salary: $127,900.00\nSalary Range: $56,748 - $338,750\nAverage DURATION: 27.98\nAverage MIN_YEARS_EXPERIENCE: 7.89\nAverage MAX_YEARS_EXPERIENCE: 7.89\nAverage TITLE_LENGTH: 23.71\nAverage REMOTE_ENCODED: 0.24\n\n============================================================\nCLUSTER 3 (n=534 jobs)\n============================================================\nAverage Salary: $95,823.92\nMedian Salary: $94,800.00\nSalary Range: $39,520 - $193,000\nAverage DURATION: 39.80\nAverage MIN_YEARS_EXPERIENCE: 2.67\nAverage MAX_YEARS_EXPERIENCE: 2.67\nAverage TITLE_LENGTH: 20.39\nAverage REMOTE_ENCODED: 0.00\n\n\n\n\n\n4.2 3.2 Cluster Comparison Chart\n\nif 'Cluster' in df_viz.columns:\n    # Create comparison of average values across clusters\n    cluster_means = df_viz.groupby('Cluster')[ml_features + ['SALARY']].mean().reset_index()\n    \n    # Melt for grouped bar chart\n    cluster_means_melted = cluster_means.melt(\n        id_vars='Cluster',\n        var_name='Feature',\n        value_name='Average Value'\n    )\n    \n    chart = cluster_means_melted.hvplot.bar(\n        x='Cluster',\n        y='Average Value',\n        by='Feature',\n        title='Average Feature Values by Cluster',\n        height=500,\n        width=1000,\n        ylabel='Average Value',\n        xlabel='Cluster',\n        legend='top_right',\n        rot=0\n    )\n    chart"
  },
  {
    "objectID": "ML.html#predictive-modeling",
    "href": "ML.html#predictive-modeling",
    "title": "Machine Learning Methods",
    "section": "5 4. Predictive Modeling",
    "text": "5 4. Predictive Modeling\n\n5.1 4.1 Salary Prediction Model\nBuild a Random Forest model to predict salary based on available features.\n\nif len(ml_features) &gt; 0 and len(df_ml) &gt; 0:\n    # Prepare data for modeling\n    X = df_ml[ml_features]\n    y = df_ml['SALARY']\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    \n    # Train Random Forest model\n    rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n    rf_model.fit(X_train, y_train)\n    \n    # Make predictions\n    y_pred = rf_model.predict(X_test)\n    \n    # Calculate metrics\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_test, y_pred)\n    \n    print(\"Salary Prediction Model Results:\")\n    print(f\"{'='*50}\")\n    print(f\"R¬≤ Score: {r2:.4f}\")\n    print(f\"RMSE: ${rmse:,.2f}\")\n    print(f\"Mean Absolute Error: ${np.mean(np.abs(y_test - y_pred)):,.2f}\")\n    print()\n    \n    # Feature importance\n    feature_importance = pd.DataFrame({\n        'Feature': ml_features,\n        'Importance': rf_model.feature_importances_\n    }).sort_values('Importance', ascending=False)\n    \n    print(\"Feature Importance:\")\n    print(feature_importance)\n\nSalary Prediction Model Results:\n==================================================\nR¬≤ Score: 0.4316\nRMSE: $24,987.86\nMean Absolute Error: $16,118.47\n\nFeature Importance:\n                Feature  Importance\n0              DURATION    0.277763\n3          TITLE_LENGTH    0.243476\n2  MAX_YEARS_EXPERIENCE    0.241799\n1  MIN_YEARS_EXPERIENCE    0.197362\n4        REMOTE_ENCODED    0.039601\n\n\n\n\n5.2 4.2 Feature Importance Visualization\n\nif len(ml_features) &gt; 0 and len(df_ml) &gt; 0:\n    # Plot feature importance\n    chart = feature_importance.hvplot.barh(\n        x='Feature',\n        y='Importance',\n        title='Feature Importance for Salary Prediction',\n        height=400,\n        width=800,\n        color='#2ecc71',\n        xlabel='Importance',\n        ylabel='',\n        flip_yaxis=True\n    )\n    chart\n\n\n\n5.3 4.3 Prediction vs Actual\n\nif len(ml_features) &gt; 0 and len(df_ml) &gt; 0:\n    # Create comparison dataframe\n    comparison_df = pd.DataFrame({\n        'Actual': y_test,\n        'Predicted': y_pred\n    })\n    \n    # Scatter plot\n    chart = comparison_df.hvplot.scatter(\n        x='Actual',\n        y='Predicted',\n        title='Predicted vs Actual Salary',\n        height=600,\n        width=700,\n        xlabel='Actual Salary ($)',\n        ylabel='Predicted Salary ($)',\n        color='#3498db',\n        alpha=0.5,\n        size=30\n    )\n    \n    # Add perfect prediction line\n    perfect_line = pd.DataFrame({\n        'x': [comparison_df['Actual'].min(), comparison_df['Actual'].max()],\n        'y': [comparison_df['Actual'].min(), comparison_df['Actual'].max()]\n    })\n    \n    line = perfect_line.hvplot.line(\n        x='x',\n        y='y',\n        color='red',\n        line_dash='dashed',\n        line_width=2\n    )\n    \n    chart * line\n\n\n\n\n\n\n\nNoteModel Performance\n\n\n\nPoints closer to the red diagonal line indicate more accurate predictions. The R¬≤ score tells us how well the model explains salary variance."
  },
  {
    "objectID": "ML.html#model-insights-and-recommendations",
    "href": "ML.html#model-insights-and-recommendations",
    "title": "Machine Learning Methods",
    "section": "6 5. Model Insights and Recommendations",
    "text": "6 5. Model Insights and Recommendations\n\n6.1 5.1 Key Insights\n\nif 'Cluster' in df_viz.columns and len(ml_features) &gt; 0:\n    print(\"Machine Learning Analysis Summary\")\n    print(\"=\"*60)\n    print()\n    \n    # Cluster insights\n    print(f\"üìä CLUSTERING INSIGHTS:\")\n    print(f\"   - Identified {optimal_k} distinct job clusters\")\n    best_cluster = df_viz.groupby('Cluster')['SALARY'].median().idxmax()\n    best_salary = df_viz.groupby('Cluster')['SALARY'].median().max()\n    print(f\"   - Highest paying cluster: Cluster {best_cluster} (median: ${best_salary:,.0f})\")\n    print()\n    \n    # Model insights\n    print(f\"üí° PREDICTION MODEL INSIGHTS:\")\n    print(f\"   - Model achieves R¬≤ score of {r2:.4f}\")\n    print(f\"   - Average prediction error: ${np.mean(np.abs(y_test - y_pred)):,.0f}\")\n    print(f\"   - Most important feature: {feature_importance.iloc[0]['Feature']}\")\n    print()\n    \n    # Recommendations\n    print(f\"üéØ RECOMMENDATIONS FOR JOB SEEKERS:\")\n    print(f\"   - Target jobs in Cluster {best_cluster} for higher salaries\")\n    print(f\"   - Focus on developing: {feature_importance.iloc[0]['Feature']}\")\n    print(f\"   - Consider: {', '.join(feature_importance.head(3)['Feature'].tolist())}\")\n\nMachine Learning Analysis Summary\n============================================================\n\nüìä CLUSTERING INSIGHTS:\n   - Identified 4 distinct job clusters\n   - Highest paying cluster: Cluster 2 (median: $127,900)\n\nüí° PREDICTION MODEL INSIGHTS:\n   - Model achieves R¬≤ score of 0.4316\n   - Average prediction error: $16,118\n   - Most important feature: DURATION\n\nüéØ RECOMMENDATIONS FOR JOB SEEKERS:\n   - Target jobs in Cluster 2 for higher salaries\n   - Focus on developing: DURATION\n   - Consider: DURATION, TITLE_LENGTH, MAX_YEARS_EXPERIENCE"
  },
  {
    "objectID": "ML.html#next-steps",
    "href": "ML.html#next-steps",
    "title": "Machine Learning Methods",
    "section": "7 Next Steps",
    "text": "7 Next Steps\nProceed to NLP Methods to extract insights from job descriptions using text analysis techniques."
  },
  {
    "objectID": "ML.html#references",
    "href": "ML.html#references",
    "title": "Machine Learning Methods",
    "section": "8 References",
    "text": "8 References\n\nScikit-learn documentation\nK-Means clustering best practices\nPCA for dimensionality reduction\nRandom Forest regression\nAnalysis tools: Python, pandas, hvPlot, Panel"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This document presents a comprehensive exploratory data analysis of the 2024 job market. We examine hiring trends, salary distributions, geographic patterns, employment types, and remote work opportunities to provide actionable insights for job seekers and market analysts.\n\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport panel as pn\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Enable panel for rendering\npn.extension()\n\n# Color palette\nCOLORS = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c', '#34495e', '#e67e22']\n\n# Load cleaned data\ndf = pd.read_csv('lightcast_job_postings.csv')\n\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Analysis period: 2024 Job Market\")\nprint(f\"Total job postings analyzed: {len(df):,}\")\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset shape: (72498, 131)\nAnalysis period: 2024 Job Market\nTotal job postings analyzed: 72,498"
  },
  {
    "objectID": "eda.html#introduction",
    "href": "eda.html#introduction",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This document presents a comprehensive exploratory data analysis of the 2024 job market. We examine hiring trends, salary distributions, geographic patterns, employment types, and remote work opportunities to provide actionable insights for job seekers and market analysts.\n\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport panel as pn\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Enable panel for rendering\npn.extension()\n\n# Color palette\nCOLORS = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c', '#34495e', '#e67e22']\n\n# Load cleaned data\ndf = pd.read_csv('lightcast_job_postings.csv')\n\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Analysis period: 2024 Job Market\")\nprint(f\"Total job postings analyzed: {len(df):,}\")\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset shape: (72498, 131)\nAnalysis period: 2024 Job Market\nTotal job postings analyzed: 72,498"
  },
  {
    "objectID": "eda.html#job-title-analysis",
    "href": "eda.html#job-title-analysis",
    "title": "Exploratory Data Analysis",
    "section": "2 1. Job Title Analysis",
    "text": "2 1. Job Title Analysis\n\n2.1 1.1 Top In-Demand Job Titles\nUnderstanding which job titles dominate the market helps identify high-demand roles and emerging career opportunities.\n\n# Get top 15 job titles\ntitle_counts = df['TITLE_NAME'].value_counts().head(15)\ntitle_df = pd.DataFrame({\n    'Job Title': title_counts.index, \n    'Count': title_counts.values,\n    'Percentage': (title_counts.values / len(df) * 100).round(2)\n})\n\n# Create interactive horizontal bar chart\ntitle_df.hvplot.barh(\n    x='Job Title',\n    y='Count',\n    title='Top 15 Most In-Demand Job Titles (2024)',\n    height=600,\n    width=900,\n    color='#3498db',\n    hover_cols=['Percentage'],\n    ylabel='',\n    xlabel='Number of Job Postings',\n    flip_yaxis=True\n)\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nThe top job titles reveal where the market has the highest demand. Software engineers, data analysts, and project managers consistently lead the rankings, indicating strong demand for technical and leadership roles."
  },
  {
    "objectID": "eda.html#employment-type-distribution",
    "href": "eda.html#employment-type-distribution",
    "title": "Exploratory Data Analysis",
    "section": "3 2. Employment Type Distribution",
    "text": "3 2. Employment Type Distribution\n\n3.1 2.1 Full-Time vs Part-Time vs Contract\nUnderstanding employment type distribution helps job seekers target positions matching their career preferences.\n\n# Count jobs by employment type\nemployment_counts = df['EMPLOYMENT_TYPE_NAME'].value_counts()\nemployment_df = pd.DataFrame({\n    'Employment Type': employment_counts.index, \n    'Count': employment_counts.values,\n    'Percentage': (employment_counts.values / employment_counts.sum() * 100).round(1)\n})\n\n# Create bar chart\nemployment_df.hvplot.bar(\n    x='Employment Type',\n    y='Count',\n    title='Job Market Distribution by Employment Type (2024)',\n    height=500,\n    width=800,\n    color='Employment Type',\n    cmap=COLORS,\n    hover_cols=['Percentage'],\n    ylabel='Number of Job Postings',\n    rot=45\n)\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nThe distribution of employment types shows whether the market primarily offers full-time stability, part-time flexibility, or contract-based project work. This helps candidates align their search with available opportunities."
  },
  {
    "objectID": "eda.html#remote-work-analysis",
    "href": "eda.html#remote-work-analysis",
    "title": "Exploratory Data Analysis",
    "section": "4 3. Remote Work Analysis",
    "text": "4 3. Remote Work Analysis\n\n4.1 3.1 Remote vs Hybrid vs On-Site\nThe prevalence of remote work reflects post-pandemic hiring trends and employer flexibility.\n\n# Count jobs by remote type\nremote_counts = df['REMOTE_TYPE_NAME'].value_counts()\nremote_df = pd.DataFrame({\n    'Remote Type': remote_counts.index,\n    'Count': remote_counts.values,\n    'Percentage': (remote_counts.values / remote_counts.sum() * 100).round(1)\n})\n\n# Create bar chart with percentages\nremote_df.hvplot.bar(\n    x='Remote Type',\n    y='Count',\n    title='Job Market Distribution: Remote, Hybrid & On-Site Opportunities (2024)',\n    height=500,\n    width=800,\n    color='Remote Type',\n    cmap=COLORS,\n    hover_cols=['Percentage'],\n    ylabel='Number of Job Postings',\n    xlabel='Work Location Type',\n    rot=0\n)\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nThe balance between remote, hybrid, and on-site jobs indicates industry acceptance of flexible work arrangements. Higher remote percentages suggest employers prioritize talent acquisition over geographic constraints."
  },
  {
    "objectID": "eda.html#salary-analysis",
    "href": "eda.html#salary-analysis",
    "title": "Exploratory Data Analysis",
    "section": "5 4. Salary Analysis",
    "text": "5 4. Salary Analysis\n\n5.1 4.1 Salary Distribution Overview\nSalary analysis reveals compensation trends and helps job seekers set realistic expectations.\n\n# Filter out unrealistic salaries (likely errors)\nsalary_data = df[df['SALARY'].notna() & (df['SALARY'] &gt; 0) & (df['SALARY'] &lt; 500000)].copy()\n\nmean_salary = salary_data['SALARY'].mean()\nmedian_salary = salary_data['SALARY'].median()\n\n# Create histogram\nsalary_data.hvplot.hist(\n    y='SALARY',\n    bins=50,\n    title='Salary Distribution in Job Market (2024)',\n    height=500,\n    width=900,\n    color='#3498db',\n    alpha=0.7,\n    ylabel='Number of Job Postings',\n    xlabel='Annual Salary ($)'\n)\n\n\n\n\n\n  \n\n\n\n\n\nprint(f\"Salary Statistics:\")\nprint(f\"  Mean: ${mean_salary:,.2f}\")\nprint(f\"  Median: ${median_salary:,.2f}\")\nprint(f\"  25th Percentile: ${salary_data['SALARY'].quantile(0.25):,.2f}\")\nprint(f\"  75th Percentile: ${salary_data['SALARY'].quantile(0.75):,.2f}\")\n\nSalary Statistics:\n  Mean: $117,928.95\n  Median: $116,300.00\n  25th Percentile: $84,919.50\n  75th Percentile: $145,600.00\n\n\n\n\n5.2 4.2 Salary by Top Job Titles\nCompare compensation across the most common job titles to identify high-paying roles.\n\n# Get top 10 job titles\ntop_titles = df['TITLE_NAME'].value_counts().head(10).index\n\n# Filter salary data for top titles\nsalary_by_title = df[\n    df['TITLE_NAME'].isin(top_titles) & \n    df['SALARY'].notna() & \n    (df['SALARY'] &gt; 0) & \n    (df['SALARY'] &lt; 500000)\n].groupby('TITLE_NAME')['SALARY'].agg(['median', 'mean', 'count']).reset_index()\n\nsalary_by_title = salary_by_title.sort_values('median', ascending=True)\n\n# Create horizontal bar chart\nsalary_by_title.hvplot.barh(\n    x='TITLE_NAME',\n    y='median',\n    title='Median Salary by Top 10 Job Titles (2024)',\n    height=600,\n    width=900,\n    color='#2ecc71',\n    hover_cols=['mean', 'count'],\n    ylabel='',\n    xlabel='Median Annual Salary ($)',\n    flip_yaxis=True\n)\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nSalary variations across job titles highlight which roles command premium compensation. Technical roles often show higher median salaries compared to administrative positions.\n\n\n\n\n5.3 4.3 Salary Comparison: Remote vs On-Site\nDoes remote work affect compensation? Let‚Äôs find out.\n\n# Compare salary by remote type\nsalary_remote = df[\n    df['SALARY'].notna() & \n    (df['SALARY'] &gt; 0) & \n    (df['SALARY'] &lt; 500000)\n].groupby('REMOTE_TYPE_NAME')['SALARY'].agg(['median', 'mean', 'count']).reset_index()\n\n# Reshape for grouped bar chart\nsalary_remote_melted = salary_remote.melt(\n    id_vars='REMOTE_TYPE_NAME',\n    value_vars=['median', 'mean'],\n    var_name='Metric',\n    value_name='Salary'\n)\n\n# Create grouped bar chart\nsalary_remote_melted.hvplot.bar(\n    x='REMOTE_TYPE_NAME',\n    y='Salary',\n    by='Metric',\n    title='Salary Comparison: Remote vs Hybrid vs On-Site (2024)',\n    height=500,\n    width=800,\n    ylabel='Annual Salary ($)',\n    xlabel='Work Location Type',\n    legend='top_right',\n    rot=0\n)\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nComparing salaries across work arrangements reveals whether remote positions offer competitive compensation or if on-site roles command premium pay due to location requirements."
  },
  {
    "objectID": "eda.html#geographic-analysis",
    "href": "eda.html#geographic-analysis",
    "title": "Exploratory Data Analysis",
    "section": "6 5. Geographic Analysis",
    "text": "6 5. Geographic Analysis\n\n6.1 5.1 Top States for Job Opportunities\nGeographic distribution shows where job opportunities are concentrated.\n\n# Get top 15 states by job postings\nstate_counts = df['STATE_NAME'].value_counts().head(15)\nstate_df = pd.DataFrame({\n    'State': state_counts.index,\n    'Job Postings': state_counts.values,\n    'Percentage': (state_counts.values / len(df) * 100).round(2)\n})\n\n# Create bar chart\nstate_df.hvplot.bar(\n    x='State',\n    y='Job Postings',\n    title='Top 15 States by Number of Job Postings (2024)',\n    height=600,\n    width=1000,\n    color='#9b59b6',\n    hover_cols=['Percentage'],\n    ylabel='Number of Job Postings',\n    xlabel='State',\n    rot=45\n)\n\n\n\n\n\n  \n\n\n\n\n\n\n6.2 5.2 Salary by State (Top 10 States)\nCompare salaries across states to identify high-paying regions.\n\n# Get salary data for top 10 states\ntop_states = df['STATE_NAME'].value_counts().head(10).index\n\nsalary_by_state = df[\n    df['STATE_NAME'].isin(top_states) & \n    df['SALARY'].notna() & \n    (df['SALARY'] &gt; 0) & \n    (df['SALARY'] &lt; 500000)\n].groupby('STATE_NAME')['SALARY'].median().sort_values(ascending=False).reset_index()\n\n# Create bar chart\nsalary_by_state.hvplot.bar(\n    x='STATE_NAME',\n    y='SALARY',\n    title='Median Salary by Top 10 States (2024)',\n    height=500,\n    width=900,\n    color='#f39c12',\n    ylabel='Median Annual Salary ($)',\n    xlabel='State',\n    rot=45\n)\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nGeographic salary variations reflect regional cost of living and market demand. Tech hubs like California and New York typically show higher median salaries."
  },
  {
    "objectID": "eda.html#top-hiring-companies",
    "href": "eda.html#top-hiring-companies",
    "title": "Exploratory Data Analysis",
    "section": "7 6. Top Hiring Companies",
    "text": "7 6. Top Hiring Companies\n\n7.1 6.1 Companies with Most Job Openings\nIdentifying top hiring companies helps job seekers target organizations with multiple opportunities.\n\n# Get top 15 companies by job postings\ncompany_counts = df['COMPANY_NAME'].value_counts().head(15)\ncompany_df = pd.DataFrame({\n    'Company': company_counts.index,\n    'Job Postings': company_counts.values\n})\n\n# Create horizontal bar chart\ncompany_df.hvplot.barh(\n    x='Company',\n    y='Job Postings',\n    title='Top 15 Companies by Number of Job Postings (2024)',\n    height=700,\n    width=900,\n    color='#e74c3c',\n    ylabel='',\n    xlabel='Number of Job Postings',\n    flip_yaxis=True\n)\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nCompanies with the highest number of job postings indicate organizations actively expanding their workforce. These companies may offer better opportunities for multiple role applications and career growth."
  },
  {
    "objectID": "eda.html#multi-dimensional-analysis",
    "href": "eda.html#multi-dimensional-analysis",
    "title": "Exploratory Data Analysis",
    "section": "8 7. Multi-Dimensional Analysis",
    "text": "8 7. Multi-Dimensional Analysis\n\n8.1 7.1 Job Postings Over Time (if date data available)\n\n# Check if date column exists\nif 'POSTED_DATE' in df.columns or 'POST_DATE' in df.columns:\n    date_col = 'POSTED_DATE' if 'POSTED_DATE' in df.columns else 'POST_DATE'\n    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n    \n    # Filter valid dates\n    time_data = df[df[date_col].notna()].copy()\n    time_data['Month'] = time_data[date_col].dt.to_period('M').astype(str)\n    \n    # Group by month\n    monthly_counts = time_data['Month'].value_counts().sort_index()\n    monthly_df = pd.DataFrame({\n        'Month': monthly_counts.index,\n        'Job Postings': monthly_counts.values\n    })\n    \n    # Create line chart\n    monthly_df.hvplot.line(\n        x='Month',\n        y='Job Postings',\n        title='Job Posting Trends Over Time (2024)',\n        height=500,\n        width=900,\n        color='#3498db',\n        line_width=3,\n        ylabel='Number of Job Postings',\n        xlabel='Month'\n    )\nelse:\n    print(\"Date column not available for time-series analysis\")\n\nDate column not available for time-series analysis"
  },
  {
    "objectID": "eda.html#next-steps",
    "href": "eda.html#next-steps",
    "title": "Exploratory Data Analysis",
    "section": "9 Next Steps",
    "text": "9 Next Steps\nWith comprehensive EDA completed, we proceed to:\n\nSkill Gap Analysis - Compare our team‚Äôs skills against market requirements\nMachine Learning Methods - Apply clustering to identify job categories\nNLP Analysis - Extract insights from job descriptions"
  },
  {
    "objectID": "eda.html#references",
    "href": "eda.html#references",
    "title": "Exploratory Data Analysis",
    "section": "10 References",
    "text": "10 References\n\nData source: Lightcast Job Postings Dataset (2024)\nVisualization tools: hvPlot, Panel, Python\nAnalysis framework: Standard EDA best practices"
  },
  {
    "objectID": "SKILL_GAP_ANALYSIS.html",
    "href": "SKILL_GAP_ANALYSIS.html",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "This analysis compares our team‚Äôs current skill levels against the skills demanded in the IT job market. By identifying gaps between our capabilities and market requirements, we can develop targeted learning strategies to enhance our competitiveness as job candidates.\n\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport panel as pn\nfrom collections import Counter\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Enable panel for rendering\npn.extension()\n\n# Color palette\nCOLORS = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c']\n\n# Load cleaned job data\ndf = pd.read_csv('lightcast_job_postings.csv')\nprint(f\"Analyzing {len(df):,} job postings for skill requirements\")\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing 72,498 job postings for skill requirements"
  },
  {
    "objectID": "SKILL_GAP_ANALYSIS.html#introduction",
    "href": "SKILL_GAP_ANALYSIS.html#introduction",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "This analysis compares our team‚Äôs current skill levels against the skills demanded in the IT job market. By identifying gaps between our capabilities and market requirements, we can develop targeted learning strategies to enhance our competitiveness as job candidates.\n\nimport pandas as pd\nimport numpy as np\nimport hvplot.pandas\nimport panel as pn\nfrom collections import Counter\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Enable panel for rendering\npn.extension()\n\n# Color palette\nCOLORS = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12', '#9b59b6', '#1abc9c']\n\n# Load cleaned job data\ndf = pd.read_csv('lightcast_job_postings.csv')\nprint(f\"Analyzing {len(df):,} job postings for skill requirements\")\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing 72,498 job postings for skill requirements"
  },
  {
    "objectID": "SKILL_GAP_ANALYSIS.html#team-skill-assessment",
    "href": "SKILL_GAP_ANALYSIS.html#team-skill-assessment",
    "title": "Skill Gap Analysis",
    "section": "2 1. Team Skill Assessment",
    "text": "2 1. Team Skill Assessment\n\n2.1 1.1 Creating Team Skills Profile\nEach team member rates their proficiency in key IT skills on a scale of 1-5: - 1 = Beginner (aware of the skill) - 2 = Basic knowledge (can perform simple tasks) - 3 = Intermediate (comfortable with common scenarios) - 4 = Advanced (can handle complex problems) - 5 = Expert (can teach others and solve any problem)\n\n# Team member skill assessments\n# TODO: Replace with actual team member names and skill ratings\nskills_data = {\n    \"Name\": [\"Tuba Anwar\", \"Kriti Singh\", \"Soham Deshkhaire\"],\n    \"Python\": [4, 3, 4],\n    \"SQL\": [3, 4, 3],\n    \"Machine Learning\": [3, 2, 4],\n    \"Data Analysis\": [4, 4, 3],\n    \"Cloud Computing\": [2, 2, 3],\n    \"Java\": [3, 2, 2],\n    \"JavaScript\": [2, 3, 3],\n    \"R\": [3, 3, 2],\n    \"Tableau\": [3, 4, 2],\n    \"Excel\": [4, 5, 3],\n    \"AWS\": [2, 1, 2],\n    \"Azure\": [1, 2, 2],\n    \"Docker\": [2, 1, 3],\n    \"Git\": [3, 3, 4],\n    \"Power BI\": [2, 3, 2],\n    \"Spark\": [2, 2, 3],\n    \"TensorFlow\": [2, 1, 3],\n    \"NLP\": [3, 2, 3]\n}\n\ndf_team_skills = pd.DataFrame(skills_data)\ndf_team_skills.set_index(\"Name\", inplace=True)\n\n# Display team skills\nprint(\"Team Skills Profile:\")\nprint(df_team_skills)\n\n# Calculate team averages\nteam_avg = df_team_skills.mean().sort_values(ascending=False)\nprint(f\"\\nTeam Average Skills (sorted):\")\nprint(team_avg)\n\nTeam Skills Profile:\n                  Python  SQL  Machine Learning  Data Analysis  \\\nName                                                             \nTuba Anwar             4    3                 3              4   \nKriti Singh            3    4                 2              4   \nSoham Deshkhaire       4    3                 4              3   \n\n                  Cloud Computing  Java  JavaScript  R  Tableau  Excel  AWS  \\\nName                                                                          \nTuba Anwar                      2     3           2  3        3      4    2   \nKriti Singh                     2     2           3  3        4      5    1   \nSoham Deshkhaire                3     2           3  2        2      3    2   \n\n                  Azure  Docker  Git  Power BI  Spark  TensorFlow  NLP  \nName                                                                    \nTuba Anwar            1       2    3         2      2           2    3  \nKriti Singh           2       1    3         3      2           1    2  \nSoham Deshkhaire      2       3    4         2      3           3    3  \n\nTeam Average Skills (sorted):\nExcel               4.000000\nPython              3.666667\nData Analysis       3.666667\nSQL                 3.333333\nGit                 3.333333\nTableau             3.000000\nMachine Learning    3.000000\nR                   2.666667\nNLP                 2.666667\nJavaScript          2.666667\nCloud Computing     2.333333\nJava                2.333333\nPower BI            2.333333\nSpark               2.333333\nDocker              2.000000\nTensorFlow          2.000000\nAWS                 1.666667\nAzure               1.666667\ndtype: float64\n\n\n\n\n2.2 1.2 Team Skills Heatmap\nVisualize each team member‚Äôs strengths and weaknesses across all skills.\n\n# Create heatmap using hvPlot\nheatmap = df_team_skills.T.hvplot.heatmap(\n    title='Team Skill Proficiency Heatmap',\n    cmap='RdYlGn',\n    height=700,\n    width=800,\n    xlabel='Team Member',\n    ylabel='Skill',\n    clabel='Proficiency Level',\n    rot=0\n)\nheatmap\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Observation\n\n\n\nThe heatmap reveals individual strengths and team-wide patterns. Skills with consistently high ratings represent team strengths, while low ratings across the board indicate collective skill gaps requiring attention.\n\n\n\n\n2.3 1.3 Team Average Skills Bar Chart\nCompare team average proficiency across all skills.\n\n# Prepare data for bar chart\nteam_avg_df = team_avg.reset_index()\nteam_avg_df.columns = ['Skill', 'Average Proficiency']\n\n# Create bar chart\nchart = team_avg_df.sort_values('Average Proficiency', ascending=True).hvplot.barh(\n    x='Skill',\n    y='Average Proficiency',\n    title='Team Average Skill Proficiency',\n    height=700,\n    width=900,\n    color='#3498db',\n    xlabel='Average Proficiency (1-5)',\n    ylabel='',\n    flip_yaxis=True\n)\nchart"
  },
  {
    "objectID": "SKILL_GAP_ANALYSIS.html#market-skill-demand-analysis",
    "href": "SKILL_GAP_ANALYSIS.html#market-skill-demand-analysis",
    "title": "Skill Gap Analysis",
    "section": "3 2. Market Skill Demand Analysis",
    "text": "3 2. Market Skill Demand Analysis\n\n3.1 2.1 Extracting Skills from Job Descriptions\nWe analyze job postings to identify the most in-demand skills in the IT job market.\n\n# Define comprehensive skill keywords to search for\nskill_keywords = {\n    'Python': ['python', 'python3', 'py'],\n    'SQL': ['sql', 'mysql', 'postgresql', 'sql server', 'oracle sql'],\n    'Machine Learning': ['machine learning', 'ml', 'deep learning', 'neural network'],\n    'Data Analysis': ['data analysis', 'data analytics', 'analytical'],\n    'Cloud Computing': ['cloud', 'cloud computing', 'cloud services'],\n    'Java': ['java', 'java8', 'java 8'],\n    'JavaScript': ['javascript', 'js', 'node.js', 'nodejs'],\n    'R': [' r ', 'r programming', 'rstudio'],\n    'Tableau': ['tableau'],\n    'Excel': ['excel', 'microsoft excel', 'advanced excel'],\n    'AWS': ['aws', 'amazon web services', 'ec2', 's3'],\n    'Azure': ['azure', 'microsoft azure'],\n    'Docker': ['docker', 'containerization'],\n    'Git': ['git', 'github', 'version control'],\n    'Power BI': ['power bi', 'powerbi'],\n    'Spark': ['spark', 'apache spark', 'pyspark'],\n    'TensorFlow': ['tensorflow', 'tf'],\n    'NLP': ['nlp', 'natural language processing', 'text mining']\n}\n\n# Function to extract skills from text\ndef extract_skills(text):\n    if pd.isna(text):\n        return []\n    text = str(text).lower()\n    found_skills = []\n    for skill, keywords in skill_keywords.items():\n        for keyword in keywords:\n            if keyword in text:\n                found_skills.append(skill)\n                break\n    return found_skills\n\n# Extract skills from job titles and descriptions (if available)\nif 'TITLE_NAME' in df.columns:\n    df['extracted_skills'] = df['TITLE_NAME'].apply(extract_skills)\n    \n    # If job description available, combine with title\n    if 'DESCRIPTION' in df.columns or 'JOB_DESCRIPTION' in df.columns:\n        desc_col = 'DESCRIPTION' if 'DESCRIPTION' in df.columns else 'JOB_DESCRIPTION'\n        df['extracted_skills'] = df.apply(\n            lambda row: list(set(extract_skills(row['TITLE_NAME']) + extract_skills(row[desc_col]))),\n            axis=1\n        )\n\n# Count skill occurrences\nall_skills = [skill for skills_list in df['extracted_skills'] for skill in skills_list]\nskill_counts = Counter(all_skills)\nmarket_skills_df = pd.DataFrame(skill_counts.items(), columns=['Skill', 'Job Postings'])\nmarket_skills_df = market_skills_df.sort_values('Job Postings', ascending=False)\nmarket_skills_df['Percentage'] = (market_skills_df['Job Postings'] / len(df) * 100).round(2)\n\nprint(\"Market Skill Demand (Top Skills):\")\nprint(market_skills_df.head(15))\n\nMarket Skill Demand (Top Skills):\n               Skill  Job Postings  Percentage\n1      Data Analysis          2654        3.66\n0    Cloud Computing          2241        3.09\n2         TensorFlow           497        0.69\n6                Git           226        0.31\n3                SQL           167        0.23\n5              Azure           127        0.18\n4               Java            57        0.08\n8              Excel            43        0.06\n10            Python            28        0.04\n7   Machine Learning            23        0.03\n9            Tableau            19        0.03\n11               AWS            15        0.02\n12             Spark             1        0.00\n\n\n\n\n3.2 2.2 Top In-Demand Skills Visualization\n\n# Create bar chart of top 15 market skills\ntop_market_skills = market_skills_df.head(15).sort_values('Job Postings', ascending=True)\n\nchart = top_market_skills.hvplot.barh(\n    x='Skill',\n    y='Job Postings',\n    title='Top 15 Most In-Demand Skills in Job Market (2024)',\n    height=600,\n    width=900,\n    color='#3498db',\n    hover_cols=['Percentage'],\n    xlabel='Number of Job Postings Requiring Skill',\n    ylabel='',\n    flip_yaxis=True\n)\nchart\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteMarket Insight\n\n\n\nThese are the skills most frequently mentioned in job postings. High-demand skills represent critical areas where job seekers should focus their learning efforts."
  },
  {
    "objectID": "SKILL_GAP_ANALYSIS.html#skill-gap-identification",
    "href": "SKILL_GAP_ANALYSIS.html#skill-gap-identification",
    "title": "Skill Gap Analysis",
    "section": "4 3. Skill Gap Identification",
    "text": "4 3. Skill Gap Identification\n\n4.1 3.1 Comparing Team Skills to Market Demand\nNow we identify gaps between our team‚Äôs capabilities and what the market requires.\n\n# Normalize market demand to 1-5 scale for comparison\n# We'll scale based on percentage of jobs requiring each skill\nmax_percentage = market_skills_df['Percentage'].max()\n\n# Create comparison dataframe\ncomparison_data = []\n\nfor skill in df_team_skills.columns:\n    team_avg_skill = team_avg[skill]\n    \n    # Get market demand (normalized to 1-5 scale)\n    if skill in market_skills_df['Skill'].values:\n        market_pct = market_skills_df[market_skills_df['Skill'] == skill]['Percentage'].values[0]\n        market_demand = (market_pct / max_percentage) * 5  # Scale to 1-5\n    else:\n        market_demand = 0\n    \n    gap = market_demand - team_avg_skill\n    \n    comparison_data.append({\n        'Skill': skill,\n        'Team Average': team_avg_skill,\n        'Market Demand': market_demand,\n        'Gap': gap,\n        'Gap_Category': 'Strength' if gap &lt;= 0 else ('Moderate Gap' if gap &lt;= 2 else 'Critical Gap')\n    })\n\ncomparison_df = pd.DataFrame(comparison_data)\ncomparison_df = comparison_df.sort_values('Gap', ascending=False)\n\nprint(\"Skill Gap Analysis:\")\nprint(comparison_df)\n\nSkill Gap Analysis:\n               Skill  Team Average  Market Demand       Gap  Gap_Category\n4    Cloud Computing      2.333333       4.221311  1.887978  Moderate Gap\n3      Data Analysis      3.666667       5.000000  1.333333  Moderate Gap\n16        TensorFlow      2.000000       0.942623 -1.057377      Strength\n11             Azure      1.666667       0.245902 -1.420765      Strength\n10               AWS      1.666667       0.027322 -1.639344      Strength\n12            Docker      2.000000       0.000000 -2.000000      Strength\n5               Java      2.333333       0.109290 -2.224044      Strength\n15             Spark      2.333333       0.000000 -2.333333      Strength\n14          Power BI      2.333333       0.000000 -2.333333      Strength\n6         JavaScript      2.666667       0.000000 -2.666667      Strength\n17               NLP      2.666667       0.000000 -2.666667      Strength\n7                  R      2.666667       0.000000 -2.666667      Strength\n13               Git      3.333333       0.423497 -2.909836      Strength\n2   Machine Learning      3.000000       0.040984 -2.959016      Strength\n8            Tableau      3.000000       0.040984 -2.959016      Strength\n1                SQL      3.333333       0.314208 -3.019126      Strength\n0             Python      3.666667       0.054645 -3.612022      Strength\n9              Excel      4.000000       0.081967 -3.918033      Strength\n\n\n\n\n4.2 3.2 Skill Gap Visualization\n\n# Create comparison dataframe for visualization\ncomparison_melted = comparison_df.melt(\n    id_vars='Skill',\n    value_vars=['Team Average', 'Market Demand'],\n    var_name='Metric',\n    value_name='Level'\n)\n\n# Sort by Market Demand\nskill_order = comparison_df.sort_values('Market Demand', ascending=False)['Skill'].tolist()\n\n# Create grouped bar chart\nchart = comparison_melted.hvplot.bar(\n    x='Skill',\n    y='Level',\n    by='Metric',\n    title='Team Skills vs Market Demand Comparison',\n    height=600,\n    width=1000,\n    ylabel='Proficiency / Demand Level (1-5)',\n    xlabel='Skill',\n    legend='top_right',\n    rot=45\n)\nchart\n\n\n\n\n\n  \n\n\n\n\n\n\n4.3 3.3 Skill Gap Priority Matrix\nIdentify which skills need immediate attention based on gap size.\n\n# Create scatter plot: Team proficiency vs Market demand\nchart = comparison_df.hvplot.scatter(\n    x='Team Average',\n    y='Market Demand',\n    by='Gap_Category',\n    size=abs(comparison_df['Gap']) * 50,\n    hover_cols=['Skill', 'Gap'],\n    title='Skill Gap Priority Matrix',\n    height=700,\n    width=900,\n    xlabel='Team Average Proficiency (1-5)',\n    ylabel='Market Demand Level (1-5 normalized)',\n    legend='top_left',\n    color=['#2ecc71', '#f39c12', '#e74c3c']\n)\nchart\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nImportantCritical Skill Gaps\n\n\n\nSkills appearing in the lower right area (high market demand, low team proficiency) represent critical gaps requiring immediate attention. These should be prioritized in our learning plan."
  },
  {
    "objectID": "SKILL_GAP_ANALYSIS.html#individual-skill-gap-analysis",
    "href": "SKILL_GAP_ANALYSIS.html#individual-skill-gap-analysis",
    "title": "Skill Gap Analysis",
    "section": "5 4. Individual Skill Gap Analysis",
    "text": "5 4. Individual Skill Gap Analysis\n\n5.1 4.1 Skill Gaps by Team Member\nIdentify personalized skill development needs for each team member.\n\n# Calculate individual gaps\nindividual_gaps = []\n\nfor member in df_team_skills.index:\n    for skill in df_team_skills.columns:\n        member_skill = df_team_skills.loc[member, skill]\n        \n        # Get market demand\n        if skill in market_skills_df['Skill'].values:\n            market_pct = market_skills_df[market_skills_df['Skill'] == skill]['Percentage'].values[0]\n            market_demand = (market_pct / max_percentage) * 5\n        else:\n            market_demand = 0\n        \n        gap = market_demand - member_skill\n        \n        if gap &gt; 0:  # Only include gaps (areas for improvement)\n            individual_gaps.append({\n                'Member': member,\n                'Skill': skill,\n                'Current Level': member_skill,\n                'Market Demand': market_demand,\n                'Gap': gap\n            })\n\nindividual_gaps_df = pd.DataFrame(individual_gaps)\nindividual_gaps_df = individual_gaps_df.sort_values(['Member', 'Gap'], ascending=[True, False])\n\n# Show top 5 gaps per member\nprint(\"Top 5 Skill Gaps per Team Member:\")\nfor member in df_team_skills.index:\n    print(f\"\\n{member}:\")\n    member_gaps = individual_gaps_df[individual_gaps_df['Member'] == member].head(5)\n    print(member_gaps[['Skill', 'Current Level', 'Gap']].to_string(index=False))\n\nTop 5 Skill Gaps per Team Member:\n\nTuba Anwar:\n          Skill  Current Level      Gap\nCloud Computing              2 2.221311\n  Data Analysis              4 1.000000\n\nKriti Singh:\n          Skill  Current Level      Gap\nCloud Computing              2 2.221311\n  Data Analysis              4 1.000000\n\nSoham Deshkhaire:\n          Skill  Current Level      Gap\n  Data Analysis              3 2.000000\nCloud Computing              3 1.221311\n\n\n\n\n5.2 4.2 Individual Gap Visualization\n\n# Get top 5 gaps per member\ntop_individual_gaps = individual_gaps_df.groupby('Member').head(5)\n\n# Create bar chart for each member\nfor member in df_team_skills.index:\n    member_data = top_individual_gaps[top_individual_gaps['Member'] == member].sort_values('Gap', ascending=True)\n    \n    plot = member_data.hvplot.barh(\n        x='Skill',\n        y='Gap',\n        title=f'Top 5 Skill Gaps - {member}',\n        height=400,\n        width=700,\n        color='#e74c3c',\n        xlabel='Skill Gap (Market - Current)',\n        ylabel='',\n        flip_yaxis=True\n    )\n    \n    display(plot)"
  },
  {
    "objectID": "SKILL_GAP_ANALYSIS.html#skill-development-plan",
    "href": "SKILL_GAP_ANALYSIS.html#skill-development-plan",
    "title": "Skill Gap Analysis",
    "section": "6 5. Skill Development Plan",
    "text": "6 5. Skill Development Plan\n\n6.1 5.1 Priority Skills for Team Development\nBased on our analysis, here are the priority skills the team should focus on:\n\n# Identify critical gaps (gap &gt; 2)\ncritical_gaps = comparison_df[comparison_df['Gap'] &gt; 2].sort_values('Gap', ascending=False)\n\nprint(\"CRITICAL SKILLS TO DEVELOP (Gap &gt; 2):\")\nprint(critical_gaps[['Skill', 'Team Average', 'Market Demand', 'Gap']])\n\n# Identify moderate gaps (1 &lt; gap &lt;= 2)\nmoderate_gaps = comparison_df[(comparison_df['Gap'] &gt; 1) & (comparison_df['Gap'] &lt;= 2)].sort_values('Gap', ascending=False)\n\nprint(\"\\n\\nMODERATE PRIORITY SKILLS (1 &lt; Gap &lt;= 2):\")\nprint(moderate_gaps[['Skill', 'Team Average', 'Market Demand', 'Gap']])\n\nCRITICAL SKILLS TO DEVELOP (Gap &gt; 2):\nEmpty DataFrame\nColumns: [Skill, Team Average, Market Demand, Gap]\nIndex: []\n\n\nMODERATE PRIORITY SKILLS (1 &lt; Gap &lt;= 2):\n             Skill  Team Average  Market Demand       Gap\n4  Cloud Computing      2.333333       4.221311  1.887978\n3    Data Analysis      3.666667       5.000000  1.333333\n\n\n\n\n6.2 5.2 Recommended Learning Resources\nBased on identified gaps, here are recommended learning resources:\n\n# Create learning recommendations\nlearning_resources = {\n    'Python': {\n        'Courses': ['Python for Data Science (Coursera)', 'Complete Python Bootcamp (Udemy)'],\n        'Practice': ['LeetCode Python problems', 'HackerRank Python track'],\n        'Time': '2-3 months for intermediate proficiency'\n    },\n    'SQL': {\n        'Courses': ['SQL for Data Science (Coursera)', 'The Complete SQL Bootcamp (Udemy)'],\n        'Practice': ['SQLZoo', 'LeetCode Database problems'],\n        'Time': '1-2 months'\n    },\n    'Machine Learning': {\n        'Courses': ['Machine Learning by Andrew Ng (Coursera)', 'Fast.ai Practical Deep Learning'],\n        'Practice': ['Kaggle competitions', 'Personal ML projects'],\n        'Time': '3-6 months'\n    },\n    'AWS': {\n        'Courses': ['AWS Certified Solutions Architect (A Cloud Guru)', 'AWS Free Tier hands-on'],\n        'Practice': ['Build personal projects on AWS', 'AWS Cloud Quest'],\n        'Time': '2-3 months'\n    },\n    'Docker': {\n        'Courses': ['Docker Mastery (Udemy)', 'Docker documentation'],\n        'Practice': ['Containerize personal projects', 'Docker Hub'],\n        'Time': '1-2 months'\n    },\n    'Cloud Computing': {\n        'Courses': ['Cloud Computing Concepts (Coursera)', 'Google Cloud Training'],\n        'Practice': ['Multi-cloud projects', 'Free tier experimentation'],\n        'Time': '2-4 months'\n    }\n}\n\n# Display learning plan for critical skills\nprint(\"RECOMMENDED LEARNING PLAN:\\n\")\nfor _, row in critical_gaps.head(5).iterrows():\n    skill = row['Skill']\n    if skill in learning_resources:\n        print(f\"üìö {skill} (Gap: {row['Gap']:.2f})\")\n        print(f\"   Courses: {', '.join(learning_resources[skill]['Courses'])}\")\n        print(f\"   Practice: {', '.join(learning_resources[skill]['Practice'])}\")\n        print(f\"   Estimated Time: {learning_resources[skill]['Time']}\")\n        print()\n\nRECOMMENDED LEARNING PLAN:\n\n\n\n\n\n6.3 5.3 Team Collaboration Strategy\nHow can team members help each other?\n\n# Identify team strengths (where team exceeds market demand)\nteam_strengths = comparison_df[comparison_df['Gap'] &lt; 0].sort_values('Team Average', ascending=False)\n\nprint(\"TEAM STRENGTHS (Can mentor others):\")\nprint(team_strengths[['Skill', 'Team Average', 'Market Demand']])\n\n# Create mentoring pairs based on individual strengths\nprint(\"\\n\\nSUGGESTED MENTORING OPPORTUNITIES:\")\n\nfor skill in df_team_skills.columns:\n    skill_levels = df_team_skills[skill].sort_values(ascending=False)\n    if skill_levels.max() - skill_levels.min() &gt;= 2:  # Significant difference\n        expert = skill_levels.index[0]\n        learner = skill_levels.index[-1]\n        print(f\"  {skill}: {expert} (Level {skill_levels.iloc[0]}) can mentor {learner} (Level {skill_levels.iloc[-1]})\")\n\nTEAM STRENGTHS (Can mentor others):\n               Skill  Team Average  Market Demand\n9              Excel      4.000000       0.081967\n0             Python      3.666667       0.054645\n1                SQL      3.333333       0.314208\n13               Git      3.333333       0.423497\n8            Tableau      3.000000       0.040984\n2   Machine Learning      3.000000       0.040984\n6         JavaScript      2.666667       0.000000\n17               NLP      2.666667       0.000000\n7                  R      2.666667       0.000000\n15             Spark      2.333333       0.000000\n14          Power BI      2.333333       0.000000\n5               Java      2.333333       0.109290\n16        TensorFlow      2.000000       0.942623\n12            Docker      2.000000       0.000000\n11             Azure      1.666667       0.245902\n10               AWS      1.666667       0.027322\n\n\nSUGGESTED MENTORING OPPORTUNITIES:\n  Machine Learning: Soham Deshkhaire (Level 4) can mentor Kriti Singh (Level 2)\n  Tableau: Kriti Singh (Level 4) can mentor Soham Deshkhaire (Level 2)\n  Excel: Kriti Singh (Level 5) can mentor Soham Deshkhaire (Level 3)\n  Docker: Soham Deshkhaire (Level 3) can mentor Kriti Singh (Level 1)\n  TensorFlow: Soham Deshkhaire (Level 3) can mentor Kriti Singh (Level 1)\n\n\n\n\n6.4 5.4 3-Month, 6-Month, and 1-Year Goals\nCreate timeline for skill development:\n\n\n\n\n\n\nTipSkill Development Timeline\n\n\n\n3-Month Goals (Immediate Priority) - Focus on critical gap skills with highest market demand - Complete foundational courses in AWS, Docker, and Cloud Computing - Build 1-2 hands-on projects demonstrating new skills - Team members mentor each other in strength areas\n6-Month Goals (Intermediate) - Achieve intermediate proficiency (Level 3) in all critical gap skills - Complete advanced courses in Machine Learning and Data Analysis - Participate in Kaggle competitions or contribute to open-source projects - Earn 1-2 professional certifications (e.g., AWS Certified Developer)\n1-Year Goals (Advanced) - Achieve advanced proficiency (Level 4) in priority skills - Entire team reaches minimum Level 3 in all high-demand market skills - Build comprehensive portfolio showcasing technical competencies - Competitive job candidates for targeted IT roles"
  },
  {
    "objectID": "SKILL_GAP_ANALYSIS.html#references",
    "href": "SKILL_GAP_ANALYSIS.html#references",
    "title": "Skill Gap Analysis",
    "section": "7 References",
    "text": "7 References\n\nJob market data: Lightcast Job Postings Dataset (2024)\nSkill assessment framework: Industry-standard proficiency scales\nLearning resources: Coursera, Udemy, AWS Training, Kaggle\nAnalysis tools: Python, pandas, hvPlot, Panel"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Job Market Analysis 2024: Business Analytics, Data Science, and Machine Learning Trends",
    "section": "",
    "text": "This research explores the evolving landscape of Business Analytics, Data Science and Machine Learning (ML) careers in 2024. As organizations increasingly integrate AI in every function from operations to strategic decision-making, the demand for professionals with strong data and analytics skills is growing rapidly. This study explores four interconnected questions: the most sought-after skills for analytics and ML roles, how job descriptions are changing to require AI literacy, which industries are leading data-driven hiring, and what the short- to medium-term career outlook looks like for analytics professionals. By combining insights from labor market data and recent academic research, this project provides a holistic view of how AI and automation are transforming workforce expectations and employability strategies for students and professionals."
  },
  {
    "objectID": "index.html#project-overview",
    "href": "index.html#project-overview",
    "title": "Job Market Analysis 2024: Business Analytics, Data Science, and Machine Learning Trends",
    "section": "",
    "text": "This research explores the evolving landscape of Business Analytics, Data Science and Machine Learning (ML) careers in 2024. As organizations increasingly integrate AI in every function from operations to strategic decision-making, the demand for professionals with strong data and analytics skills is growing rapidly. This study explores four interconnected questions: the most sought-after skills for analytics and ML roles, how job descriptions are changing to require AI literacy, which industries are leading data-driven hiring, and what the short- to medium-term career outlook looks like for analytics professionals. By combining insights from labor market data and recent academic research, this project provides a holistic view of how AI and automation are transforming workforce expectations and employability strategies for students and professionals."
  },
  {
    "objectID": "index.html#data-source",
    "href": "index.html#data-source",
    "title": "Job Market Analysis 2024: Business Analytics, Data Science, and Machine Learning Trends",
    "section": "Data Source",
    "text": "Data Source\nWe will utilize the Lightcast 2024 dataset, which includes: - Job posting volumes for analytics, data science, and ML roles by industry and location - Salary data across data-related positions and geographies - Skill requirements extracted from job descriptions - Hiring trends across time periods - Company size and industry classifications - Emerging role titles and job requirements"
  },
  {
    "objectID": "index.html#analysis-approach",
    "href": "index.html#analysis-approach",
    "title": "Job Market Analysis 2024: Business Analytics, Data Science, and Machine Learning Trends",
    "section": "Analysis Approach",
    "text": "Analysis Approach\nOur team will:\n\nClean and preprocess the Lightcast data using Python (pandas, NumPy)\nExtract and categorize skills mentioned in job descriptions for analytics and ML roles\nCompare trends across industries, geographies, and job levels\nAnalyze salary patterns to understand compensation for different skill combinations\nIdentify emerging roles and how job requirements are evolving\nVisualize findings through interactive dashboards using Plotly and Matplotlib\nDevelop career strategy recommendations based on market insights"
  },
  {
    "objectID": "index.html#expected-findings",
    "href": "index.html#expected-findings",
    "title": "Job Market Analysis 2024: Business Analytics, Data Science, and Machine Learning Trends",
    "section": "Expected Findings",
    "text": "Expected Findings\nWe anticipate discovering that: - Python, SQL, and cloud platforms (AWS, GCP, Azure) remain foundational skills - Generative AI and prompt engineering are emerging as newly valued competencies - Finance, healthcare, and technology sectors lead in analytics hiring - Soft skills like communication and domain expertise are increasingly emphasized - Analytics roles offer strong job security and career growth potential - Salary premiums exist for ML/AI-specialized roles compared to traditional business analytics"
  },
  {
    "objectID": "index.html#deliverables-future-phases",
    "href": "index.html#deliverables-future-phases",
    "title": "Job Market Analysis 2024: Business Analytics, Data Science, and Machine Learning Trends",
    "section": "Deliverables (Future Phases)",
    "text": "Deliverables (Future Phases)\n\nExploratory Data Analysis (EDA) with visualizations\nInteractive dashboards showing skill trends, industry hiring patterns, and salary insights\nCareer pathway recommendations for different specializations\nPersonal career action plans for each team member"
  }
]