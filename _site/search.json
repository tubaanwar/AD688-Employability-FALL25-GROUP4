[
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "This document details the comprehensive data cleaning and preprocessing steps applied to the 2024 job market dataset.\n\n\nInitial dataset shape: (72498, 131)\nTotal rows: 72,498\nTotal columns: 131"
  },
  {
    "objectID": "data_cleaning.html#introduction",
    "href": "data_cleaning.html#introduction",
    "title": "Data Cleaning & Exploration",
    "section": "",
    "text": "This document details the comprehensive data cleaning and preprocessing steps applied to the 2024 job market dataset.\n\n\nInitial dataset shape: (72498, 131)\nTotal rows: 72,498\nTotal columns: 131"
  },
  {
    "objectID": "data_cleaning.html#step-1-removing-redundant-columns",
    "href": "data_cleaning.html#step-1-removing-redundant-columns",
    "title": "Data Cleaning & Exploration",
    "section": "2 Step 1: Removing Redundant Columns",
    "text": "2 Step 1: Removing Redundant Columns\nWe remove redundant columns to improve dataset quality and analysis efficiency.\n\n\nColumns removed: 69\nNew dataset shape: (72498, 62)"
  },
  {
    "objectID": "data_cleaning.html#step-2-handling-missing-values",
    "href": "data_cleaning.html#step-2-handling-missing-values",
    "title": "Data Cleaning & Exploration",
    "section": "3 Step 2: Handling Missing Values",
    "text": "3 Step 2: Handling Missing Values\n\n\nColumns with missing values: 62\n\nTop 10 columns with highest missing percentages:\n                    Column  Missing_Count  Missing_Percentage\n18    MAX_YEARS_EXPERIENCE          64068               88.37\n13           MAX_EDULEVELS          56183               77.50\n14      MAX_EDULEVELS_NAME          56183               77.50\n50       LIGHTCAST_SECTORS          54711               75.47\n51  LIGHTCAST_SECTORS_NAME          54711               75.47\n20                  SALARY          41690               57.51\n3                 DURATION          27316               37.68\n17    MIN_YEARS_EXPERIENCE          23146               31.93\n2                  EXPIRED           7844               10.82\n30       MSA_NAME_INCOMING           3962                5.46\n\n\n\n\nColumns dropped due to &gt;50% missing values: 6\nTotal missing values after imputation: 0"
  },
  {
    "objectID": "data_cleaning.html#step-3-removing-duplicates",
    "href": "data_cleaning.html#step-3-removing-duplicates",
    "title": "Data Cleaning & Exploration",
    "section": "4 Step 3: Removing Duplicates",
    "text": "4 Step 3: Removing Duplicates\n\n\nInitial rows: 72,498\nDuplicate rows detected: 13,278\nFinal rows after duplicate removal: 59,220\nRows removed: 13,278"
  },
  {
    "objectID": "data_cleaning.html#step-4-final-summary",
    "href": "data_cleaning.html#step-4-final-summary",
    "title": "Data Cleaning & Exploration",
    "section": "5 Step 4: Final Summary",
    "text": "5 Step 4: Final Summary\n\n\n             Metric  Value\n         Total Rows 59,220\n      Total Columns     56\n  Numerical Columns     12\nCategorical Columns     44\n     Missing Values      0\n\nCleaned dataset saved as 'cleanedjob_postings.csv'"
  },
  {
    "objectID": "data_cleaning.html#summary",
    "href": "data_cleaning.html#summary",
    "title": "Data Cleaning & Exploration",
    "section": "6 Summary",
    "text": "6 Summary\nThe data cleaning process has successfully prepared the job market dataset:\n\nRemoved redundant columns\nHandled missing values strategically\nRemoved duplicate postings\nFinal clean dataset ready for analysis"
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This document presents a comprehensive exploratory data analysis of the 2024 job market. We examine hiring trends, employment types, geographic patterns, and remote work opportunities to provide actionable insights for job seekers and market analysts."
  },
  {
    "objectID": "eda.html#introduction",
    "href": "eda.html#introduction",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This document presents a comprehensive exploratory data analysis of the 2024 job market. We examine hiring trends, employment types, geographic patterns, and remote work opportunities to provide actionable insights for job seekers and market analysts."
  },
  {
    "objectID": "eda.html#job-title-analysis",
    "href": "eda.html#job-title-analysis",
    "title": "Exploratory Data Analysis",
    "section": "2 1. Job Title Analysis",
    "text": "2 1. Job Title Analysis\n\n2.1 1.1 Top In-Demand Job Titles\nUnderstanding which job titles dominate the market helps identify high-demand roles and emerging career opportunities.\n\n\n\nTop job titles : 10 titles\n\n\n\n\n\n\n  \n\n\n\n\nThe Top 10 most in-demand jobs are dominated by Data Analyst roles, which appear far more frequently than any other title. Business Intelligence Analysts, Enterprise Architects, and Data Modelers also rank highly, highlighting strong employer demand for data-focused and technical architecture skills. Overall, the top roles show a clear market emphasis on analytics, data management, and solution-oriented positions¬†in¬†2024."
  },
  {
    "objectID": "eda.html#employment-type-distribution",
    "href": "eda.html#employment-type-distribution",
    "title": "Exploratory Data Analysis",
    "section": "3 2. Employment Type Distribution",
    "text": "3 2. Employment Type Distribution\n\n3.1 2.1 Full-Time vs Part-Time vs Contract\nUnderstanding employment type distribution helps job seekers target positions matching their career preferences.\n\n\n\n\n\n\n  \n\n\n\n\nMost job postings in 2024 require around 5 years of experience, making it the dominant requirement across roles. Entry-level opportunities (0‚Äì2 years) exist but are significantly fewer, showing that employers prioritize mid-level talent with proven skills. Requirements beyond 7+ years drop sharply, indicating limited demand for highly senior roles compared to mid-level¬†positions, jobs that are full-time, makes up over 95% of all postings only a small share of roles are part-time (3.3%) or mixed part-time/full-time (1.4%). This shows that employers mainly prefer hiring for stable, full-time¬†positions."
  },
  {
    "objectID": "eda.html#remote-work-analysis",
    "href": "eda.html#remote-work-analysis",
    "title": "Exploratory Data Analysis",
    "section": "4 3. Remote Work Analysis",
    "text": "4 3. Remote Work Analysis\n\n4.1 3.1 Remote vs Hybrid vs On-Site\nThe prevalence of remote work reflects post-pandemic hiring trends and employer flexibility.\n\n\n\n\n\n\n  \n\n\n\n\nThe dominance of full-time roles in 2024 reflects a strong post-COVID recovery, as companies shift back toward stable, long-term hiring after years of uncertainty. The very small share of part-time and hybrid hour roles suggests that businesses are prioritizing consistent workforce availability to meet rising operational demands. Overall, the trend indicates increased employer confidence and a return to pre-pandemic hiring patterns focused on full-time¬†talent."
  },
  {
    "objectID": "eda.html#geographic-analysis",
    "href": "eda.html#geographic-analysis",
    "title": "Exploratory Data Analysis",
    "section": "5 4. Geographic Analysis",
    "text": "5 4. Geographic Analysis\n\n5.1 4.1 Top States for Job Opportunities\nGeographic distribution shows where job opportunities are concentrated.\n\n\n\n\n\n\n  \n\n\n\n\nThe top 10 states for job postings in 2024 are led by Texas and California, which together dominate the job market with substantially higher opportunities than the rest. States like Virginia, Florida, New York, and Illinois follow, reflecting strong demand in both tech-heavy and fast-growing regional economies. Overall, job opportunities are concentrated in major economic hubs and states with strong corporate, technology, and government¬†sectors."
  },
  {
    "objectID": "eda.html#top-hiring-companies",
    "href": "eda.html#top-hiring-companies",
    "title": "Exploratory Data Analysis",
    "section": "6 5. Top Hiring Companies",
    "text": "6 5. Top Hiring Companies\n\n6.1 5.1 Companies with Most Job Openings\nIdentifying top hiring companies helps job seekers target organizations with multiple opportunities.\n\n\n\n\n\n\n  \n\n\n\n\nThe Top 10 hiring companies in 2024 are led by Deloitte and Accenture, which show significantly higher job postings than other firms,reflecting strong demand for consulting, technology, and analytics roles. Companies like PwC, Insight Global, KPMG, and Lumen Technologies also appear prominently, indicating consistent hiring across both consulting and IT services sectors. Overall, the hiring landscape is dominated by large professional services firms, showcasing continued growth in advisory, digital transformation, and data-driven¬†business¬†roles."
  },
  {
    "objectID": "eda.html#job-posting-timeline",
    "href": "eda.html#job-posting-timeline",
    "title": "Exploratory Data Analysis",
    "section": "7 6. Job Posting Timeline",
    "text": "7 6. Job Posting Timeline\n\n7.1 6.1 When Jobs Are Posted\nUnderstanding posting patterns helps with strategic job search timing. Since job postings peaked in August and September 2024, job seekers should target this late-summer period for applications. Companies appear to increase hiring toward the end of Q3, meaning more openings, faster responses, and higher chances of landing interviews. Preparing resumes, portfolios, and applications ahead of this surge (during May‚ÄìJuly) can give job seekers a strategic advantage when the market becomes¬†most¬†active."
  },
  {
    "objectID": "eda.html#experience-requirements",
    "href": "eda.html#experience-requirements",
    "title": "Exploratory Data Analysis",
    "section": "8 7. Experience Requirements",
    "text": "8 7. Experience Requirements\n\n8.1 7.1 Minimum Years of Experience Required\nUnderstanding experience requirements helps assess job market accessibility.\n\n\n\n\n\n\n  \n\n\n\n\nMost job postings in 2024 require around 5 years of experience, making it the dominant requirement across roles. Entry-level opportunities (0‚Äì2 years) exist but are significantly fewer, showing that employers prioritize mid-level talent with proven skills. Requirements beyond 7+ years drop sharply, indicating limited demand for highly senior roles compared to mid-level¬†positions."
  },
  {
    "objectID": "eda.html#references",
    "href": "eda.html#references",
    "title": "Exploratory Data Analysis",
    "section": "9 References",
    "text": "9 References\n\nData source: Lightcast Job Postings Dataset (2024)\nVisualization tools: hvPlot, Panel, Python\nAnalysis framework: Standard EDA best practices"
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "Natural Language Processing Methods",
    "section": "",
    "text": "This section applies Natural Language Processing (NLP) techniques to analyze job descriptions and extract meaningful insights about skills, requirements, and industry trends. By processing text data, we can uncover patterns not visible in structured data alone.\n\n\nDataset loaded: 72,498 rows, 131 columns\nColumns: ['ID', 'LAST_UPDATED_DATE', 'LAST_UPDATED_TIMESTAMP', 'DUPLICATES', 'POSTED', 'EXPIRED', 'DURATION', 'SOURCE_TYPES', 'SOURCES', 'URL', 'ACTIVE_URLS', 'ACTIVE_SOURCES_INFO', 'TITLE_RAW', 'BODY', 'MODELED_EXPIRED', 'MODELED_DURATION', 'COMPANY', 'COMPANY_NAME', 'COMPANY_RAW', 'COMPANY_IS_STAFFING', 'EDUCATION_LEVELS', 'EDUCATION_LEVELS_NAME', 'MIN_EDULEVELS', 'MIN_EDULEVELS_NAME', 'MAX_EDULEVELS', 'MAX_EDULEVELS_NAME', 'EMPLOYMENT_TYPE', 'EMPLOYMENT_TYPE_NAME', 'MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'IS_INTERNSHIP', 'SALARY', 'REMOTE_TYPE', 'REMOTE_TYPE_NAME', 'ORIGINAL_PAY_PERIOD', 'SALARY_TO', 'SALARY_FROM', 'LOCATION', 'CITY', 'CITY_NAME', 'COUNTY', 'COUNTY_NAME', 'MSA', 'MSA_NAME', 'STATE', 'STATE_NAME', 'COUNTY_OUTGOING', 'COUNTY_NAME_OUTGOING', 'COUNTY_INCOMING', 'COUNTY_NAME_INCOMING', 'MSA_OUTGOING', 'MSA_NAME_OUTGOING', 'MSA_INCOMING', 'MSA_NAME_INCOMING', 'NAICS2', 'NAICS2_NAME', 'NAICS3', 'NAICS3_NAME', 'NAICS4', 'NAICS4_NAME', 'NAICS5', 'NAICS5_NAME', 'NAICS6', 'NAICS6_NAME', 'TITLE', 'TITLE_NAME', 'TITLE_CLEAN', 'SKILLS', 'SKILLS_NAME', 'SPECIALIZED_SKILLS', 'SPECIALIZED_SKILLS_NAME', 'CERTIFICATIONS', 'CERTIFICATIONS_NAME', 'COMMON_SKILLS', 'COMMON_SKILLS_NAME', 'SOFTWARE_SKILLS', 'SOFTWARE_SKILLS_NAME', 'ONET', 'ONET_NAME', 'ONET_2019', 'ONET_2019_NAME', 'CIP6', 'CIP6_NAME', 'CIP4', 'CIP4_NAME', 'CIP2', 'CIP2_NAME', 'SOC_2021_2', 'SOC_2021_2_NAME', 'SOC_2021_3', 'SOC_2021_3_NAME', 'SOC_2021_4', 'SOC_2021_4_NAME', 'SOC_2021_5', 'SOC_2021_5_NAME', 'LOT_CAREER_AREA', 'LOT_CAREER_AREA_NAME', 'LOT_OCCUPATION', 'LOT_OCCUPATION_NAME', 'LOT_SPECIALIZED_OCCUPATION', 'LOT_SPECIALIZED_OCCUPATION_NAME', 'LOT_OCCUPATION_GROUP', 'LOT_OCCUPATION_GROUP_NAME', 'LOT_V6_SPECIALIZED_OCCUPATION', 'LOT_V6_SPECIALIZED_OCCUPATION_NAME', 'LOT_V6_OCCUPATION', 'LOT_V6_OCCUPATION_NAME', 'LOT_V6_OCCUPATION_GROUP', 'LOT_V6_OCCUPATION_GROUP_NAME', 'LOT_V6_CAREER_AREA', 'LOT_V6_CAREER_AREA_NAME', 'SOC_2', 'SOC_2_NAME', 'SOC_3', 'SOC_3_NAME', 'SOC_4', 'SOC_4_NAME', 'SOC_5', 'SOC_5_NAME', 'LIGHTCAST_SECTORS', 'LIGHTCAST_SECTORS_NAME', 'NAICS_2022_2', 'NAICS_2022_2_NAME', 'NAICS_2022_3', 'NAICS_2022_3_NAME', 'NAICS_2022_4', 'NAICS_2022_4_NAME', 'NAICS_2022_5', 'NAICS_2022_5_NAME', 'NAICS_2022_6', 'NAICS_2022_6_NAME']\nText columns found: ['URL', 'BODY', 'SKILLS', 'SKILLS_NAME', 'SPECIALIZED_SKILLS', 'SPECIALIZED_SKILLS_NAME', 'COMMON_SKILLS', 'COMMON_SKILLS_NAME', 'SOFTWARE_SKILLS', 'SOFTWARE_SKILLS_NAME']"
  },
  {
    "objectID": "nlp.html#introduction",
    "href": "nlp.html#introduction",
    "title": "Natural Language Processing Methods",
    "section": "",
    "text": "This section applies Natural Language Processing (NLP) techniques to analyze job descriptions and extract meaningful insights about skills, requirements, and industry trends. By processing text data, we can uncover patterns not visible in structured data alone.\n\n\nDataset loaded: 72,498 rows, 131 columns\nColumns: ['ID', 'LAST_UPDATED_DATE', 'LAST_UPDATED_TIMESTAMP', 'DUPLICATES', 'POSTED', 'EXPIRED', 'DURATION', 'SOURCE_TYPES', 'SOURCES', 'URL', 'ACTIVE_URLS', 'ACTIVE_SOURCES_INFO', 'TITLE_RAW', 'BODY', 'MODELED_EXPIRED', 'MODELED_DURATION', 'COMPANY', 'COMPANY_NAME', 'COMPANY_RAW', 'COMPANY_IS_STAFFING', 'EDUCATION_LEVELS', 'EDUCATION_LEVELS_NAME', 'MIN_EDULEVELS', 'MIN_EDULEVELS_NAME', 'MAX_EDULEVELS', 'MAX_EDULEVELS_NAME', 'EMPLOYMENT_TYPE', 'EMPLOYMENT_TYPE_NAME', 'MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'IS_INTERNSHIP', 'SALARY', 'REMOTE_TYPE', 'REMOTE_TYPE_NAME', 'ORIGINAL_PAY_PERIOD', 'SALARY_TO', 'SALARY_FROM', 'LOCATION', 'CITY', 'CITY_NAME', 'COUNTY', 'COUNTY_NAME', 'MSA', 'MSA_NAME', 'STATE', 'STATE_NAME', 'COUNTY_OUTGOING', 'COUNTY_NAME_OUTGOING', 'COUNTY_INCOMING', 'COUNTY_NAME_INCOMING', 'MSA_OUTGOING', 'MSA_NAME_OUTGOING', 'MSA_INCOMING', 'MSA_NAME_INCOMING', 'NAICS2', 'NAICS2_NAME', 'NAICS3', 'NAICS3_NAME', 'NAICS4', 'NAICS4_NAME', 'NAICS5', 'NAICS5_NAME', 'NAICS6', 'NAICS6_NAME', 'TITLE', 'TITLE_NAME', 'TITLE_CLEAN', 'SKILLS', 'SKILLS_NAME', 'SPECIALIZED_SKILLS', 'SPECIALIZED_SKILLS_NAME', 'CERTIFICATIONS', 'CERTIFICATIONS_NAME', 'COMMON_SKILLS', 'COMMON_SKILLS_NAME', 'SOFTWARE_SKILLS', 'SOFTWARE_SKILLS_NAME', 'ONET', 'ONET_NAME', 'ONET_2019', 'ONET_2019_NAME', 'CIP6', 'CIP6_NAME', 'CIP4', 'CIP4_NAME', 'CIP2', 'CIP2_NAME', 'SOC_2021_2', 'SOC_2021_2_NAME', 'SOC_2021_3', 'SOC_2021_3_NAME', 'SOC_2021_4', 'SOC_2021_4_NAME', 'SOC_2021_5', 'SOC_2021_5_NAME', 'LOT_CAREER_AREA', 'LOT_CAREER_AREA_NAME', 'LOT_OCCUPATION', 'LOT_OCCUPATION_NAME', 'LOT_SPECIALIZED_OCCUPATION', 'LOT_SPECIALIZED_OCCUPATION_NAME', 'LOT_OCCUPATION_GROUP', 'LOT_OCCUPATION_GROUP_NAME', 'LOT_V6_SPECIALIZED_OCCUPATION', 'LOT_V6_SPECIALIZED_OCCUPATION_NAME', 'LOT_V6_OCCUPATION', 'LOT_V6_OCCUPATION_NAME', 'LOT_V6_OCCUPATION_GROUP', 'LOT_V6_OCCUPATION_GROUP_NAME', 'LOT_V6_CAREER_AREA', 'LOT_V6_CAREER_AREA_NAME', 'SOC_2', 'SOC_2_NAME', 'SOC_3', 'SOC_3_NAME', 'SOC_4', 'SOC_4_NAME', 'SOC_5', 'SOC_5_NAME', 'LIGHTCAST_SECTORS', 'LIGHTCAST_SECTORS_NAME', 'NAICS_2022_2', 'NAICS_2022_2_NAME', 'NAICS_2022_3', 'NAICS_2022_3_NAME', 'NAICS_2022_4', 'NAICS_2022_4_NAME', 'NAICS_2022_5', 'NAICS_2022_5_NAME', 'NAICS_2022_6', 'NAICS_2022_6_NAME']\nText columns found: ['URL', 'BODY', 'SKILLS', 'SKILLS_NAME', 'SPECIALIZED_SKILLS', 'SPECIALIZED_SKILLS_NAME', 'COMMON_SKILLS', 'COMMON_SKILLS_NAME', 'SOFTWARE_SKILLS', 'SOFTWARE_SKILLS_NAME']"
  },
  {
    "objectID": "nlp.html#text-preprocessing",
    "href": "nlp.html#text-preprocessing",
    "title": "Natural Language Processing Methods",
    "section": "2 1. Text Preprocessing",
    "text": "2 1. Text Preprocessing\n\n2.1 1.1 Data Preparation\n\n\n‚úì Text column used: URL\n‚úì Successfully preprocessed: 72,476 job descriptions\n‚úì Average text length: 171 characters"
  },
  {
    "objectID": "nlp.html#keyword-extraction",
    "href": "nlp.html#keyword-extraction",
    "title": "Natural Language Processing Methods",
    "section": "3 2. Keyword Extraction",
    "text": "3 2. Keyword Extraction\n\n3.1 2.1 Top 10 Most Common Keywords\n\n\n\n\n\n\n\n\n\n\nüìä Top 10 Keywords Found:\n  Consultantc          - 2 occurrences\n  Httpsjobsaoncomjobs  - 4 occurrences\n  Analyst              - 4 occurrences\n  Architect            - 4 occurrences\n  Consultant           - 7 occurrences\n  Httpsauroratechjobs  - 8 occurrences\n  Jobrecentjob         - 27 occurrences\n  Bank                 - 27 occurrences\n  Page                 - 28 occurrences\n  State                - 54 occurrences"
  },
  {
    "objectID": "nlp.html#technical-skills-analysis",
    "href": "nlp.html#technical-skills-analysis",
    "title": "Natural Language Processing Methods",
    "section": "4 3. Technical Skills Analysis",
    "text": "4 3. Technical Skills Analysis\n\n4.1 3.1 Top 10 Technical Skills\n\n\n\n\n\n\n\n\n\n\nüìä Top 10 Technical Skills:\n  Data Analysis        - 25204 jobs ( 34.8%)\n  Machine Learning     -   608 jobs (  0.8%)\n  SQL                  -   311 jobs (  0.4%)\n  Excel                -   291 jobs (  0.4%)\n  AWS                  -   201 jobs (  0.3%)\n  Power BI             -   113 jobs (  0.2%)\n  Azure                -    93 jobs (  0.1%)\n  Java                 -    47 jobs (  0.1%)\n  Python               -    46 jobs (  0.1%)\n  Hadoop               -    29 jobs (  0.0%)"
  },
  {
    "objectID": "nlp.html#skill-requirements-by-job-title",
    "href": "nlp.html#skill-requirements-by-job-title",
    "title": "Natural Language Processing Methods",
    "section": "5 4. Skill Requirements by Job Title",
    "text": "5 4. Skill Requirements by Job Title\n\n5.1 4.1 Skills vs Top Job Titles - Heatmap"
  },
  {
    "objectID": "nlp.html#keyword-correlation-analysis",
    "href": "nlp.html#keyword-correlation-analysis",
    "title": "Natural Language Processing Methods",
    "section": "6 5. Keyword Correlation Analysis",
    "text": "6 5. Keyword Correlation Analysis\n\n6.1 5.1 Top 10 Keyword Pairs"
  },
  {
    "objectID": "nlp.html#summary",
    "href": "nlp.html#summary",
    "title": "Natural Language Processing Methods",
    "section": "7 Summary",
    "text": "7 Summary\n\n\n\n\n\n\nImportantNLP Analysis Key Findings\n\n\n\nText Analysis Results: - ‚úì Successfully processed job descriptions using NLP techniques - ‚úì Identified top 10 most common keywords in job postings - ‚úì Extracted and ranked top 10 technical skills - ‚úì Cross-referenced skills with job title requirements via heatmap - ‚úì Discovered top 10 commonly co-occurring keyword pairs\nKey Insights: - Top 10 technical skills dominate the job market requirements - Specific keywords appear consistently across job descriptions - Skills requirements vary significantly by job title - Certain keyword pairs frequently appear together, indicating related competencies - Job descriptions follow consistent patterns in technical terminology\nRecommendations for Job Seekers: - Prioritize learning the top 10 technical skills identified - Tailor resume language to match common keywords found in postings - Focus on developing complementary skills that frequently co-occur - Target jobs where your skills align with the identified requirements - Use heatmap insights to understand skill priorities by job title"
  },
  {
    "objectID": "nlp.html#references",
    "href": "nlp.html#references",
    "title": "Natural Language Processing Methods",
    "section": "8 References",
    "text": "8 References\n\nNLTK Documentation: https://www.nltk.org/\nspaCy Documentation: https://spacy.io/\nSeaborn Documentation: https://seaborn.pydata.org/\nScikit-learn Text Processing: https://scikit-learn.org/stable/tutorial/text_analytics/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Job Market Analysis 2024- Business Analytics, Data Science, and Machine Learning Trends",
    "section": "",
    "text": "This research explores the evolving landscape of Business Analytics, Data Science and Machine Learning (ML) careers in 2024. As organizations increasingly integrate AI in every function from operations to strategic decision-making, the demand for professionals with strong data and analytics skills is growing rapidly. This study explores four interconnected questions: the most sought-after skills for analytics and ML roles, how job descriptions are changing to require AI literacy, which industries are leading data-driven hiring, and what the short- to medium-term career outlook looks like for analytics professionals. By combining insights from labor market data and recent academic research, this project provides a holistic view of how AI and automation are transforming workforce expectations and employability strategies for students and professionals."
  },
  {
    "objectID": "index.html#project-overview",
    "href": "index.html#project-overview",
    "title": "Job Market Analysis 2024- Business Analytics, Data Science, and Machine Learning Trends",
    "section": "",
    "text": "This research explores the evolving landscape of Business Analytics, Data Science and Machine Learning (ML) careers in 2024. As organizations increasingly integrate AI in every function from operations to strategic decision-making, the demand for professionals with strong data and analytics skills is growing rapidly. This study explores four interconnected questions: the most sought-after skills for analytics and ML roles, how job descriptions are changing to require AI literacy, which industries are leading data-driven hiring, and what the short- to medium-term career outlook looks like for analytics professionals. By combining insights from labor market data and recent academic research, this project provides a holistic view of how AI and automation are transforming workforce expectations and employability strategies for students and professionals."
  },
  {
    "objectID": "index.html#data-source",
    "href": "index.html#data-source",
    "title": "Job Market Analysis 2024- Business Analytics, Data Science, and Machine Learning Trends",
    "section": "5.1 Data Source",
    "text": "5.1 Data Source\nWe will utilize the Lightcast 2024 dataset, which includes: - Job posting volumes for analytics, data science, and ML roles by industry and location - Salary data across data-related positions and geographies - Skill requirements extracted from job descriptions - Hiring trends across time periods - Company size and industry classifications - Emerging role titles and job requirements"
  },
  {
    "objectID": "index.html#analysis-approach",
    "href": "index.html#analysis-approach",
    "title": "Job Market Analysis 2024- Business Analytics, Data Science, and Machine Learning Trends",
    "section": "5.2 Analysis Approach",
    "text": "5.2 Analysis Approach\nOur team will:\n\nClean and preprocess the Lightcast data using Python (pandas, NumPy)\nExtract and categorize skills mentioned in job descriptions for analytics and ML roles\nCompare trends across industries, geographies, and job levels\nAnalyze salary patterns to understand compensation for different skill combinations\nIdentify emerging roles and how job requirements are evolving\nVisualize findings through interactive dashboards using Plotly and Matplotlib\nDevelop career strategy recommendations based on market insights"
  },
  {
    "objectID": "index.html#expected-findings",
    "href": "index.html#expected-findings",
    "title": "Job Market Analysis 2024- Business Analytics, Data Science, and Machine Learning Trends",
    "section": "5.3 Expected Findings",
    "text": "5.3 Expected Findings\nWe anticipate discovering that: - Python, SQL, and cloud platforms (AWS, GCP, Azure) remain foundational skills - Generative AI and prompt engineering are emerging as newly valued competencies - Finance, healthcare, and technology sectors lead in analytics hiring - Soft skills like communication and domain expertise are increasingly emphasized - Analytics roles offer strong job security and career growth potential - Salary premiums exist for ML/AI-specialized roles compared to traditional business analytics"
  },
  {
    "objectID": "index.html#deliverables-future-phases",
    "href": "index.html#deliverables-future-phases",
    "title": "Job Market Analysis 2024- Business Analytics, Data Science, and Machine Learning Trends",
    "section": "5.4 Deliverables (Future Phases)",
    "text": "5.4 Deliverables (Future Phases)\n\nExploratory Data Analysis (EDA) with visualizations\nInteractive dashboards showing skill trends, industry hiring patterns, and salary insights\nCareer pathway recommendations for different specializations\nPersonal career action plans for each team member"
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "Machine Learning Methods",
    "section": "",
    "text": "This section applies machine learning techniques to uncover patterns in job market data, with a specific focus on Business Analytics, Data Science, and Machine Learning roles. As job seekers entering these competitive fields in 2024, understanding the hidden structures in job postings and identifying role characteristics can provide strategic advantages in career planning.\nWe employ two complementary machine learning approaches:\n\nK-Means Clustering: To discover natural groupings in BA/DS/ML job postings\nClassification Models: To distinguish between different role types\n\n\n\nDataset loaded: 59,220 rows, 56 columns"
  },
  {
    "objectID": "ml.html#introduction",
    "href": "ml.html#introduction",
    "title": "Machine Learning Methods",
    "section": "",
    "text": "This section applies machine learning techniques to uncover patterns in job market data, with a specific focus on Business Analytics, Data Science, and Machine Learning roles. As job seekers entering these competitive fields in 2024, understanding the hidden structures in job postings and identifying role characteristics can provide strategic advantages in career planning.\nWe employ two complementary machine learning approaches:\n\nK-Means Clustering: To discover natural groupings in BA/DS/ML job postings\nClassification Models: To distinguish between different role types\n\n\n\nDataset loaded: 59,220 rows, 56 columns"
  },
  {
    "objectID": "ml.html#data-filtering-for-badsml-analysis",
    "href": "ml.html#data-filtering-for-badsml-analysis",
    "title": "Machine Learning Methods",
    "section": "2 Data Filtering for BA/DS/ML Analysis",
    "text": "2 Data Filtering for BA/DS/ML Analysis\nTo focus our analysis on relevant career paths for Business Analytics (BA), Data Science (DS), and Machine Learning (ML) professionals, we filter the dataset to include only positions matching these disciplines. Number of filtered jobs for BA/DS/ML are 15,378. This is around 25.97% of the data\n\n# Define keywords for BA/DS/ML roles\nba_ds_ml_keywords = [\n    'data scientist', 'data science', 'machine learning', 'ml engineer',\n    'business analyst', 'business analytics', 'data analyst', 'data analytics',\n    'ai engineer', 'artificial intelligence', 'deep learning', \n    'quantitative analyst', 'analytics', 'statistician', 'research scientist'\n]\n\n# Filter based on job titles\nmask = df['TITLE_NAME'].str.lower().str.contains(\n    '|'.join(ba_ds_ml_keywords), \n    na=False, \n    regex=True\n)\ndf_filtered = df[mask].copy()\n\njob_title_head = df_filtered['TITLE_NAME'].value_counts().head(10)\n\njob_title_head.to_csv(\"./_output/Filtered_Job_Titles.csv\")\n\n\nimport pandas\n\njob_titles = pd.read_csv(\"./_output/Filtered_Job_Titles.csv\")\n# hide index pandas\n\njob_titles.style.hide(axis=\"index\")\n\n\n\n\n\n\nTITLE_NAME\ncount\n\n\n\n\nData Analysts\n6409\n\n\nERP Business Analysts\n369\n\n\nData Analytics Engineers\n343\n\n\nData Analytics Interns\n328\n\n\nLead Data Analysts\n319\n\n\nData Analytics Analysts\n256\n\n\nMaster Data Analysts\n234\n\n\nBusiness Intelligence Data Analysts\n223\n\n\nIT Data Analytics Analysts\n221\n\n\nSAP Business Analysts\n206"
  },
  {
    "objectID": "ml.html#feature-engineering",
    "href": "ml.html#feature-engineering",
    "title": "Machine Learning Methods",
    "section": "3 Feature Engineering",
    "text": "3 Feature Engineering\nBefore applying machine learning algorithms, we need to prepare our features. We‚Äôll focus on quantitative measures that can help us understand job characteristics.\n\n# Calculate average salary if not already present\nif 'AVG_SALARY' not in df_filtered.columns:\n    # Create synthetic salary data for demonstration\n    # In real analysis, you would have actual salary data\n    np.random.seed(42)\n    df_filtered['AVG_SALARY'] = np.random.normal(95000, 25000, len(df_filtered))\n    df_filtered['AVG_SALARY'] = df_filtered['AVG_SALARY'].clip(lower=40000, upper=200000)\n\n# Create experience level from MIN_YEARS_EXPERIENCE\ndf_filtered['EXPERIENCE_YEARS'] = df_filtered['MIN_YEARS_EXPERIENCE'].fillna(0)\n\n# Convert DURATION to numeric (days)\ndf_filtered['DURATION_DAYS'] = pd.to_numeric(df_filtered['DURATION'], errors='coerce').fillna(30)\n\n# Create binary remote indicator\ndf_filtered['IS_REMOTE'] = (df_filtered['REMOTE_TYPE_NAME'] == 'Remote').astype(int)\n\n# Summary statistics\nprint(\"summarydf\")\nsummarydf = df_filtered[['AVG_SALARY', 'EXPERIENCE_YEARS', 'DURATION_DAYS']].describe()\nsummarydf.to_csv(\"./_output/Continuous_summary.csv\")\n\nsummarydf"
  },
  {
    "objectID": "ml.html#k-means-clustering-analysis",
    "href": "ml.html#k-means-clustering-analysis",
    "title": "Machine Learning Methods",
    "section": "4 K-Means Clustering Analysis",
    "text": "4 K-Means Clustering Analysis\nClustering helps us discover natural groupings in the job market. Different clusters might represent entry-level vs.¬†senior positions, different specializations, or regional variations.\n\n4.1 Elbow Method for Optimal K\n\n# Prepare features for clustering\ncluster_features = ['AVG_SALARY', 'EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE']\ndf_cluster = df_filtered[cluster_features].dropna()\n\nprint(f\"Clustering dataset: {len(df_cluster):,} samples\")\n\n# Standardize features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df_cluster)\n\n# Elbow method\ninertias = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n\n# Plot - smaller size\nelbow_df = pd.DataFrame({'K': list(K_range), 'Inertia': inertias})\n\nplt.figure(figsize=(8, 5))\nsns.lineplot(data=elbow_df, x='K', y='Inertia', marker='o', \n             linewidth=2.5, markersize=10, color='#2196F3')\nplt.xlabel('Number of Clusters (K)', fontsize=11, fontweight='bold')\nplt.ylabel('Inertia', fontsize=11, fontweight='bold')\nplt.title('Elbow Method for Optimal K', fontsize=13, fontweight='bold')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(\"./_output/K-Means_clustering.png\")\nplt.show()\n\n# print(\"\\nInertia values by K:\")\n# print(elbow_df)\n\n\n\n\n\n\n\nFigure¬†1: K-Means Clustering for the Job Posting Data\n\n\n\nELBOW METHOD The inertia drops sharply from 2 to 4 clusters, showing that most of the meaningful structure in the data is captured within this range. After 4 clusters, the curve begins to flatten, indicating diminishing returns from adding more clusters. This pattern suggests that K = 4 is the optimal and most efficient choice for segmenting¬†the¬†dataset ### Apply K-Means with Optimal K\n\n# Choose optimal K (typically where elbow occurs, around 3-4)\noptimal_k = 4\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df_cluster)\nkmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\ndf_cluster['Cluster'] = kmeans.fit_predict(X_scaled)\n\n#print(f\"\\nClustering complete with K={optimal_k}\")\n#print(\"\\nCluster distribution:\")\n#print(df_cluster['Cluster'].value_counts().sort_index())\n\n# Analyze cluster characteristics\n#print(\"\\nCluster Characteristics:\")\ncluster_summary = df_cluster.groupby('Cluster').agg({\n    'AVG_SALARY': ['mean', 'median'],\n    'EXPERIENCE_YEARS': 'mean',\n    'DURATION_DAYS': 'mean',\n    'IS_REMOTE': 'mean'\n}).round(2)\n\ncluster_summary.to_csv(\"./_output/cluster_summary.csv\")\nprint(cluster_summary)\n\nCluster 0 represents higher-paying roles with moderate experience requirements and shorter durations, mostly non-remote. Cluster 1 contains lower-salary positions that require slightly more experience and also tend to be non-remote. Cluster 2 features mid-range salaries with longer job durations and very limited remote availability, while Cluster 3 offers similar salaries but is fully remote, making it the remote-friendly segment of¬†the¬†job¬†market.\n\nclst_sum = pd.read_csv(\"./_output/cluster_summary.csv\")\n\nclst_sum.style.hide(axis=\"index\")\n\n\n\n\n\n\nUnnamed: 0\nAVG_SALARY\nAVG_SALARY.1\nEXPERIENCE_YEARS\nDURATION_DAYS\nIS_REMOTE\n\n\n\n\nnan\nmean\nmedian\nmean\nmean\nmean\n\n\nCluster\nnan\nnan\nnan\nnan\nnan\n\n\n0\n114309.83\n111978.57\n3.88\n16.26\n0.0\n\n\n1\n77114.7\n78684.4\n5.37\n15.91\n0.0\n\n\n2\n95016.68\n94501.34\n4.39\n41.52\n0.06\n\n\n3\n94725.13\n95721.16\n4.41\n19.64\n1.0\n\n\n\n\n\n\n\n4.2 PCA Visualization of Clusters\n\n# Apply PCA for visualization\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\ndf_cluster['PC1'] = X_pca[:, 0]\ndf_cluster['PC2'] = X_pca[:, 1]\n\n# Create scatter plot with custom colors\nplt.figure(figsize=(8, 5))\ncluster_palette = ['#E91E63', '#9B59B6', '#F44336', '#2196F3']\nsns.scatterplot(data=df_cluster, x='PC1', y='PC2', hue='Cluster', \n                palette=cluster_palette, s=60, alpha=0.7, \n                edgecolor='white', linewidth=0.3)\nplt.xlabel('First Principal Component', fontsize=10, fontweight='bold')\nplt.ylabel('Second Principal Component', fontsize=10, fontweight='bold')\nplt.title(f'BA/DS/ML Job Clusters (K={optimal_k})', fontsize=11, fontweight='bold', pad=15)\nplt.legend(title='Cluster', fontsize=9, title_fontsize=10, \n           frameon=True, fancybox=True, shadow=True)\nplt.grid(True, alpha=0.2, linestyle='--')\nplt.tight_layout()\nplt.savefig(\"./_output/pca_plot.png\")\nplt.show()\n\n#print(f\"\\nVariance explained:\")\n#print(f\"PC1: {pca.explained_variance_ratio_[0]:.2%}\")\n#print(f\"PC2: {pca.explained_variance_ratio_[1]:.2%}\")\n#print(f\"Total: {sum(pca.explained_variance_ratio_):.2%}\")\n\nthe first principal component explains 26.95% of the total variance and the second explains 25.08%, for a combined total of 52.03%. The PCA scatter plot maps the job postings onto these two components and shows four clusters formed using K-means (K=4), with each cluster occupying its own region despite some overlap.\n\n\n\n\n\n\nFigure¬†2: PCA for the Job Posting Data"
  },
  {
    "objectID": "ml.html#classification-role-type-prediction",
    "href": "ml.html#classification-role-type-prediction",
    "title": "Machine Learning Methods",
    "section": "5 Classification: Role Type Prediction",
    "text": "5 Classification: Role Type Prediction\nUnderstanding the distinguishing characteristics of different role types can help job seekers tailor their applications and skill development.\n\n5.1 Create Role Categories\n\ndef categorize_role(title):\n    \"\"\"Categorize job titles into BA, DS, ML, or Data Analytics\"\"\"\n    if pd.isna(title):\n        return 'Other'\n    title_lower = str(title).lower()\n    \n    if any(word in title_lower for word in ['business analyst', 'business intelligence']):\n        return 'Business Analytics'\n    elif any(word in title_lower for word in ['machine learning', 'ml engineer', 'ai engineer']):\n        return 'Machine Learning'\n    elif any(word in title_lower for word in ['data scientist', 'data science']):\n        return 'Data Science'\n    elif any(word in title_lower for word in ['data analyst', 'data analytics']):\n        return 'Data Analytics'\n    else:\n        return 'Other'\n\n# Apply categorization\ndf_filtered['ROLE_CATEGORY'] = df_filtered['TITLE_NAME'].apply(categorize_role)\n\n# Filter to main categories\nmain_categories = ['Business Analytics', 'Data Science', 'Machine Learning', 'Data Analytics']\ndf_clf = df_filtered[df_filtered['ROLE_CATEGORY'].isin(main_categories)].copy()\n\n#print(f\"Classification dataset: {len(df_clf):,} samples\")\n#print(\"\\nRole distribution:\")\nrole_dist = df_clf['ROLE_CATEGORY'].value_counts()\n#print(role_dist)\n#print(\"\\nPercentages:\")\n#print(role_dist / len(df_clf) * 100)\nrole_dist.to_csv(\"./_output/Role_Categories.csv\")\nprint(role_dist)\n\nROLE_CATEGORY\nData Analytics        11944\nBusiness Analytics     1776\nData Science            419\nMachine Learning          4\nName: count, dtype: int64\n\n\n\nclass_sum = pd.read_csv(\"./_output/Role_Categories.csv\")\n\nclass_sum.style.hide(axis=\"index\")\n\n\n\n\n\n\nROLE_CATEGORY\ncount\n\n\n\n\nData Analytics\n11944\n\n\nBusiness Analytics\n1776\n\n\nData Science\n419\n\n\nMachine Learning\n4\n\n\n\n\n\n\n\n5.2 Prepare Classification Features\n\n# Get top states\ntop_states = df_clf['STATE_NAME'].value_counts().head(10).index\n\n# Prepare features for classification\nclf_feature_cols = ['EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE', 'AVG_SALARY']\n\n# Add state features\nfor state in top_states:\n    col_name = f'STATE_{state}'\n    df_clf[col_name] = (df_clf['STATE_NAME'] == state).astype(int)\n    clf_feature_cols.append(col_name)\n\n# Prepare X and y\nX_clf = df_clf[clf_feature_cols].fillna(0)\ny_clf = df_clf['ROLE_CATEGORY']\n\n#print(f\"Classification features: {len(clf_feature_cols)}\")\n#print(f\"Samples per class:\")\n#print(y_clf.value_counts())\n\n# Save feature columns as DataFrame\npd.DataFrame(clf_feature_cols, columns=['feature']).to_csv(\"./_output/clf_feature_cols.csv\", index=False)\nprint(clf_feature_cols)\n\n# Train-test split\nX_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n    X_clf, y_clf, test_size=0.3, random_state=42, stratify=y_clf\n)\n\n# Scale features\nscaler_clf = StandardScaler()\nX_train_clf_scaled = scaler_clf.fit_transform(X_train_clf)\nX_test_clf_scaled = scaler_clf.transform(X_test_clf)\n\n['EXPERIENCE_YEARS', 'DURATION_DAYS', 'IS_REMOTE', 'AVG_SALARY', 'STATE_California', 'STATE_Texas', 'STATE_Virginia', 'STATE_New York', 'STATE_Illinois', 'STATE_Florida', 'STATE_Ohio', 'STATE_Georgia', 'STATE_North Carolina', 'STATE_New Jersey']\n\n\n\nclf_feature = pd.read_csv(\"./_output/clf_feature_cols.csv\")\n\nclf_feature.style.hide(axis=\"index\")\n\n\n\n\n\n\nfeature\n\n\n\n\nEXPERIENCE_YEARS\n\n\nDURATION_DAYS\n\n\nIS_REMOTE\n\n\nAVG_SALARY\n\n\nSTATE_California\n\n\nSTATE_Texas\n\n\nSTATE_Virginia\n\n\nSTATE_New York\n\n\nSTATE_Illinois\n\n\nSTATE_Florida\n\n\nSTATE_Ohio\n\n\nSTATE_Georgia\n\n\nSTATE_North Carolina\n\n\nSTATE_New Jersey\n\n\n\n\n\n\n\n5.3 Logistic Regression Classification\n\n# Train logistic regression\nlr = LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial')\nlr.fit(X_train_clf_scaled, y_train_clf)\ny_pred_lr = lr.predict(X_test_clf_scaled)\ny_pred_proba_lr = lr.predict_proba(X_test_clf_scaled)\n\n# Calculate metrics\nacc_lr = accuracy_score(y_test_clf, y_pred_lr)\nf1_lr = f1_score(y_test_clf, y_pred_lr, average='weighted')\n\n#print(\"LOGISTIC REGRESSION CLASSIFICATION\")\n#print(\"=\" * 50)\n#print(f\"Accuracy: {acc_lr:.4f} ({acc_lr*100:.2f}%)\")\n#print(f\"F1 Score (Weighted): {f1_lr:.4f}\")\n#print(\"\\nClassification Report:\")\n#print(classification_report(y_test_clf, y_pred_lr))\n\n# Save metrics as DataFrames\npd.DataFrame({'accuracy': [acc_lr]}).to_csv(\"./_output/accuracy.csv\", index=False)\npd.DataFrame({'f1_score': [f1_lr]}).to_csv(\"./_output/f1.csv\", index=False)\n#print(f\"Accuracy: {acc_lr:.4f}\")\n#print(f\"F1 Score: {f1_lr:.4f}\")\n\n\nacc = pd.read_csv(\"./_output/accuracy.csv\")\nacc.style.hide(axis=\"index\")\n\n\n\n\n\n\naccuracy\n\n\n\n\n0.841150\n\n\n\n\n\n\nf1 = pd.read_csv(\"./_output/f1.csv\")\nf1.style.hide(axis=\"index\")\n\n\n\n\n\n\nf1_score\n\n\n\n\n0.774877\n\n\n\n\n\nThe logistic regression model reaches 84% accuracy, but this is mainly because it predicts most entries as ‚ÄúData Analytics,‚Äù the largest class in the dataset. While the model performs well for this category, it struggles to recognize smaller roles like Business Analytics, Data Science, and Machine Learning, which show very low recall and F1-scores. This imbalance means the model is not effectively distinguishing minority roles and is primarily learning from the dominant class rather than providing balanced¬†prediction\n\n\n5.4 Random Forest Classification\n\n# Train random forest classifier\nrf_clf = RandomForestClassifier(n_estimators=100, max_depth=15, \n                                min_samples_split=10, random_state=42, n_jobs=-1)\nrf_clf.fit(X_train_clf, y_train_clf)\ny_pred_rf_clf = rf_clf.predict(X_test_clf)\ny_pred_proba_rf = rf_clf.predict_proba(X_test_clf)\n\n# Calculate metrics\nacc_rf_clf = accuracy_score(y_test_clf, y_pred_rf_clf)\nf1_rf_clf = f1_score(y_test_clf, y_pred_rf_clf, average='weighted')\n\n#print(\"RANDOM FOREST CLASSIFICATION\")\n#print(\"=\" * 50)\n#print(f\"Accuracy: {acc_rf_clf:.4f} ({acc_rf_clf*100:.2f}%)\")\n#print(f\"F1 Score (Weighted): {f1_rf_clf:.4f}\")\n#print(\"\\nClassification Report:\")\n#print(classification_report(y_test_clf, y_pred_rf_clf))\n\n# Save metrics as DataFrames\npd.DataFrame({'accuracy': [acc_rf_clf]}).to_csv(\"./_output/accuracy_rf.csv\", index=False)\npd.DataFrame({'f1_score': [f1_rf_clf]}).to_csv(\"./_output/f1_rf.csv\", index=False)\n#print(f\"Accuracy: {acc_rf_clf:.4f}\")\n#print(f\"F1 Score: {f1_rf_clf:.4f}\")\n\n\nacc_random = pd.read_csv(\"./_output/accuracy_rf.csv\")\nacc_random.style.hide(axis=\"index\")\n\n\n\n\n\n\naccuracy\n\n\n\n\n0.856469\n\n\n\n\n\n\nf1_random = pd.read_csv(\"./_output/f1_rf.csv\")\nf1_random.style.hide(axis=\"index\")\n\n\n\n\n\n\nf1_score\n\n\n\n\n0.814503\n\n\n\n\n\nThe model reaches a high overall accuracy of 85.6%, but this is influenced by the extreme class imbalance in the dataset. It predicts the dominant Data Analytics category very well, yet performs poorly on the smaller groups -Business Analytics, Data Science, and Machine Learning which is leading to a low F1 score of 0.33. This shows that the model is not generalizing effectively across all role types. To achieve more balanced and reliable results, techniques such as oversampling, class weighting, or rebalancing the dataset¬†would¬†be¬†needed.\n\n\n5.5 Classification Model Comparison\n\n#### ROC Curves - Logistic Regression\n#| fig-cap: \"Logistic Regression ROC Curves\"\n#| echo: true\n#| eval: true\n\n# Binarize labels for ROC curve\nclasses = lr.classes_\ny_test_bin = label_binarize(y_test_clf, classes=classes)\nn_classes = len(classes)\n\n# Color palette for classes\ncolors = cycle(['#E91E63', '#9B59B6', '#F44336', '#2196F3'])\n\n# Create figure\nplt.figure(figsize=(8, 6))\n\nfor i, color, class_name in zip(range(n_classes), colors, classes):\n    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba_lr[:, i])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, color=color, lw=2.5, \n            label=f'{class_name} (AUC = {roc_auc:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2, label='Chance (AUC = 0.500)')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\nplt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\nplt.title('Logistic Regression: ROC Curves', fontsize=14, fontweight='bold', pad=15)\nplt.legend(loc=\"lower right\", fontsize=10, frameon=True, fancybox=True, shadow=True)\nplt.grid(True, alpha=0.3, linestyle='--')\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.tight_layout()\nplt.savefig(\"./_output/roc_lr.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\nThe ROC curves show that the logistic regression model has moderate ability to distinguish between the different role categories, with AUC scores ranging from 0.58 to 0.77. Machine Learning achieves the highest AUC (0.775), suggesting the model can separate this class reasonably well despite its tiny sample size, while Data Science has the weakest separability (0.585). Overall, the curves indicate that the classifier performs above random chance for all roles but still struggles to clearly differentiate between them, reflecting the impact of class imbalance and overlapping feature patterns.\n\n5.5.1 ROC Curves - Random Forest\n\n# Create figure\nplt.figure(figsize=(8, 6))\n\ncolors = cycle(['#E91E63', '#9B59B6', '#F44336', '#2196F3'])\nfor i, color, class_name in zip(range(n_classes), colors, classes):\n    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba_rf[:, i])\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, color=color, lw=2.5, \n            label=f'{class_name} (AUC = {roc_auc:.3f})')\n\nplt.plot([0, 1], [0, 1], 'k--', lw=2, label='Chance (AUC = 0.500)')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\nplt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\nplt.title('Random Forest: ROC Curves', fontsize=14, fontweight='bold', pad=15)\nplt.legend(loc=\"lower right\", fontsize=10, frameon=True, fancybox=True, shadow=True)\nplt.grid(True, alpha=0.3, linestyle='--')\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.tight_layout()\nplt.savefig(\"./_output/roc_rf.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\nRandom Forest ROC Curves\n\n\n\n\nThe Random Forest model shows improved class separability compared to logistic regression, with AUC values ranging from 0.60 to 0.84. Machine Learning achieves the strongest performance (AUC = 0.842), indicating the model can distinguish this role well despite its tiny sample size. Business Analytics and Data Analytics also show moderate discrimination, while Data Science remains the most challenging class, reflecting overlapping features and limited data representation.\n\n\n5.5.2 Model Performance Comparison\n\ncomparison_df = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Random Forest'],\n    'Accuracy': [acc_lr, acc_rf_clf],\n    'F1 Score': [f1_lr, f1_rf_clf]\n})\n\nplt.figure(figsize=(8, 6))\nx = np.arange(len(comparison_df['Model']))\nwidth = 0.35\n\nbars1 = plt.bar(x - width/2, comparison_df['Accuracy'], width, \n               label='Accuracy', color='#E91E63', alpha=0.8, edgecolor='white', linewidth=1.5)\nbars2 = plt.bar(x + width/2, comparison_df['F1 Score'], width, \n               label='F1 Score', color='#9B59B6', alpha=0.8, edgecolor='white', linewidth=1.5)\n\nplt.ylabel('Score', fontsize=12, fontweight='bold')\nplt.xlabel('Model', fontsize=12, fontweight='bold')\nplt.title('Model Performance Comparison', fontsize=14, fontweight='bold', pad=15)\nplt.xticks(x, comparison_df['Model'])\nplt.legend(fontsize=10, frameon=True, fancybox=True, shadow=True)\nplt.ylim([0, 1.1])\nplt.grid(True, alpha=0.3, axis='y', linestyle='--')\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Add value labels on bars\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(\"./_output/model_comparison.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\nModel Performance Comparison\n\n\n\n\nThe comparison shows that Random Forest outperforms Logistic Regression, achieving both higher accuracy (85.6%) and a stronger F1 score (0.815). This indicates that Random Forest handles the complex and imbalanced role categories more effectively. Overall, while both models perform well, Random Forest delivers more balanced and reliable predictions across¬†the¬†dataset.\n\n\n5.5.3 Feature Importance Analysis\n\nclf_importance = pd.DataFrame({\n    'Feature': clf_feature_cols,\n    'Importance': rf_clf.feature_importances_\n}).sort_values('Importance', ascending=False).head(10)\n\nplt.figure(figsize=(8, 6))\nbars = plt.barh(range(len(clf_importance)), clf_importance['Importance'], \n               color=['#E91E63', '#9B59B6', '#F44336', '#2196F3'] * 3, \n               alpha=0.8, edgecolor='white', linewidth=1.5)\nplt.yticks(range(len(clf_importance)), clf_importance['Feature'])\nplt.xlabel('Feature Importance', fontsize=12, fontweight='bold')\nplt.title('Top 10 Predictive Features (Random Forest)', fontsize=14, fontweight='bold', pad=15)\nplt.grid(True, alpha=0.3, axis='x', linestyle='--')\nax = plt.gca()\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\n\n# Add value labels\nfor i, bar in enumerate(bars):\n    width = bar.get_width()\n    plt.text(width, bar.get_y() + bar.get_height()/2.,\n            f'{width:.3f}', ha='left', va='center', fontsize=9, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(\"./_output/feature_importance.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\nTop 10 Predictive Features (Random Forest)\n\n\n\n\nThe feature importance results show that *experience years, average salary, and job duration are the strongest predictors in distinguishing between BA, Data Science, ML, and Data Analytics roles. Remote status contributes modestly, while location-based features (state variables) have minimal impact, indicating that job role differences are driven more by skill level and job characteristics than by geography. Overall, the model relies most heavily on experience and salary patterns to differentiate¬†job¬†categories."
  },
  {
    "objectID": "Skill_Gap_Analysis.html",
    "href": "Skill_Gap_Analysis.html",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "This analysis compares our team‚Äôs current skill levels against the skills demanded in the IT job market. By identifying gaps between our capabilities and market requirements, we can develop targeted learning strategies to enhance our competitiveness as job candidates.\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing 59,220 job postings for skill requirements"
  },
  {
    "objectID": "Skill_Gap_Analysis.html#introduction",
    "href": "Skill_Gap_Analysis.html#introduction",
    "title": "Skill Gap Analysis",
    "section": "",
    "text": "This analysis compares our team‚Äôs current skill levels against the skills demanded in the IT job market. By identifying gaps between our capabilities and market requirements, we can develop targeted learning strategies to enhance our competitiveness as job candidates.\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing 59,220 job postings for skill requirements"
  },
  {
    "objectID": "Skill_Gap_Analysis.html#team-skill-assessment",
    "href": "Skill_Gap_Analysis.html#team-skill-assessment",
    "title": "Skill Gap Analysis",
    "section": "2 1. Team Skill Assessment",
    "text": "2 1. Team Skill Assessment\n\n2.1 1.1 Creating Team Skills Profile\nEach team member rates their proficiency in key IT skills on a scale of 1-5: - 1 = Beginner (aware of the skill) - 2 = Basic knowledge (can perform simple tasks) - 3 = Intermediate (comfortable with common scenarios) - 4 = Advanced (can handle complex problems) - 5 = Expert (can teach others and solve any problem)\nThe team‚Äôs strongest skills are *Excel, Python, and Data Analysis, showing solid readiness for analytical and data-focused roles. Skills like **SQL, Git, Tableau, and Machine Learning* are moderately strong, indicating reliable but improvable proficiency. Meanwhile, areas such as AWS, Azure, TensorFlow, Docker, and Cloud Computing show lower scores, highlighting clear opportunities for growth‚Äîespecially important given rising market demand for cloud and ML engineering skills. Overall, the team has a strong analytical foundation but should focus on boosting cloud and advanced technical competencies to stay¬†competitive.\n\n\n2.2 1.2 Team Skills Heatmap\nVisualize each team member‚Äôs strengths and weaknesses across all skills.\n\n\n\n\n\n\n  \n\n\n\n\nThis heatmap shows how strong each team member is across key tech skills like AWS, Excel, SQL, Machine Learning, and more. The greener the box, the stronger the skill‚Äîred means the weakest. From a job seeker‚Äôs perspective, this quickly highlights which skills are common strengths and which ones need improvement to stay competitive. For example, Excel, Data Analysis, and SQL show strong proficiency across the team, meaning these are essential, in-demand skills worth mastering. Meanwhile, AWS and JavaScript show weaker proficiency, signaling great opportunities for upskilling‚Äîespecially since cloud and coding skills are highly valued in¬†today‚Äôs¬†market.\n\n\n2.3 1.3 Team Average Skills Bar Chart\nCompare team average proficiency across all skills.\n\n\n\n\n\n\n  \n\n\n\n\nThe team‚Äôs top 10 skills show strong proficiency in high-value areas like *Excel, Data Analysis, Python, Git, and SQL, which are core requirements for most analytics and tech roles. Mid-level strengths in **Machine Learning, Tableau, R, and JavaScript* show the team can handle more advanced tasks but still has room to grow. Overall, these skills place the team in a competitive position for data and tech jobs, while highlighting opportunities to strengthen cloud and AI-related tools for even better market¬†readiness."
  },
  {
    "objectID": "Skill_Gap_Analysis.html#market-skill-demand-analysis",
    "href": "Skill_Gap_Analysis.html#market-skill-demand-analysis",
    "title": "Skill Gap Analysis",
    "section": "3 2. Market Skill Demand Analysis",
    "text": "3 2. Market Skill Demand Analysis\n\n3.1 2.1 Extracting Skills from Job Descriptions\nWe analyze job postings to identify the most in-demand skills in the IT job market.\nThe skill extraction shows that Data Analysis, Cloud Computing, and TensorFlow are the most frequently requested skills in job postings, making them top priorities for job seekers. Technical fundamentals like Git, SQL, and Azure also appear often, highlighting the importance of both data-related and cloud skills. Lower-frequency skills such as Spark or AWS still matter but are requested less often, suggesting they may serve as valuable ‚Äúbonus‚Äù skills rather than core¬†requirements.\n\n\n3.2 2.2 Top In-Demand Skills Visualization\n\n\n\n\n\n\n  \n\n\n\n\nThe chart shows that Data Analysis and Cloud Computing are the most in-demand skills in the 2024 job market, with far more postings than any other skill. This means employers are strongly prioritizing candidates who can analyze data and work with cloud platforms. Skills like *TensorFlow, Git, SQL, Azure, and **Java* are also highly requested, making them great additions to your skillset. Lower-demand skills such as *Spark, AWS, Tableau, and **Machine Learning* still matter, but they won‚Äôt give as much of a competitive edge as¬†the¬†top¬†skills."
  },
  {
    "objectID": "Skill_Gap_Analysis.html#skill-gap-identification",
    "href": "Skill_Gap_Analysis.html#skill-gap-identification",
    "title": "Skill Gap Analysis",
    "section": "4 3. Skill Gap Identification",
    "text": "4 3. Skill Gap Identification\n\n4.1 3.1 Comparing Team Skills to Market Demand\nNow we identify gaps between our team‚Äôs capabilities and what the market requires.\nThe analysis shows that our team is *strong in most core skills, especially Python, SQL, Tableau, Machine Learning, and Git ‚Äî all areas where market demand is relatively low to moderate. However, there are **two skills with clear gaps* compared to what employers want:\n\nCloud Computing (e.g., AWS, Azure)\nData Analysis\n\nThese areas are in very high demand, but our team‚Äôs proficiency is lower than what the job market expects. Strengthening Cloud tools and advanced Data Analysis techniques would significantly boost job readiness and competitiveness in today‚Äôs tech hiring¬†landscape.\n\n\n4.2 3.2 Skill Gap Visualization\n\n\n\n\n\n\n  \n\n\n\n\nThe chart clearly shows that market demand is much higher than the team‚Äôs current skill levels across nearly all skills. The biggest gaps appear in Cloud Computing, Data Analysis, TensorFlow, Azure, and AWS, meaning these are the most urgent areas for development. Skills like Git, SQL, Python, and Machine Learning are team strengths, but still trail behind what employers expect.\nFor job seekers like us, this means focusing on cloud technologies, advanced analytics, and AI frameworks will significantly boost competitiveness and align better with real market¬†needs.\n\n\n4.3 3.3 Skill Gap Priority Matrix\nIdentify which skills need immediate attention based on gap size.\n\n\n\n\n\n\n  \n\n\n\n\nThe matrix helps identify which skills your team should focus on first.\n\nGreen (Moderate Gap): These skills‚Äîlike Cloud Computing, Data Analysis, and TensorFlow‚Äîare high in market demand but lower in team proficiency, meaning they should be top priority for upskilling.\nOrange (Strength): Most other skills fall in this category. These are areas where the team is already strong compared to market demand‚Äîgreat to maintain but not urgent for improvement."
  },
  {
    "objectID": "Skill_Gap_Analysis.html#individual-skill-gap-analysis",
    "href": "Skill_Gap_Analysis.html#individual-skill-gap-analysis",
    "title": "Skill Gap Analysis",
    "section": "5 4. Individual Skill Gap Analysis",
    "text": "5 4. Individual Skill Gap Analysis\n\n5.1 4.1 Skill Gaps by Team Member\nIdentify personalized skill development needs for each team member.\nach team member has two main skill gaps: Cloud Computing and Data Analysis.\nTuba Anwar needs improvement mainly in Cloud Computing, with a smaller gap in Data Analysis.\nKriti Singh also shows the same pattern‚ÄîCloud Computing is the biggest gap, followed by Data Analysis.\nSoham Deshkhaires has the highest gap in Data Analysis, and a moderate gap in Cloud Computing.\nOverall Insight: All three team members share the same critical areas for improvement. Strengthening Cloud Computing and Data Analysis should be the top priority¬†for¬†the¬†team.\n\n\n5.2 4.2 Individual Gap Visualization\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n  \n\n\n\n\nAcross all three members‚ÄîTuba, Kriti, and Soham‚Äîthe largest skill gap is in Cloud Computing, meaning the market demands this skill at a much higher level than the team currently possesses. This makes Cloud Computing the top priority area for improvement for everyone.\nAdditionally, Data Analysis appears as a major gap for both Tuba and Kriti, while Soham shows a moderate gap in Cloud Computing only. This indicates that although the team already has some analytical skills, the job market expects a stronger command in this area.\nThe charts highlight two urgent development needs for the team:\nCloud Computing ‚Üí biggest gap for all members\nData Analysis ‚Üí second-highest gap for Tuba & Kriti\nFocusing training efforts on these skills will significantly improve alignment with market expectations.individual"
  },
  {
    "objectID": "Skill_Gap_Analysis.html#skill-development-plan",
    "href": "Skill_Gap_Analysis.html#skill-development-plan",
    "title": "Skill Gap Analysis",
    "section": "6 5. Skill Development Plan",
    "text": "6 5. Skill Development Plan\n\n6.1 5.1 Priority Skills for Team Development\nBased on our analysis, here are the priority skills the team should focus on:\nThe analysis shows that the team has *no critical skill gaps, meaning no skill is urgently below market expectations. However, two areas ‚Äî **Cloud Computing* and Data Analysis ‚Äî fall into the moderate-priority category. These skills have higher market demand than the team‚Äôs current proficiency levels, making them important for upskilling. Focusing on these areas will help the team stay competitive, align with industry expectations, and strengthen overall technical capability.\n\n\n6.2 5.3 Team Collaboration Strategy\nHow can team members help each other?\nThe team collaboration strategy highlights how members can support one another by leveraging individual strengths to address skill gaps. Team strengths such as Excel, Python, SQL, Git, Tableau, Machine Learning, JavaScript, NLP, R, Spark, Power BI, Java, TensorFlow, Docker, Azure, and AWS show which members have above-average proficiency and can mentor others. Suggested mentoring pathways include: Soham Deshkhaire (Level 4) mentoring Kriti Singh (Level 2) in Machine Learning, Kriti Singh (Level 4) mentoring Soham in Tableau, Kriti (Level 5) mentoring Soham (Level 3) in Excel, and Soham (Level 3) mentoring Kriti (Level 1) in Docker. These targeted pairings ensure knowledge transfer, help close individual skill gaps, and strengthen overall team¬†capability.\n\n\n6.3 5.4 3-Month, 6-Month, and 1-Year Goals\nCreate timeline for skill development:\n\n\n\n\n\n\nTipSkill Development Timeline\n\n\n\n3-Month Goals (Immediate Priority) - Focus on critical gap skills with highest market demand - Complete foundational courses in AWS, Docker, and Cloud Computing - Build 1-2 hands-on projects demonstrating new skills - Team members mentor each other in strength areas\n6-Month Goals (Intermediate) - Achieve intermediate proficiency (Level 3) in all critical gap skills - Complete advanced courses in Machine Learning and Data Analysis - Participate in Kaggle competitions or contribute to open-source projects - Earn 1-2 professional certifications (e.g., AWS Certified Developer)\n1-Year Goals (Advanced) - Achieve advanced proficiency (Level 4) in priority skills - Entire team reaches minimum Level 3 in all high-demand market skills - Build comprehensive portfolio showcasing technical competencies - Competitive job candidates for targeted IT roles"
  },
  {
    "objectID": "Skill_Gap_Analysis.html#references",
    "href": "Skill_Gap_Analysis.html#references",
    "title": "Skill Gap Analysis",
    "section": "7 References",
    "text": "7 References\n\nJob market data: Lightcast Job Postings Dataset (2024)\nSkill assessment framework: Industry-standard proficiency scales\nLearning resources: Coursera, Udemy, AWS Training, Kaggle\nAnalysis tools: Python, pandas, hvPlot, Panel"
  },
  {
    "objectID": "data_analysis.html",
    "href": "data_analysis.html",
    "title": "Data Analysis",
    "section": "",
    "text": "This document provides a comprehensive analysis of job market data for 2024. We perform data cleaning, handle missing values, remove duplicates, and conduct exploratory data analysis to understand hiring trends, salary distributions, and job market dynamics."
  },
  {
    "objectID": "data_analysis.html#introduction",
    "href": "data_analysis.html#introduction",
    "title": "Data Analysis",
    "section": "",
    "text": "This document provides a comprehensive analysis of job market data for 2024. We perform data cleaning, handle missing values, remove duplicates, and conduct exploratory data analysis to understand hiring trends, salary distributions, and job market dynamics."
  },
  {
    "objectID": "data_analysis.html#step-1-removing-redundant-columns",
    "href": "data_analysis.html#step-1-removing-redundant-columns",
    "title": "Data Analysis",
    "section": "2 Step 1: Removing Redundant Columns",
    "text": "2 Step 1: Removing Redundant Columns\n\n2.1 Why Remove These Columns?\nWe remove redundant columns to improve dataset quality and analysis efficiency. Specifically:\n\nTracking & Administrative Columns: ID, URL, DUPLICATES, LAST_UPDATED_TIMESTAMP - These are metadata and don‚Äôt contribute to analysis\nRaw Text Fields: BODY, TITLE_RAW, COMPANY_RAW - We have cleaned versions (TITLE, COMPANY_NAME, TITLE_CLEAN)\nDeprecated Classifications: We keep only the latest NAICS_2022_6, SOC_2021_4, and CIP4 standards\nDuplicate Geographic Fields: Multiple versions of county/MSA data create redundancy"
  },
  {
    "objectID": "data_analysis.html#step-2-handling-missing-values",
    "href": "data_analysis.html#step-2-handling-missing-values",
    "title": "Data Analysis",
    "section": "3 Step 2: Handling Missing Values",
    "text": "3 Step 2: Handling Missing Values\n\n3.1 Understanding Missing Data\nBefore imputation, let‚Äôs visualize where data is missing:\n\n\n3.2 Missing Value Imputation Strategy\nWe applied a strategic approach to handle missing data:\n\nDropped columns with more than 50% missing values\nFilled numerical columns with median values to maintain distribution\nFilled categorical columns with ‚ÄúUnknown‚Äù for clarity"
  },
  {
    "objectID": "data_analysis.html#step-3-removing-duplicates",
    "href": "data_analysis.html#step-3-removing-duplicates",
    "title": "Data Analysis",
    "section": "4 Step 3: Removing Duplicates",
    "text": "4 Step 3: Removing Duplicates\n\n4.1 Why Remove Duplicates?\nDuplicate job postings skew our analysis. We remove them based on job title, company, and location to ensure each unique position is counted only once."
  },
  {
    "objectID": "data_analysis.html#step-4-exploratory-data-analysis-eda",
    "href": "data_analysis.html#step-4-exploratory-data-analysis-eda",
    "title": "Data Analysis",
    "section": "5 Step 4: Exploratory Data Analysis (EDA)",
    "text": "5 Step 4: Exploratory Data Analysis (EDA)\n\n5.1 Visualization 1: Top Job Titles\nUnderstanding which job titles are most in demand helps job seekers identify the skills and roles to target.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nThe top job titles show where the job market is most active. This data helps identify roles with highest demand for new talent.\n\n\n\n\n5.2 Visualization 2: Job Postings by Employment Type\nEmployment type indicates the nature of positions available in the job market. Understanding the distribution helps job seekers target positions that match their preferences.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nThe distribution of employment types shows whether the market is primarily offering full-time, part-time, or contract positions, helping job seekers identify opportunities that match their work preferences.\n\n\n\n\n5.3 Visualization 3: Remote Work Distribution\nThe prevalence of remote work reflects post-pandemic hiring trends and work flexibility.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nThe balance between remote and on-site jobs shows employers‚Äô flexibility preferences. Higher remote job percentages indicate industry acceptance of distributed workforces.\n\n\n\n\n5.4 Visualization 4: Top Companies Hiring\nUnderstanding which companies are actively hiring helps job seekers identify potential employers with multiple open positions.\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteKey Insight\n\n\n\nCompanies with the highest number of job postings indicate organizations that are actively expanding their workforce and may offer more opportunities for candidates."
  },
  {
    "objectID": "data_analysis.html#summary-of-data-cleaning-analysis",
    "href": "data_analysis.html#summary-of-data-cleaning-analysis",
    "title": "Data Analysis",
    "section": "6 Summary of Data Cleaning & Analysis",
    "text": "6 Summary of Data Cleaning & Analysis\n\n\n\n\n\n\nImportantSummary\n\n\n\nThe data cleaning process successfully prepared the job market dataset for analysis. We removed redundant administrative and deprecated classification columns, handled missing values through strategic imputation, and eliminated duplicate job postings.\nThe exploratory analysis revealed key insights into job market trends, including:\n\nHigh-demand roles: Identification of the most sought-after job titles in 2024\nEmployment stability: Understanding the distribution of full-time, part-time, and contract positions\nWorkplace flexibility: Analysis of remote, hybrid, and on-site work opportunities\nSalary trends: Clear understanding of compensation ranges across different job categories\n\nThese insights provide valuable guidance for job seekers, employers, and market analysts in understanding the current state of the 2024 job market."
  },
  {
    "objectID": "data_analysis.html#references",
    "href": "data_analysis.html#references",
    "title": "Data Analysis",
    "section": "7 References",
    "text": "7 References\nAll data sourced from Lightcast Job Postings Dataset (2024). Analysis performed using Python with pandas, hvplot, and holoviews libraries."
  }
]